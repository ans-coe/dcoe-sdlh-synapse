{
	"name": "nb_util_sql_staged_etl_and_logging_py",
	"properties": {
		"folder": {
			"name": "utils"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "5ded12d6-7658-460a-acd2-b25fdbd0560e"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/6358b997-b7c0-4d30-b9bd-9ab4057ac712/resourceGroups/rg-sigma-data-dev-uksouth-002/providers/Microsoft.Synapse/workspaces/synw-sigma-data-dev-uksouth-002/bigDataPools/synspsm33as34",
				"name": "synspsm33as34",
				"type": "Spark",
				"endpoint": "https://synw-sigma-data-dev-uksouth-002.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/synspsm33as34",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## SDLH (Strategic Data Lakehouse) Utility\r\n",
					"<p><b>Description: </b>Enter notebook decription</p>\r\n",
					"<b>Parent Process: </b>SDLH Pipelines</p>\r\n",
					"<table align=\"left\">\r\n",
					" <thead>\r\n",
					"  <tr>\r\n",
					"   <th>Contributor</th>\r\n",
					"   <th>Date</th>\r\n",
					"   <th>Version</th>\r\n",
					"   <th>Comment</th>\r\n",
					"   <th>WorkItem No</th>\r\n",
					"  </tr>\r\n",
					" </thead>\r\n",
					" <tbody>\r\n",
					"  <tr>\r\n",
					"   <td>Andrei Dumitru</td>\r\n",
					"   <td>2022-12-09</td>\r\n",
					"   <td>1.0</td>\r\n",
					"   <td>Create initial release</td>\r\n",
					"   <td></td>\r\n",
					"  </tr>\r\n",
					"  <tr>\r\n",
					"   <td>Darren Price</td>\r\n",
					"   <td>2023-10-23</td>\r\n",
					"   <td>2.0</td>\r\n",
					"   <td>Updated for SDLH v2</td>\r\n",
					"   <td></td>\r\n",
					"  </tr>\r\n",
					" </tbody>\r\n",
					"</table>"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#not logged\r\n",
					"def query_db_jdbc(jdbc_url:str, jdbc_token:str, pushdown_query:str) -> pyspark.sql.dataframe.DataFrame:\r\n",
					"    \"\"\"\r\n",
					"    Queries a SQL Server database using JDBC and returns a PySpark DataFrame.\r\n",
					"\r\n",
					"    Parameters:\r\n",
					"    - jdbc_hostname (str): the hostname or IP address of the SQL Server instance\r\n",
					"    - jdbc_database (str): the name of the database to connect to\r\n",
					"    - jdbc_port (str): the port number on which the SQL Server instance is listening\r\n",
					"    - jdbc_options (str): any options to add to the connection\r\n",
					"    - jdbc_token (str): the managed identity token to authenticate with\r\n",
					"    - pushdown_query (str): a SQL query to execute in the database (with optional filters and transformations)\r\n",
					"\r\n",
					"    Returns:\r\n",
					"    - df (pyspark.sql.dataframe.DataFrame): a PySpark DataFrame containing the result set of the query, with an additional \"current_date\" column that contains the current date.\r\n",
					"\r\n",
					"    \"\"\"\r\n",
					"    # Create connection properties, uses generated managed identity token\r\n",
					"    connection_properties = {\r\n",
					"      \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\r\n",
					"      \"accessToken\" : jdbc_token\r\n",
					"    }\r\n",
					"\r\n",
					"    # Use PySpark's built-in JDBC reader to execute the query and create a DataFrame and add current_date column\r\n",
					"    df = spark.read.jdbc(url=jdbc_url, table=pushdown_query, properties=connection_properties).withColumn(\"current_date\", current_date())\r\n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def query_db_odbc(odbc_url:str, odbc_token:str, pushdown_query:str) -> pyspark.sql.dataframe.DataFrame:\r\n",
					"    \"\"\"\r\n",
					"    Queries a SQL Server database using ODBC and returns a PySpark DataFrame.\r\n",
					"\r\n",
					"    Parameters:\r\n",
					"    - odbc_hostname (str): the hostname or IP address of the SQL Server instance\r\n",
					"    - odbc_database (str): the name of the database to connect to\r\n",
					"    - odbc_port (str): the port number on which the SQL Server instance is listening\r\n",
					"    - odbc_options (str): any options to add to the connection\r\n",
					"    - odbc_token (str): the managed identity token to authenticate with\r\n",
					"    - pushdown_query (str): a SQL query to execute in the database (with optional filters and transformations)\r\n",
					"\r\n",
					"    Returns:\r\n",
					"    - df (pyspark.sql.dataframe.DataFrame): a PySpark DataFrame containing the result set of the stored procedure.\r\n",
					"\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    # Create connection properties, uses generated managed identity token\r\n",
					"    connection_properties = pyodbc.connect(odbc_url, attrs_before = { SQL_COPT_SS_ACCESS_TOKEN:odbc_token })\r\n",
					"\r\n",
					"    # Read the data into a Pandas dataframe\r\n",
					"    df_pd = pd.read_sql(pushdown_query, connection_properties)\r\n",
					"\r\n",
					"    # Create a SparkSession\r\n",
					"    spark = SparkSession.builder.appName(\"Pandas to Spark\").getOrCreate()\r\n",
					"\r\n",
					"    # Convert the Pandas dataframe to a Spark dataframe\r\n",
					"    df_spark = spark.createDataFrame(df_pd)\r\n",
					"\r\n",
					"    # Show the Spark dataframe\r\n",
					"    return df_spark"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_raw_staged_etl_instructions(adls_container:str, adls_name:str,) -> dict:\r\n",
					"    JOB_ID = synapse_adls_remount(adls_container, adls_name, \"datalake_data_azir\")\r\n",
					"    # Get tables that have already been migrated from the raw_migrated_tables_log DataFrame\r\n",
					"    tables_migrated_raw = df_objects_metadata_extracted.selectExpr(\"OBJECT_NAME\", \\\r\n",
					"                                                        \"SOURCE_SYSTEM\", \\\r\n",
					"                                                        \"DATABASE_NAME\",\\\r\n",
					"                                                        \"SCHEMA_NAME\",\\\r\n",
					"                                                        \"TABLE_NAME\",  \\\r\n",
					"                                                        \"SERVERLESS_SQL_POOL_DATABASE\",  \\\r\n",
					"                                                        \"SERVERLESS_SQL_POOL_SCHEMA\",  \\\r\n",
					"                                                        \"ETL_QUEUE_NAME\",  \\\r\n",
					"                                                        \"PIPELINE_NAME\", \\\r\n",
					"                                                        \"LOAD_TYPE\", \\\r\n",
					"                                                        \"PRIMARY_KEYS\", \\\r\n",
					"                                                        f\"CONCAT('abfss://raw@{adls_name}.dfs.core.windows.net/', FP1_RAW) AS RAW_PATH\",\\\r\n",
					"                                                        f\"CONCAT('synfs:/{JOB_ID}/mount/', FP1_STAGED) AS STAGED_PATH\")\\\r\n",
					"                                  .rdd.map(lambda x: (x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7], x[8], x[9], x[10], x[11], x[12])).collect()\r\n",
					"    tables_migrated_raw=list(tables_migrated_raw)\r\n",
					"\r\n",
					"    # Construct the data path for each table and store it in a dictionary\r\n",
					"    staged_instructions = {}\r\n",
					"    primary_key_lookup = {}\r\n",
					"\r\n",
					"    for row in df_objects_metadata_extracted.select(\"TABLE_NAME\", \"PRIMARY_KEYS\").filter(df_objects_metadata_extracted.PRIMARY_KEYS.isNotNull()).distinct().collect():\r\n",
					"        primary_key_lookup[row.TABLE_NAME] = row.PRIMARY_KEYS.split(\", \")\r\n",
					"\r\n",
					"\r\n",
					"    for OBJECT_NAME, SOURCE_SYSTEM, DATABASE_NAME, SCHEMA_NAME, TABLE_NAME, SERVERLESS_SQL_POOL_DATABASE, SERVERLESS_SQL_POOL_SCHEMA, ETL_QUEUE_NAME, PIPELINE_NAME, LOAD_TYPE, PRIMARY_KEYS, RAW_PATH, STAGED_PATH  in tables_migrated_raw:\r\n",
					"        data_path = f\"{RAW_PATH}\"\r\n",
					"        if LOAD_TYPE == \"FULL_LOAD\":\r\n",
					"            table_primary_key_statements = [\"NULL\"]\r\n",
					"        else:\r\n",
					"            # Simplify string concatenation using f-strings\r\n",
					"            primary_keys = primary_key_lookup[TABLE_NAME]\r\n",
					"            table_primary_key_statements = [f\"{TABLE_NAME}_parquet.{pk} = {TABLE_NAME}_delta.{pk}\".lower() for pk in primary_keys]\r\n",
					"            \r\n",
					"        staged_instructions[TABLE_NAME] = {\"OBJECT_NAME\":OBJECT_NAME, \"SOURCE_SYSTEM\":SOURCE_SYSTEM, \"DATABASE_NAME\": DATABASE_NAME, \"SCHEMA_NAME\": SCHEMA_NAME, \"TABLE_NAME\": TABLE_NAME, \"SERVERLESS_SQL_POOL_DATABASE\":SERVERLESS_SQL_POOL_DATABASE, \"SERVERLESS_SQL_POOL_SCHEMA\":SERVERLESS_SQL_POOL_SCHEMA, \"ETL_QUEUE_NAME\":ETL_QUEUE_NAME, \"PIPELINE_NAME\": PIPELINE_NAME, \"LOAD_TYPE\":LOAD_TYPE, \"PRIMARY_KEYS\":PRIMARY_KEYS , \"PRIMARY_KEY_STATEMENTS\": table_primary_key_statements[0] , \"RAW_PATH\":RAW_PATH, \"STAGED_PATH\":STAGED_PATH}\r\n",
					"    return staged_instructions\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#not logged\r\n",
					"def synapse_adls_remount(file_system_name:str, datalake_name:str, linked_service_name:str)->str:\r\n",
					"    \"\"\"\r\n",
					"    Mounts an Azure Data Lake Storage Gen2 filesystem using the specified file_system_name, datalake_name, and \r\n",
					"    linked_service_name. The mounted filesystem will be available at \"/mount\" in the current workspace.\r\n",
					"    \r\n",
					"    Parameters:\r\n",
					"    - file_system_name (str): The name of the filesystem to mount.\r\n",
					"    - datalake_name (str): The name of the Azure Data Lake Storage Gen2 account.\r\n",
					"    - linked_service_name (str): The name of the Azure Synapse linked service to use for mounting the filesystem.\r\n",
					"    \r\n",
					"    Returns:\r\n",
					"    - str: The job ID of the current Synapse Analytics job.\r\n",
					"    \r\n",
					"    \"\"\"\r\n",
					"    # Unmounting any existing mount points\r\n",
					"    mssparkutils.fs.unmount(\"/mount\")\r\n",
					"\r\n",
					"    # Mounting the specified filesystem\r\n",
					"    mssparkutils.fs.mount( \r\n",
					"        f\"abfss://{file_system_name}@{datalake_name}.dfs.core.windows.net\", \r\n",
					"        \"/mount\", \r\n",
					"        {\"linkedService\":f\"{linked_service_name}\"} \r\n",
					"    )\r\n",
					"    \r\n",
					"    # Returning the current job ID\r\n",
					"    JOB_ID = mssparkutils.env.getJobId()\r\n",
					"    return JOB_ID"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#primary\r\n",
					"def primary_keys_instructions(staged_instructions:dict, df_pk:pyspark.sql.dataframe.DataFrame) ->dict:\r\n",
					"    \"\"\"\r\n",
					"    Constructs a dictionary of primary key instructions for each table.\r\n",
					"\r\n",
					"    Parameters:\r\n",
					"    -----------\r\n",
					"    staged_instructions : dict\r\n",
					"        A dictionary containing the staged path for each table.\r\n",
					"    df_pk : pyspark.sql.dataframe.DataFrame\r\n",
					"        A DataFrame containing the primary keys for each table.\r\n",
					"\r\n",
					"    Returns:\r\n",
					"    --------\r\n",
					"    primary_key_statements_dict, primary_key_lookup (tuple)\r\n",
					"        A tuple of two dictionaries. The first dictionary contains the primary key instructions\r\n",
					"        for each table. The second dictionary contains the primary keys for each table.\r\n",
					"    \"\"\"\r\n",
					"    # Extract all primary keys\r\n",
					"    primary_key_lookup = {}\r\n",
					"    for row in df_pk.select(\"TABLE_NAME\", \"PRIMARY_KEYS\").distinct().collect():\r\n",
					"        primary_key_lookup[row.TABLE_NAME] = row.PRIMARY_KEYS.split(\", \")\r\n",
					"\r\n",
					"    primary_key_statements_dict = {}\r\n",
					"    for table, file_path in staged_instructions.items():\r\n",
					"        primary_keys = primary_key_lookup[table]\r\n",
					"\r\n",
					"        # Simplify string concatenation using f-strings\r\n",
					"        table_primary_key_statements = [f\"{table}_parquet.{pk} = {table}_delta.{pk}\".lower() for pk in primary_keys]\r\n",
					"\r\n",
					"        primary_key_statements_dict[table] = table_primary_key_statements\r\n",
					"\r\n",
					"    return primary_key_statements_dict, primary_key_lookup"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def deserialize_json_meta_to_notebook_etl_relational_meta(odbc_url:str, odbc_token:str) -> pyspark.sql.dataframe.DataFrame:\r\n",
					"    \"\"\"\r\n",
					"    Deserializes JSON metadata from a specified database using the provided parameters and returns two Spark dataframes. The first dataframe contains extracted object metadata, such as server and database names, table and column names, and file paths. The second dataframe contains information schema metadata, such as column names, data types, and character maximum lengths.\r\n",
					"\r\n",
					"    Parameters:\r\n",
					"    - odbc_hostname (str): the hostname or IP address of the SQL Server instance\r\n",
					"    - odbc_database (str): the name of the database to connect to\r\n",
					"    - odbc_port (str): the port number on which the SQL Server instance is listening\r\n",
					"    - odbc_options (str): any options to add to the connection\r\n",
					"    - odbc_token (str): the managed identity token to authenticate with\r\n",
					"\r\n",
					"    Returns:\r\n",
					"    pyspark.sql.dataframe.DataFrame: A tuple of two Spark dataframes. The first dataframe contains extracted object metadata, and the second dataframe contains information schema metadata.\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    # Define the schema for the JSON string\r\n",
					"    schema_adls_paths = \"array<struct<raw:array<struct<FP0:string,FP1:string>>,\" \\\r\n",
					"            \"staged:array<struct<FP0:string,FP1:string>>>>\"\r\n",
					"\r\n",
					"    schema_object_parameters = \"\"\"\r\n",
					"    array<\r\n",
					"    struct<\r\n",
					"        SOURCE_SYSTEM: string,\r\n",
					"        DATABASE_NAME: string,\r\n",
					"        SCHEMA_NAME: string,\r\n",
					"        TABLE_NAME: string,\r\n",
					"        SOURCE_SYSTEM_CONNECTION_STRING: string,\r\n",
					"        PRIMARY_KEYS: string,\r\n",
					"        VERSION_NUMBER: int,\r\n",
					"        COLUMNS_META: string,\r\n",
					"        OBJECT_LOAD_PARAMETERS: array<\r\n",
					"        struct<\r\n",
					"            MAX_SYS_CHANGE_VERSION: int\r\n",
					"        >\r\n",
					"        >,\r\n",
					"        SERVERLESS_SQL_POOL_DATABASE: string,\r\n",
					"        SERVERLESS_SQL_POOL_SCHEMA: string,\r\n",
					"        ETL_QUEUE_NAME: string,\r\n",
					"        INFORMATION_SCHEMA: array<\r\n",
					"        struct<\r\n",
					"            COLUMN_NAME: string,\r\n",
					"            IS_NULLABLE: string,\r\n",
					"            DATA_TYPE: string,\r\n",
					"            CHARACTER_MAXIMUM_LENGTH: bigint,\r\n",
					"            COLLATION_NAME: string\r\n",
					"        >\r\n",
					"        >\r\n",
					"    >\r\n",
					"    >\"\"\"\r\n",
					"\r\n",
					"    \r\n",
					"\r\n",
					"    pushdown_query_select_json_metadata = f\"EXEC [ETL].[usp_GetPipelineTriggerMetadataRetrievalFull] @PIPELINE_NAME = '{PARAM_PIPELINE_NAME}',@PIPELINE_TRIGGER_NAME = '{PARAM_TRIGGER_NAME}', @PIPELINE_CURRENT_DATETIME = '{PARAM_TRIGGER_TIME}'\"\r\n",
					"    df_json_metadata = query_db_odbc(odbc_url, odbc_token, pushdown_query_select_json_metadata)\r\n",
					"\r\n",
					"    # Parse the JSON string into a struct column\r\n",
					"    df = df_json_metadata.select(col(\"OBJECT_NAME\"), \\\r\n",
					"                                col(\"OBJECT_TYPE\"), \\\r\n",
					"                                col(\"PIPELINE_NAME\"),\\\r\n",
					"                                col(\"LOAD_TYPE\") ,\\\r\n",
					"                                from_json(col(\"ADLS_PATHS\"), schema_adls_paths).alias(\"ADLS_PATHS\"), \\\r\n",
					"                                from_json(col(\"OBJECT_PARAMETERS\"), schema_object_parameters).alias(\"OBJECT_PARAMETERS\"),\\\r\n",
					"                                col(\"HNS\"),\\\r\n",
					"                                col(\"TRIGGER_NAME\"))\r\n",
					"    # Extract the values of FP0 and FP1 from the raw array\r\n",
					"    df_objects_metadata_extracted = df.selectExpr(\"OBJECT_NAME\",\\\r\n",
					"                                \"OBJECT_PARAMETERS[0].SOURCE_SYSTEM AS SOURCE_SYSTEM\",\\\r\n",
					"                                \"OBJECT_PARAMETERS[0].DATABASE_NAME AS DATABASE_NAME\",\\\r\n",
					"                                \"OBJECT_PARAMETERS[0].SCHEMA_NAME AS SCHEMA_NAME\",\\\r\n",
					"                                \"OBJECT_PARAMETERS[0].TABLE_NAME AS TABLE_NAME\",\\\r\n",
					"                                \"OBJECT_PARAMETERS[0].SERVERLESS_SQL_POOL_DATABASE AS SERVERLESS_SQL_POOL_DATABASE\",\\\r\n",
					"                                \"OBJECT_PARAMETERS[0].SERVERLESS_SQL_POOL_SCHEMA AS SERVERLESS_SQL_POOL_SCHEMA\",\\\r\n",
					"                                \"OBJECT_PARAMETERS[0].ETL_QUEUE_NAME AS ETL_QUEUE_NAME\",\\\r\n",
					"                                \"TRIGGER_NAME\",\\\r\n",
					"                                \"PIPELINE_NAME\",\\\r\n",
					"                                \"LOAD_TYPE\",\\\r\n",
					"                                \"OBJECT_PARAMETERS[0].INFORMATION_SCHEMA AS INFORMATION_SCHEMA\",\\\r\n",
					"                                \"CASE WHEN LOAD_TYPE = 'FULL_LOAD' THEN NULL ELSE OBJECT_PARAMETERS[0].PRIMARY_KEYS END AS PRIMARY_KEYS \", \\\r\n",
					"                                \"ADLS_PATHS[0].raw[0].FP0 AS FP0_RAW \", \\\r\n",
					"                                \"concat(ADLS_PATHS.raw[0].FP1[0], HNS) as FP1_RAW\", \\\r\n",
					"                                \"ADLS_PATHS[0].staged[0].FP0 as FP0_STAGED\", \\\r\n",
					"                                \"ADLS_PATHS.staged[0].FP1[0] as FP1_STAGED\")\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"    from pyspark.sql.functions import explode,arrays_zip\r\n",
					"    df_info_schema = df.select(df.OBJECT_NAME,\\\r\n",
					"                                df.OBJECT_PARAMETERS.SOURCE_SYSTEM[0].alias(\"SOURCE_SYSTEM\"),\\\r\n",
					"                                df.OBJECT_PARAMETERS.DATABASE_NAME[0].alias(\"DATABASE_NAME\"),\\\r\n",
					"                                df.OBJECT_PARAMETERS.SCHEMA_NAME[0].alias(\"SCHEMA_NAME\"),\\\r\n",
					"                                df.OBJECT_PARAMETERS.TABLE_NAME[0].alias(\"TABLE_NAME\"),\\\r\n",
					"                                explode(df.OBJECT_PARAMETERS.INFORMATION_SCHEMA[0]).alias(\"COLUMN_INFO\"))\r\n",
					"\r\n",
					"    df_info_schema = df_info_schema.select(\r\n",
					"                                        col(\"OBJECT_NAME\"),\\\r\n",
					"                                        col(\"SOURCE_SYSTEM\"),\\\r\n",
					"                                        col(\"DATABASE_NAME\"),\\\r\n",
					"                                        col(\"SCHEMA_NAME\"),\\\r\n",
					"                                        col(\"TABLE_NAME\"),\\\r\n",
					"                                        col(\"COLUMN_INFO.COLUMN_NAME\"), \\\r\n",
					"                                        col(\"COLUMN_INFO.DATA_TYPE\"), \\\r\n",
					"                                        col(\"COLUMN_INFO.CHARACTER_MAXIMUM_LENGTH\"),\\\r\n",
					"                                        col(\"COLUMN_INFO.IS_NULLABLE\"))\r\n",
					"    return df_objects_metadata_extracted, df_info_schema\r\n",
					"    #display(df_info_schema)\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#secondary\r\n",
					"def create_query_successful_etl_log_CT(trigger_name:str, trigger_time:str, trigger_type:str, pipeline_run_id:str, pipeline_name:str, notebook_name:str, source_system_path:str, target_system_path:str, schema_name:str, table:str, file_type:str, operation:str, start_time:datetime, inserts,updates) -> str:\r\n",
					"    \"\"\"Creates a SQL query to insert a log of a successful ETL operation into a database.\r\n",
					"\r\n",
					"    Parameters:\r\n",
					"        trigger_name (str): The name of the trigger that started the ETL operation.\r\n",
					"        trigger_time (datetime): The date and time the ETL operation was triggered.\r\n",
					"        trigger_type (str): The type of trigger that started the ETL operation.\r\n",
					"        pipeline_run_id (str): The unique ID of the pipeline run that triggered the ETL operation.\r\n",
					"        pipeline_name (str): The name of the pipeline that triggered the ETL operation.\r\n",
					"        notebook_name (str): The name of the notebook or script that performed the ETL operation.\r\n",
					"        source_system_path (str): The path to the source system data.\r\n",
					"        target_system_path (str): The path to the target system data.\r\n",
					"        schema_name (str): The name of the schema containing the target table.\r\n",
					"        table (str): The name of the target table.\r\n",
					"        file_type (str): The type of file used for the target system data.\r\n",
					"        operation (str): The type of operation performed on the target system data.\r\n",
					"        start_time (datetime): The date and time the ETL operation started.\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        pushdown_query (str): A SQL query to insert a log of the ETL operation into a database.\r\n",
					"\r\n",
					"    \"\"\"\r\n",
					"    if file_type == 'DELTA':\r\n",
					"        if operation == 'WRITE':\r\n",
					"            end_time = datetime.utcnow()\r\n",
					"            deltaTime = end_time-start_time\r\n",
					"            deltaTime = deltaTime.total_seconds()\r\n",
					"            deltaTime = int(deltaTime)\r\n",
					"            delta_table = DeltaTable.forPath(spark, f\"{staged_instructions[table]['STAGED_PATH']}\")\r\n",
					"            lastOperationDF = delta_table.history(1)\r\n",
					"            operation_metrics = lastOperationDF.select(F.explode(lastOperationDF.operationMetrics))\r\n",
					"            numOutputRows = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numOutputRows')\r\n",
					"            numOutputRows = numOutputRows.rdd.flatMap(lambda x:x).collect()[0]\r\n",
					"\r\n",
					"            pushdown_query = f\"\"\"EXEC [ETL].[usp_InsertLog] \r\n",
					"                                @TRIGGER_NAME = '{trigger_name}'\r\n",
					"                                ,@TRIGGER_TIME = '{trigger_time}'\r\n",
					"                                ,@TRIGGER_TYPE = '{trigger_type}'\r\n",
					"                                ,@PIPELINE_RUN_ID = '{pipeline_run_id}'\r\n",
					"                                ,@PIPELINE_NAME = '{pipeline_name}'\r\n",
					"                                ,@COMPONENT_NAME = '{notebook_name}'\r\n",
					"                                ,@SOURCE_SYSTEM = '{source_system_path}'\r\n",
					"                                ,@TARGET_SYSTEM = '{target_system_path}'\r\n",
					"                                ,@SCHEMA_NAME = '{schema_name}'\r\n",
					"                                ,@TABLE_NAME = '{table}'\r\n",
					"                                ,@OPERATION = '{operation}'\r\n",
					"                                ,@START_TIME = '{start_time}'\r\n",
					"                                ,@END_TIME =  '{end_time}'\r\n",
					"                                ,@DURATION_SECONDS = {deltaTime}\r\n",
					"                                ,@INSERTS = {numOutputRows}\r\n",
					"                                ,@UPDATES = 0\r\n",
					"                                ,@DELETES = 0\r\n",
					"                                ,@ERROR_MESSAGE = NULL \"\"\"\r\n",
					"            return pushdown_query\r\n",
					"        elif operation == 'MERGE':\r\n",
					"            end_time = datetime.utcnow()\r\n",
					"            deltaTime = end_time-start_time\r\n",
					"            deltaTime = deltaTime.total_seconds()\r\n",
					"            deltaTime = int(deltaTime)\r\n",
					"            delta_table = DeltaTable.forPath(spark, f\"{staged_instructions[table]['STAGED_PATH']}\")\r\n",
					"            lastMergeOperationDF = delta_table.history(2)\r\n",
					"            operation_metrics = lastMergeOperationDF.select(F.explode(lastMergeOperationDF.operationMetrics))\r\n",
					"            # numOutputRows = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numOutputRows')\r\n",
					"            # numOutputRows = numOutputRows.rdd.flatMap(lambda x:x).collect()[0]\r\n",
					"            numRowsInserted = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numTargetRowsInserted').rdd.flatMap(lambda x:x).collect()[0]\r\n",
					"            try:\r\n",
					"                numRowsDeleted = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numTargetRowsDeleted').rdd.flatMap(lambda x:x).collect()[0]\r\n",
					"            except IndexError: \r\n",
					"                numRowsDeleted = 0\r\n",
					"            numRowsUpdated = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numTargetRowsUpdated').rdd.flatMap(lambda x:x).collect()[0]\r\n",
					"            pushdown_query = f\"\"\"EXEC [ETL].[usp_InsertLog] \r\n",
					"                        @TRIGGER_NAME = '{trigger_name}'\r\n",
					"                        ,@TRIGGER_TIME = '{trigger_time}'\r\n",
					"                        ,@TRIGGER_TYPE = '{trigger_type}'\r\n",
					"                        ,@PIPELINE_RUN_ID = '{pipeline_run_id}'\r\n",
					"                        ,@PIPELINE_NAME = '{pipeline_name}'\r\n",
					"                        ,@COMPONENT_NAME = '{notebook_name}'\r\n",
					"                        ,@SOURCE_SYSTEM = '{source_system_path}'\r\n",
					"                        ,@TARGET_SYSTEM = '{target_system_path}'\r\n",
					"                        ,@SCHEMA_NAME = '{schema_name}'\r\n",
					"                        ,@TABLE_NAME = '{table}'\r\n",
					"                        ,@OPERATION = '{operation}'\r\n",
					"                        ,@START_TIME = '{start_time}'\r\n",
					"                        ,@END_TIME =  '{end_time}'\r\n",
					"                        ,@DURATION_SECONDS = {deltaTime}\r\n",
					"                        ,@INSERTS = {inserts}\r\n",
					"                        ,@UPDATES = {updates}\r\n",
					"                        ,@DELETES = {numRowsDeleted}\r\n",
					"                        ,@ERROR_MESSAGE = NULL \"\"\"\r\n",
					"            return pushdown_query\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"def create_query_successful_etl_log(trigger_name:str, trigger_time:str, trigger_type:str, pipeline_run_id:str, pipeline_name:str, notebook_name:str, source_system_path:str, target_system_path:str, schema_name:str, table:str, file_type:str, operation:str, start_time:datetime, merge_type:str) -> str:\r\n",
					"    \"\"\"Creates a SQL query to insert a log of a successful ETL operation into a database.\r\n",
					"\r\n",
					"    Parameters:\r\n",
					"        trigger_name (str): The name of the trigger that started the ETL operation.\r\n",
					"        trigger_time (datetime): The date and time the ETL operation was triggered.\r\n",
					"        trigger_type (str): The type of trigger that started the ETL operation.\r\n",
					"        pipeline_run_id (str): The unique ID of the pipeline run that triggered the ETL operation.\r\n",
					"        pipeline_name (str): The name of the pipeline that triggered the ETL operation.\r\n",
					"        notebook_name (str): The name of the notebook or script that performed the ETL operation.\r\n",
					"        source_system_path (str): The path to the source system data.\r\n",
					"        target_system_path (str): The path to the target system data.\r\n",
					"        schema_name (str): The name of the schema containing the target table.\r\n",
					"        table (str): The name of the target table.\r\n",
					"        file_type (str): The type of file used for the target system data.\r\n",
					"        operation (str): The type of operation performed on the target system data.\r\n",
					"        start_time (datetime): The date and time the ETL operation started.\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        pushdown_query (str): A SQL query to insert a log of the ETL operation into a database.\r\n",
					"\r\n",
					"    \"\"\"\r\n",
					"    if file_type == 'DELTA':\r\n",
					"        if operation == 'WRITE':\r\n",
					"            end_time = datetime.utcnow()\r\n",
					"            deltaTime = end_time-start_time\r\n",
					"            deltaTime = deltaTime.total_seconds()\r\n",
					"            deltaTime = int(deltaTime)\r\n",
					"            delta_table = DeltaTable.forPath(spark, f\"{staged_instructions[table]['STAGED_PATH']}\")\r\n",
					"            lastOperationDF = delta_table.history(1)\r\n",
					"            operation_metrics = lastOperationDF.select(F.explode(lastOperationDF.operationMetrics))\r\n",
					"            numOutputRows = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numOutputRows')\r\n",
					"            numOutputRows = numOutputRows.rdd.flatMap(lambda x:x).collect()[0]\r\n",
					"\r\n",
					"            pushdown_query = f\"\"\"EXEC [ETL].[usp_InsertLog] \r\n",
					"                                @TRIGGER_NAME = '{trigger_name}'\r\n",
					"                                ,@TRIGGER_TIME = '{trigger_time}'\r\n",
					"                                ,@TRIGGER_TYPE = '{trigger_type}'\r\n",
					"                                ,@PIPELINE_RUN_ID = '{pipeline_run_id}'\r\n",
					"                                ,@PIPELINE_NAME = '{pipeline_name}'\r\n",
					"                                ,@COMPONENT_NAME = '{notebook_name}'\r\n",
					"                                ,@SOURCE_SYSTEM = '{source_system_path}'\r\n",
					"                                ,@TARGET_SYSTEM = '{target_system_path}'\r\n",
					"                                ,@SCHEMA_NAME = '{schema_name}'\r\n",
					"                                ,@TABLE_NAME = '{table}'\r\n",
					"                                ,@OPERATION = '{operation}'\r\n",
					"                                ,@START_TIME = '{start_time}'\r\n",
					"                                ,@END_TIME =  '{end_time}'\r\n",
					"                                ,@DURATION_SECONDS = {deltaTime}\r\n",
					"                                ,@INSERTS = {numOutputRows}\r\n",
					"                                ,@UPDATES = 0\r\n",
					"                                ,@DELETES = 0\r\n",
					"                                ,@ERROR_MESSAGE = NULL \"\"\"\r\n",
					"            return pushdown_query\r\n",
					"        elif operation == 'MERGE':\r\n",
					"            end_time = datetime.utcnow()\r\n",
					"            deltaTime = end_time-start_time\r\n",
					"            deltaTime = deltaTime.total_seconds()\r\n",
					"            deltaTime = int(deltaTime)\r\n",
					"            delta_table = DeltaTable.forPath(spark, f\"{staged_instructions[table]['STAGED_PATH']}\")\r\n",
					"            lastMergeOperationDF = delta_table.history(2)\r\n",
					"            operation_metrics = lastMergeOperationDF.select(F.explode(lastMergeOperationDF.operationMetrics), lastMergeOperationDF.version)\r\n",
					"            if merge_type == 'CHANGE_TRACKING':\r\n",
					"                numRowsInserted = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numTargetRowsInserted').rdd.flatMap(lambda x:x).collect()[1]\r\n",
					"                try:\r\n",
					"                    numRowsDeleted = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numTargetRowsDeleted').rdd.flatMap(lambda x:x).collect()[0]\r\n",
					"                except IndexError: \r\n",
					"                    numRowsDeleted = 0\r\n",
					"                numRowsUpdated = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numTargetRowsUpdated').rdd.flatMap(lambda x:x).collect()[1]\r\n",
					"            else:\r\n",
					"                numRowsInserted = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numTargetRowsInserted').rdd.flatMap(lambda x:x).collect()[0]\r\n",
					"                numRowsDeleted = 0\r\n",
					"                numRowsUpdated = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numTargetRowsUpdated').rdd.flatMap(lambda x:x).collect()[0]\r\n",
					"\r\n",
					"            pushdown_query = f\"\"\"EXEC [ETL].[usp_InsertLog] \r\n",
					"                        @TRIGGER_NAME = '{trigger_name}'\r\n",
					"                        ,@TRIGGER_TIME = '{trigger_time}'\r\n",
					"                        ,@TRIGGER_TYPE = '{trigger_type}'\r\n",
					"                        ,@PIPELINE_RUN_ID = '{pipeline_run_id}'\r\n",
					"                        ,@PIPELINE_NAME = '{pipeline_name}'\r\n",
					"                        , @COMPONENT_NAME = '{notebook_name}'\r\n",
					"                        , @SOURCE_SYSTEM = '{source_system_path}'\r\n",
					"                        , @TARGET_SYSTEM = '{target_system_path}'\r\n",
					"                        , @SCHEMA_NAME = '{schema_name}'\r\n",
					"                        , @TABLE_NAME = '{table}'\r\n",
					"                        , @OPERATION = '{operation}'\r\n",
					"                        , @START_TIME = '{start_time}'\r\n",
					"                        , @END_TIME =  '{end_time}'\r\n",
					"                        , @DURATION_SECONDS = {deltaTime}\r\n",
					"                        , @INSERTS = {numRowsInserted}\r\n",
					"                        , @UPDATES = {numRowsUpdated}\r\n",
					"                        , @DELETES = {numRowsDeleted}\r\n",
					"                        , @ERROR_MESSAGE = NULL \"\"\"\r\n",
					"            return pushdown_query\r\n",
					"    \r\n",
					"    elif file_type == 'PARQUET':\r\n",
					"        if operation == 'WRITE':\r\n",
					"            # Gathering Successful WRITE use-case specific parameters for the Stored Procedure string\r\n",
					"            end_time = datetime.utcnow()\r\n",
					"            deltaTime = end_time-start_time\r\n",
					"            deltaTime = deltaTime.total_seconds()\r\n",
					"            deltaTime = int(deltaTime)\r\n",
					"            parquet_file = spark.read.parquet(f\"{target_system_path}\")\r\n",
					"            numOutputRows = parquet_file.count()\r\n",
					"\r\n",
					"            # Passing the parameters to the stored procedure string\r\n",
					"            pushdown_query = f\"\"\"EXEC [ETL].[usp_InsertLog] \r\n",
					"                                     @TRIGGER_NAME = '{trigger_name}'\r\n",
					"                                ,@TRIGGER_TIME = '{trigger_time}'\r\n",
					"                                ,@TRIGGER_TYPE = '{trigger_type}'\r\n",
					"                                ,@PARAM_PIPELINE_RUN_ID = '{pipeline_run_id}'\r\n",
					"                                ,@PIPELINE_NAME = '{pipeline_name}'\r\n",
					"                                , @COMPONENT_NAME = '{notebook_name}'\r\n",
					"                                , @SOURCE_SYSTEM = '{source_system_path}'\r\n",
					"                                , @TARGET_SYSTEM = '{target_system_path}'\r\n",
					"                                , @SCHEMA_NAME = '{schema_name}'\r\n",
					"                                , @TABLE_NAME = '{table}'\r\n",
					"                                , @OPERATION = '{operation}'\r\n",
					"                                , @START_TIME = '{start_time}'\r\n",
					"                                , @END_TIME =  '{end_time}'\r\n",
					"                                , @DURATION_SECONDS = {deltaTime}\r\n",
					"                                , @INSERTS = {numOutputRows}\r\n",
					"                                , @UPDATES = 0\r\n",
					"                                , @DELETES = 0\r\n",
					"                                , @ERROR_MESSAGE = NULL \"\"\"\r\n",
					"\r\n",
					"            # Returning the query as a string\r\n",
					"            return pushdown_query\r\n",
					"\r\n",
					"#secondary\r\n",
					"def create_query_failed_etl_log(trigger_name:str, trigger_time:str, trigger_type:str, pipeline_run_id:str, pipeline_name:str,notebook_name:str,source_system_path:str,target_system_path:str,schema_name:str, table:str, file_type:str, operation:str,start_time:datetime, error_msg:str)->str:\r\n",
					"    \"\"\"\r\n",
					"    Creates a stored procedure query to insert ETL log information into the database.\r\n",
					"\r\n",
					"    Parameters:\r\n",
					"        trigger_name (str): The name of the trigger that caused the ETL process.\r\n",
					"        trigger_time (str): The time at which the trigger occurred.\r\n",
					"        trigger_type (str): The type of the trigger that caused the ETL process.\r\n",
					"        pipeline_run_id (str): The ID of the pipeline run.\r\n",
					"        pipeline_name (str): The name of the pipeline that was executed.\r\n",
					"        notebook_name (str): The name of the notebook that ran the ETL process.\r\n",
					"        source_system_path (str): The path to the source system.\r\n",
					"        target_system_path (str): The path to the target system.\r\n",
					"        schema_name (str): The name of the schema in which the table resides.\r\n",
					"        table (str): The name of the table that was processed.\r\n",
					"        file_type (str): The type of file that was processed (e.g. PARQUET, DELTA).\r\n",
					"        operation (str): The type of operation that was performed (e.g. WRITE, MERGE).\r\n",
					"        start_time (datetime.datetime): The time at which the ETL process began.\r\n",
					"        error_msg (str): The error message that was encountered during the ETL process.\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        pushdown_query (str): The stored procedure query as a string.\r\n",
					"    \"\"\"\r\n",
					"    if file_type == 'DELTA':\r\n",
					"        if operation == 'WRITE':\r\n",
					"            end_time = datetime.utcnow()\r\n",
					"            deltaTime = end_time-start_time\r\n",
					"            deltaTime = deltaTime.total_seconds()\r\n",
					"            deltaTime = int(deltaTime)\r\n",
					"\r\n",
					"            pushdown_query = f\"\"\"EXEC [ETL].[usp_InsertLog] \r\n",
					"                                @TRIGGER_NAME = '{trigger_name}'\r\n",
					"                                ,@TRIGGER_TIME = '{trigger_time}'\r\n",
					"                                ,@TRIGGER_TYPE = '{trigger_type}'\r\n",
					"                                ,@PIPELINE_RUN_ID = '{pipeline_run_id}'\r\n",
					"                                ,@PIPELINE_NAME = '{pipeline_name}'\r\n",
					"                                ,@COMPONENT_NAME = '{notebook_name}'\r\n",
					"                                ,@SOURCE_SYSTEM = '{source_system_path}'\r\n",
					"                                ,@TARGET_SYSTEM = '{target_system_path}'\r\n",
					"                                ,@SCHEMA_NAME = '{schema_name}'\r\n",
					"                                ,@TABLE_NAME = '{table}'\r\n",
					"                                ,@OPERATION = '{operation}'\r\n",
					"                                ,@START_TIME = '{start_time}'\r\n",
					"                                ,@END_TIME =  '{end_time}'\r\n",
					"                                ,@DURATION_SECONDS = {deltaTime}\r\n",
					"                                ,@INSERTS = 0\r\n",
					"                                ,@UPDATES = 0\r\n",
					"                                ,@DELETES = 0\r\n",
					"                                ,@ERROR_MESSAGE = '{error_msg}' \"\"\"\r\n",
					"            #return pushdown_query\r\n",
					"        elif operation == 'MERGE':\r\n",
					"            end_time = datetime.utcnow()\r\n",
					"            deltaTime = end_time-start_time\r\n",
					"            \r\n",
					"            deltaTime = deltaTime.total_seconds()\r\n",
					"            deltaTime = int(deltaTime)\r\n",
					"            \r\n",
					"\r\n",
					"\r\n",
					"            pushdown_query = f\"\"\"EXEC [ETL].[usp_InsertLog] \r\n",
					"                                @TRIGGER_NAME = '{trigger_name}'\r\n",
					"                                ,@TRIGGER_TIME = '{trigger_time}'\r\n",
					"                                ,@TRIGGER_TYPE = '{trigger_type}'\r\n",
					"                                ,@PIPELINE_RUN_ID = '{pipeline_run_id}'\r\n",
					"                                ,@PIPELINE_NAME = '{pipeline_name}'\r\n",
					"                                ,@COMPONENT_NAME = '{notebook_name}'\r\n",
					"                                ,@SOURCE_SYSTEM = '{source_system_path}'\r\n",
					"                                ,@TARGET_SYSTEM = '{target_system_path}'\r\n",
					"                                ,@SCHEMA_NAME = '{schema_name}'\r\n",
					"                                ,@TABLE_NAME = '{table}'\r\n",
					"                                ,@OPERATION = '{operation}'\r\n",
					"                                ,@START_TIME = '{start_time}'\r\n",
					"                                ,@END_TIME =  '{end_time}'\r\n",
					"                                ,@DURATION_SECONDS = {deltaTime}\r\n",
					"                                ,@INSERTS = 0\r\n",
					"                                ,@UPDATES = 0\r\n",
					"                                ,@DELETES = 0\r\n",
					"                                ,@ERROR_MESSAGE = '{error_msg}' \"\"\"\r\n",
					"            return pushdown_query\r\n",
					"\r\n",
					"    elif file_type == 'PARQUET':\r\n",
					"        if operation == 'WRITE':\r\n",
					"            # Gathering Successful WRITE use-case specific parameters for the Stored Procedure string\r\n",
					"            end_time = datetime.utcnow()\r\n",
					"            deltaTime = end_time-start_time\r\n",
					"            deltaTime = deltaTime.total_seconds()\r\n",
					"            deltaTime = int(deltaTime)\r\n",
					"            parquet_file = spark.read.parquet(f\"{target_system_path}\")\r\n",
					"            numOutputRows = parquet_file.count()\r\n",
					"\r\n",
					"            # Passing the parameters to the stored procedure string\r\n",
					"            pushdown_query = f\"\"\"EXEC [ETL].[usp_InsertLog]  \r\n",
					"                                     @TRIGGER_NAME = '{trigger_name}'\r\n",
					"                                ,@TRIGGER_TIME = '{trigger_time}'\r\n",
					"                                ,@TRIGGER_TYPE = '{trigger_type}'\r\n",
					"                                ,@PARAM_PIPELINE_RUN_ID = '{pipeline_run_id}'\r\n",
					"                                ,@PIPELINE_NAME = '{pipeline_name}'\r\n",
					"                                , @COMPONENT_NAME = '{notebook_name}'\r\n",
					"                                , @SOURCE_SYSTEM = '{source_system_path}'\r\n",
					"                                , @TARGET_SYSTEM = '{target_system_path}'\r\n",
					"                                , @SCHEMA_NAME = '{schema_name}'\r\n",
					"                                , @TABLE_NAME = '{table}'\r\n",
					"                                , @OPERATION = '{operation}'\r\n",
					"                                , @START_TIME = '{start_time}'\r\n",
					"                                , @END_TIME =  '{end_time}'\r\n",
					"                                , @DURATION_SECONDS = {deltaTime}\r\n",
					"                                , @INSERTS = 0\r\n",
					"                                , @UPDATES = 0\r\n",
					"                                , @DELETES = 0\r\n",
					"                                , @ERROR_MESSAGE = NULL \"\"\"\r\n",
					"\r\n",
					"            # Returning the query as a string\r\n",
					"            return pushdown_query"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def run_tasks(function, q):\r\n",
					"    while not q.empty():\r\n",
					"        value = q.get()\r\n",
					"        function(value)\r\n",
					"        q.task_done()\r\n",
					"    q.task_done()\r\n",
					"    print(\"Tables Migrated\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def raw_to_staged_etl_plus_logging(table:str, storage_queue:str, trigger_name:str, trigger_time:str, trigger_type:str, pipeline_run_id:str, staged_instructions:dict):\r\n",
					"    \"\"\"\r\n",
					"    This function extracts data from a RAW source, transforms the data, and writes/merges it to a Delta table in Staged. \r\n",
					"\r\n",
					"    Parameters:\r\n",
					"    - trigger_name (str): The name of the event or trigger that initiated the function.\r\n",
					"    - trigger_time (str): The time at which the function was triggered.\r\n",
					"    - trigger_type (str): The type of trigger that initiated the function (e.g. event-based, scheduled, manual).\r\n",
					"    - pipeline_run_id (str): A unique ID for the current pipeline run.\r\n",
					"    - migrated_to_raw (list): A list of table names that have already been migrated from the RAW layer to the Staged layer.\r\n",
					"    - migrated_to_staged (list): A list of table names that have already been migrated from the Staged layer to the Curated layer.\r\n",
					"    - staged_existing_files (list): A list of table names that already exist in the Staged layer.\r\n",
					"    - dimensions_list (list): A list of table names that are dimensions.\r\n",
					"    - staged_instructions (dict): A dictionary where keys are table names and values are the file paths in the Staged layer.\r\n",
					"\r\n",
					"    Returns:\r\n",
					"    - None\r\n",
					"    \"\"\"\r\n",
					"    \r\n",
					"    staged_failures = []\r\n",
					"    date_now = datetime.utcnow().date()\r\n",
					"\r\n",
					"    # Reading the RAW parquet file into a DataFrame\r\n",
					"    df_to_merge = spark.read.parquet(f\"{staged_instructions[table]['RAW_PATH']}\")\r\n",
					"    schema = f\"{staged_instructions[table]['SCHEMA_NAME']}\"\r\n",
					"    pipeline_name = f\"{staged_instructions[table]['PIPELINE_NAME']}\"\r\n",
					"    load_type = f\"{staged_instructions[table]['LOAD_TYPE']}\"\r\n",
					"    serverless_sql_database = f\"{staged_instructions[table]['SERVERLESS_SQL_POOL_DATABASE']}\"\r\n",
					"    serverless_sql_schema = f\"{staged_instructions[table]['SERVERLESS_SQL_POOL_SCHEMA']}\"\r\n",
					"    JOB_ID = mssparkutils.env.getJobId()\r\n",
					"    staged_root_path = f\"synfs:/{JOB_ID}/mount/{staged_instructions[table]['SERVERLESS_SQL_POOL_DATABASE']}/{staged_instructions[table]['SERVERLESS_SQL_POOL_SCHEMA']}\" #'synfs:/125/mount/synw_data_uks_dna_lakehouse/IHHO001/ab_asbestos_dtl/'},\r\n",
					"    staged_existing_files = []\r\n",
					"    try:\r\n",
					"        staged_files_list = mssparkutils.fs.ls(staged_root_path)\r\n",
					"        for file in staged_files_list:\r\n",
					"            staged_existing_files.append(file.name)\r\n",
					"    except Exception as e:\r\n",
					"        staged_existing_files = []\r\n",
					"\r\n",
					"        #logging.info(\"The path doesn't exist - meaning there are no existing staged files corresponding to this datasource.\")\r\n",
					"\r\n",
					"\r\n",
					"    #If the file from the current run hasn't been migrated (written/merged) to staged:\r\n",
					"        #If the file doesn't already exist in staged: WRITE\r\n",
					"    if table not in staged_existing_files or load_type == 'FULL_LOAD':\r\n",
					"\r\n",
					"        start_time = datetime.utcnow()\r\n",
					"        try:\r\n",
					"            # Try WRITE FACT on ETLDate \r\n",
					"\r\n",
					"            # vv Workaround if there is no ModifiedDate present in the file to assist MERGE operations. vv\r\n",
					"            #           df_to_merge= df_to_merge.withColumn('ETLDate',lit(date_now))\r\n",
					"\r\n",
					"            df_to_merge.write\\\r\n",
					"                .mode(\"overwrite\")\\\r\n",
					"                .format(\"delta\")\\\r\n",
					"                .option(\"overwriteSchema\", \"true\")\\\r\n",
					"                .save(f\"{staged_instructions[table]['STAGED_PATH']}\")\r\n",
					"            \r\n",
					"\r\n",
					"            \r\n",
					"            # Log successful ETL WRITE for FACT\r\n",
					"            pushdown_query_success= create_query_successful_etl_log(trigger_name = trigger_name, \\\r\n",
					"                                                trigger_time= trigger_time,\\\r\n",
					"                                                trigger_type = trigger_type,\\\r\n",
					"                                                pipeline_run_id= pipeline_run_id ,\\\r\n",
					"                                                pipeline_name= pipeline_name, \\\r\n",
					"                                                notebook_name= 'Lakehouse Orchestration', \\\r\n",
					"                                                source_system_path = f\"{staged_instructions[table]['RAW_PATH']}\",\\\r\n",
					"                                                target_system_path = f\"{staged_instructions[table]['STAGED_PATH']}\", \\\r\n",
					"                                                schema_name = f\"{staged_instructions[table]['SCHEMA_NAME']}\", \\\r\n",
					"                                                table = f\"{staged_instructions[table]['TABLE_NAME']}\", \\\r\n",
					"                                                file_type = 'DELTA',\\\r\n",
					"                                                operation = 'WRITE',\\\r\n",
					"                                                start_time = start_time,\\\r\n",
					"                                                merge_type = 'N/A')\r\n",
					"\r\n",
					"            cursor_etl_sql.execute(pushdown_query_success)\r\n",
					"            #queue_client.update_message(table, pop_receipt=pop_receipt, visibility_timeout=3600) #delete_message(table, pop_receipt=pop_receipt)\r\n",
					"\r\n",
					"\r\n",
					"        # Log Failed ETL WRITE for FACT\r\n",
					"        except Exception as err_message:\r\n",
					"            pushdown_query_fail = create_query_failed_etl_log(trigger_name = trigger_name, \\\r\n",
					"                                                            trigger_time= trigger_time,\\\r\n",
					"                                                            trigger_type = trigger_type,\\\r\n",
					"                                                            pipeline_run_id= pipeline_run_id ,\\\r\n",
					"                                                            pipeline_name = pipeline_name, \\\r\n",
					"                                                            notebook_name = 'Lakehouse Orchestration', \\\r\n",
					"                                                            source_system_path = f\"{staged_instructions[table]['RAW_PATH']}\", \\\r\n",
					"                                                            target_system_path = f\"{staged_instructions[table]['STAGED_PATH']}\", \\\r\n",
					"                                                            schema_name = f\"{staged_instructions[table]['SCHEMA_NAME']}\", \\\r\n",
					"                                                            table = f\"{staged_instructions[table]['TABLE_NAME']}\", \\\r\n",
					"                                                            file_type = 'DELTA',\\\r\n",
					"                                                            operation = 'WRITE', \\\r\n",
					"                                                            start_time = start_time, \\\r\n",
					"                                                            error_msg = err_message)\r\n",
					"            #print(pushdown_query_fail)\r\n",
					"            cursor_etl_sql.execute(pushdown_query_fail)\r\n",
					"            staged_failures.append(table)\r\n",
					"            raise err_message\r\n",
					"            #logging.info(table + 'has failed as FULL_LOAD')\r\n",
					"            #logging.info(err_message)\r\n",
					"            #logging.error('***' + table + 'has failed as FULL_LOAD ***')\r\n",
					"\r\n",
					"        \r\n",
					"\r\n",
					"\r\n",
					"    #Else if the file already exist in staged: MERGE\r\n",
					"    elif table in staged_existing_files:\r\n",
					"        \r\n",
					"        delta_table = DeltaTable.forPath(spark, f\"{staged_instructions[table]['STAGED_PATH']}\")\r\n",
					"        \r\n",
					"        # Get the Primary Key statements for the specific table\r\n",
					"        primary_key_statement = staged_instructions[table]['PRIMARY_KEY_STATEMENTS']\r\n",
					"        #print(primary_key_statement)\r\n",
					"        # Record start_date for the stored procedure\r\n",
					"        start_time = datetime.utcnow()\r\n",
					"        primary_key = staged_instructions[table]['PRIMARY_KEYS'].lower()\r\n",
					"        #df_to_merge = df_to_merge.dropDuplicates(primary_keys[table],'last')\r\n",
					"        try:\r\n",
					"            if load_type == 'CHANGE_TRACKING':\r\n",
					"                try:\r\n",
					"                    df_deletes = df_to_merge.filter(df_to_merge['SYS_CHANGE_OPERATION'] == 'D').select(df_to_merge.PK_VALUE.alias(f\"{primary_key}\"))\r\n",
					"                    df_to_merge = df_to_merge.filter(df_to_merge['SYS_CHANGE_OPERATION'] != 'D').drop('PK_VALUE', 'SYS_CHANGE_OPERATION')\r\n",
					"                    delta_table.alias(f\"{table}_delta\").merge(\r\n",
					"                    df_to_merge.alias(f\"{table}_parquet\"),\r\n",
					"                    f\"{primary_key_statement}\")\\\r\n",
					"                    .whenNotMatchedInsertAll()\\\r\n",
					"                    .whenMatchedUpdateAll()\\\r\n",
					"                    .execute()\r\n",
					"                except Exception as e:\r\n",
					"                    raise e\r\n",
					"                    #logging.info(\"Attempting to delete records failed\")\r\n",
					"                    #logging.error(f\" *** {table} has failed while attempting to merge using CHANGE TRACKING ***\")\r\n",
					"                try:\r\n",
					"                    delta_table.alias(\"target\").merge(\r\n",
					"                    df_deletes.alias(\"source\"),\r\n",
					"                    f\"source.{primary_key} = target.{primary_key}\")\\\r\n",
					"                    .whenMatchedDelete()\\\r\n",
					"                    .execute() \r\n",
					"                except Exception as e:\r\n",
					"                    #logging.info(\"Attempting to delete records failed\")\r\n",
					"                    #logging.error(f\" *** {table} has failed while attempting to delete using CHANGE TRACKING ***\")\r\n",
					"                    raise e\r\n",
					"            else:\r\n",
					"                try:\r\n",
					"                    # Try to do a merge on primary key for dimension\r\n",
					"                    delta_table.alias(f\"{table}_delta\").merge(\r\n",
					"                    df_to_merge.alias(f\"{table}_parquet\"),\r\n",
					"                    f\"{primary_key_statement}\")\\\r\n",
					"                    .whenNotMatchedInsertAll()\\\r\n",
					"                    .whenMatchedUpdateAll()\\\r\n",
					"                    .execute()\r\n",
					"                except Exception as e:\r\n",
					"                    #logging.info(\"Attempting to delete records failed\")\r\n",
					"                    #logging.error(f\" *** {table} has failed while attempting to merge normally***\")\r\n",
					"                    raise e\r\n",
					"\r\n",
					"            # Log Successful ETL MERGE for FACT\r\n",
					"            pushdown_query_success= create_query_successful_etl_log(trigger_name = trigger_name, \\\r\n",
					"                                                trigger_time= trigger_time,\\\r\n",
					"                                                trigger_type = trigger_type,\\\r\n",
					"                                                pipeline_run_id= pipeline_run_id ,\\\r\n",
					"                                                pipeline_name= pipeline_name, \\\r\n",
					"                                                notebook_name= 'Lakehouse Orchestration', \\\r\n",
					"                                                source_system_path =f\"{staged_instructions[table]['RAW_PATH']}\", \\\r\n",
					"                                                target_system_path = f\"{staged_instructions[table]['STAGED_PATH']}\", \\\r\n",
					"                                                schema_name = f\"{staged_instructions[table]['SCHEMA_NAME']}\",\\\r\n",
					"                                                table = f\"{staged_instructions[table]['TABLE_NAME']}\",\\\r\n",
					"                                                file_type = 'DELTA',\\\r\n",
					"                                                operation = 'MERGE',\\\r\n",
					"                                                start_time = start_time,\\\r\n",
					"                                                merge_type= load_type)\r\n",
					"            cursor_etl_sql.execute(pushdown_query_success)            \r\n",
					"\r\n",
					"\r\n",
					"            #queue_client.update_message(table, pop_receipt=pop_receipt, visibility_timeout=3600)#queue_client.delete_message(table, pop_receipt=pop_receipt) #delete_message(table, pop_receipt=pop_receipt)\r\n",
					"\r\n",
					"        # Log failed ETL MERGE for FACT\r\n",
					"        except Exception as err_message:\r\n",
					"            pushdown_query_fail = create_query_failed_etl_log(trigger_name = trigger_name, \\\r\n",
					"                                                            trigger_time= trigger_time,\\\r\n",
					"                                                            trigger_type = trigger_type,\\\r\n",
					"                                                            pipeline_run_id= pipeline_run_id ,\\\r\n",
					"                                                            pipeline_name = pipeline_name, \\\r\n",
					"                                                            notebook_name = 'Lakehouse Orchestration', \\\r\n",
					"                                                            source_system_path =f\"{staged_instructions[table]['RAW_PATH']}\", \\\r\n",
					"                                                            target_system_path = f\"{staged_instructions[table]['STAGED_PATH']}\", \\\r\n",
					"                                                            schema_name = f\"{staged_instructions[table]['SCHEMA_NAME']}\", \\\r\n",
					"                                                            table = f\"{staged_instructions[table]['TABLE_NAME']}\", \\\r\n",
					"                                                            file_type = 'DELTA',\\\r\n",
					"                                                            operation = 'MERGE', \\\r\n",
					"                                                            start_time = start_time, \\\r\n",
					"                                                            error_msg = err_message)\r\n",
					"            cursor_etl_sql.execute(pushdown_query_fail)\r\n",
					"            staged_failures.append(table)\r\n",
					"            #logging.info(table + ' has failed to merge ')\r\n",
					"            #logging.info(err_message)\r\n",
					"            raise err_message"
				],
				"execution_count": null
			}
		]
	}
}