{
	"name": "nb_util_sql_staged_etl_and_logging_py",
	"properties": {
		"folder": {
			"name": "utils"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "012fa9dc-fd32-4a84-a99f-59c22acd61d5"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## SDLH (Strategic Data Lakehouse) Utility\r\n",
					"<p><b>Description: </b>Enter notebook decription</p>\r\n",
					"<b>Parent Process: </b>SDLH Pipelines</p>\r\n",
					"<table align=\"left\">\r\n",
					" <thead>\r\n",
					"  <tr>\r\n",
					"   <th>Contributor</th>\r\n",
					"   <th>Date</th>\r\n",
					"   <th>Version</th>\r\n",
					"   <th>Comment</th>\r\n",
					"   <th>WorkItem No</th>\r\n",
					"  </tr>\r\n",
					" </thead>\r\n",
					" <tbody>\r\n",
					"  <tr>\r\n",
					"   <td>Andrei Dumitru</td>\r\n",
					"   <td>2022-12-09</td>\r\n",
					"   <td>1.0</td>\r\n",
					"   <td>Create initial release</td>\r\n",
					"   <td></td>\r\n",
					"  </tr>\r\n",
					"  <tr>\r\n",
					"   <td>Darren Price</td>\r\n",
					"   <td>2023-10-23</td>\r\n",
					"   <td>2.0</td>\r\n",
					"   <td>Updated for SDLH v2</td>\r\n",
					"   <td></td>\r\n",
					"  </tr>\r\n",
					"  <tr>\r\n",
					"   <td>Darren Price</td>\r\n",
					"   <td>2024-01-09</td>\r\n",
					"   <td>2.1</td>\r\n",
					"   <td>Updated for SDLH v2.1</td>\r\n",
					"   <td></td>\r\n",
					"  </tr>\r\n",
					"  <tr>\r\n",
					"   <td>Darren Price</td>\r\n",
					"   <td>2024-03-08</td>\r\n",
					"   <td>2.2</td>\r\n",
					"   <td>Updated for SDLH v2.2</td>\r\n",
					"   <td></td>\r\n",
					"  </tr>\r\n",
					" </tbody>\r\n",
					"</table>"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#not logged\r\n",
					"def query_db_jdbc(jdbc_url:str, jdbc_token:str, pushdown_query:str) -> pyspark.sql.dataframe.DataFrame:\r\n",
					"    \"\"\"\r\n",
					"    Queries a SQL Server database using JDBC and returns a PySpark DataFrame.\r\n",
					"\r\n",
					"    Parameters:\r\n",
					"    - jdbc_hostname (str): the hostname or IP address of the SQL Server instance\r\n",
					"    - jdbc_database (str): the name of the database to connect to\r\n",
					"    - jdbc_port (str): the port number on which the SQL Server instance is listening\r\n",
					"    - jdbc_options (str): any options to add to the connection\r\n",
					"    - jdbc_token (str): the managed identity token to authenticate with\r\n",
					"    - pushdown_query (str): a SQL query to execute in the database (with optional filters and transformations)\r\n",
					"\r\n",
					"    Returns:\r\n",
					"    - df (pyspark.sql.dataframe.DataFrame): a PySpark DataFrame containing the result set of the query, with an additional \"current_date\" column that contains the current date.\r\n",
					"\r\n",
					"    \"\"\"\r\n",
					"    # Create connection properties, uses generated managed identity token\r\n",
					"    connection_properties = {\r\n",
					"      \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\r\n",
					"      \"accessToken\" : jdbc_token\r\n",
					"    }\r\n",
					"\r\n",
					"    # Use PySpark's built-in JDBC reader to execute the query and create a DataFrame and add current_date column\r\n",
					"    df = spark.read.jdbc(url=jdbc_url, table=pushdown_query, properties=connection_properties).withColumn(\"current_date\", current_date())\r\n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def query_db_odbc(odbc_url:str, odbc_token:str, pushdown_query:str) -> pyspark.sql.dataframe.DataFrame:\r\n",
					"    \"\"\"\r\n",
					"    Queries a SQL Server database using ODBC and returns a PySpark DataFrame.\r\n",
					"\r\n",
					"    Parameters:\r\n",
					"    - odbc_hostname (str): the hostname or IP address of the SQL Server instance\r\n",
					"    - odbc_database (str): the name of the database to connect to\r\n",
					"    - odbc_port (str): the port number on which the SQL Server instance is listening\r\n",
					"    - odbc_options (str): any options to add to the connection\r\n",
					"    - odbc_token (str): the managed identity token to authenticate with\r\n",
					"    - pushdown_query (str): a SQL query to execute in the database (with optional filters and transformations)\r\n",
					"\r\n",
					"    Returns:\r\n",
					"    - df (pyspark.sql.dataframe.DataFrame): a PySpark DataFrame containing the result set of the stored procedure.\r\n",
					"\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    # Create connection properties, uses generated managed identity token\r\n",
					"    connection_properties = pyodbc.connect(odbc_url, attrs_before = { SQL_COPT_SS_ACCESS_TOKEN:odbc_token })\r\n",
					"\r\n",
					"    # Read the data into a Pandas dataframe\r\n",
					"    df_pd = pd.read_sql(pushdown_query, connection_properties)\r\n",
					"\r\n",
					"    # Create a SparkSession\r\n",
					"    spark = SparkSession.builder.appName(\"Pandas to Spark\").getOrCreate()\r\n",
					"\r\n",
					"    # Convert the Pandas dataframe to a Spark dataframe\r\n",
					"    df_spark = spark.createDataFrame(df_pd)\r\n",
					"\r\n",
					"    # Show the Spark dataframe\r\n",
					"    return df_spark"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_raw_staged_etl_instructions(adls_container:str, adls_name:str,) -> dict:\r\n",
					"    \"\"\"\r\n",
					"    Given the ADLS container and name, it'll take the df_objects_metadata_extracted dataframe and parse all the info into a python\r\n",
					"    dictionary staged_instructions. This dictionary contains all the info needed to perform ETL from raw to enriched.\r\n",
					"\r\n",
					"    Parameters:\r\n",
					"    - adls_container (str): The enriched storage account container name as a string.\r\n",
					"    - adls_name (str): The storage account name as a string.\r\n",
					"\r\n",
					"    Returns:\r\n",
					"    - staged_instructions (dict): Dictionary containing raw to enriched ETL instructions.\r\n",
					"\r\n",
					"    \"\"\"\r\n",
					"    # Mount onto the enriched container\r\n",
					"    JOB_ID = synapse_adls_remount(adls_container, adls_name, \"datalake_data_azir\")\r\n",
					"    # Get tables that have already been migrated from the raw_migrated_tables_log DataFrame\r\n",
					"    tables_migrated_raw = df_objects_metadata_extracted.selectExpr(\"OBJECT_NAME\", \\\r\n",
					"                                                        \"SOURCE_SYSTEM\", \\\r\n",
					"                                                        \"DATABASE_NAME\",\\\r\n",
					"                                                        \"SCHEMA_NAME\",\\\r\n",
					"                                                        \"TABLE_NAME\",  \\\r\n",
					"                                                        \"SERVERLESS_SQL_POOL_DATABASE\",  \\\r\n",
					"                                                        \"SERVERLESS_SQL_POOL_SCHEMA\",  \\\r\n",
					"                                                        \"LOAD_TYPE\", \\\r\n",
					"                                                        \"PRIMARY_KEYS\", \\\r\n",
					"                                                        f\"CONCAT('abfss://raw@{adls_name}.dfs.core.windows.net/', FP1_RAW) AS RAW_PATH\",\\\r\n",
					"                                                        f\"CONCAT('synfs:/{JOB_ID}/mount/', FP1_STAGED) AS STAGED_PATH\")\\\r\n",
					"                                  .rdd.map(lambda x: (x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7], x[8], x[9], x[10])).collect()\r\n",
					"    tables_migrated_raw=list(tables_migrated_raw)\r\n",
					"\r\n",
					"    # Construct the data path for each table and store it in a dictionary\r\n",
					"    staged_instructions = {}\r\n",
					"    primary_key_lookup = {}\r\n",
					"    # Compose primary key statement for delta merges\r\n",
					"    for row in df_objects_metadata_extracted.select(\"TABLE_NAME\", \"PRIMARY_KEYS\").filter(df_objects_metadata_extracted.PRIMARY_KEYS.isNotNull()).distinct().collect():\r\n",
					"        primary_key_lookup[row.TABLE_NAME] = row.PRIMARY_KEYS.split(\",\")\r\n",
					"\r\n",
					"    # Parse and populate the python dictionary\r\n",
					"    for OBJECT_NAME, SOURCE_SYSTEM, DATABASE_NAME, SCHEMA_NAME, TABLE_NAME, SERVERLESS_SQL_POOL_DATABASE, SERVERLESS_SQL_POOL_SCHEMA, LOAD_TYPE, PRIMARY_KEYS, RAW_PATH, STAGED_PATH in tables_migrated_raw:\r\n",
					"        data_path = f\"{RAW_PATH}\"\r\n",
					"        if LOAD_TYPE == \"FULL_LOAD\":\r\n",
					"            table_primary_key_statements = [\"NULL\"]\r\n",
					"        else:\r\n",
					"            # Simplify string concatenation using f-strings\r\n",
					"            primary_keys = primary_key_lookup[TABLE_NAME]\r\n",
					"            table_primary_key_statements = [f\"{TABLE_NAME}_parquet.{pk} = {TABLE_NAME}_delta.{pk}\".lower() for pk in primary_keys]\r\n",
					"            table_primary_key_statements = ' and '.join(table_primary_key_statements)\r\n",
					"            \r\n",
					"        staged_instructions[TABLE_NAME] = {\"OBJECT_NAME\":OBJECT_NAME, \"SOURCE_SYSTEM\":SOURCE_SYSTEM, \"DATABASE_NAME\": DATABASE_NAME, \"SCHEMA_NAME\": SCHEMA_NAME, \"TABLE_NAME\": TABLE_NAME, \"SERVERLESS_SQL_POOL_DATABASE\":SERVERLESS_SQL_POOL_DATABASE, \"SERVERLESS_SQL_POOL_SCHEMA\":SERVERLESS_SQL_POOL_SCHEMA, \"LOAD_TYPE\":LOAD_TYPE, \"PRIMARY_KEYS\":PRIMARY_KEYS , \"PRIMARY_KEY_STATEMENTS\": table_primary_key_statements , \"RAW_PATH\":RAW_PATH, \"STAGED_PATH\":STAGED_PATH}\r\n",
					"    return staged_instructions"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#not logged\r\n",
					"def synapse_adls_remount(file_system_name:str, datalake_name:str, linked_service_name:str)->str:\r\n",
					"    \"\"\"\r\n",
					"    Mounts an Azure Data Lake Storage Gen2 filesystem using the specified file_system_name, datalake_name, and \r\n",
					"    linked_service_name. The mounted filesystem will be available at \"/mount\" in the current workspace.\r\n",
					"    \r\n",
					"    Parameters:\r\n",
					"    - file_system_name (str): The name of the filesystem to mount.\r\n",
					"    - datalake_name (str): The name of the Azure Data Lake Storage Gen2 account.\r\n",
					"    - linked_service_name (str): The name of the Azure Synapse linked service to use for mounting the filesystem.\r\n",
					"    \r\n",
					"    Returns:\r\n",
					"    - str: The job ID of the current Synapse Analytics job.\r\n",
					"    \r\n",
					"    \"\"\"\r\n",
					"    # Unmounting any existing mount points\r\n",
					"    mssparkutils.fs.unmount(\"/mount\")\r\n",
					"\r\n",
					"    # Mounting the specified filesystem\r\n",
					"    mssparkutils.fs.mount( \r\n",
					"        f\"abfss://{file_system_name}@{datalake_name}.dfs.core.windows.net\", \r\n",
					"        \"/mount\", \r\n",
					"        {\"linkedService\":f\"{linked_service_name}\"} \r\n",
					"    )\r\n",
					"    \r\n",
					"    # Returning the current job ID\r\n",
					"    JOB_ID = mssparkutils.env.getJobId()\r\n",
					"    return JOB_ID"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def deserialize_json_meta_to_notebook_etl_relational_meta(odbc_url:str, odbc_token:str) -> pyspark.sql.dataframe.DataFrame:\r\n",
					"    \"\"\"\r\n",
					"    Deserializes JSON metadata from a specified database using the provided parameters and returns two Spark dataframes. \r\n",
					"    The first dataframe contains extracted object metadata, such as server and database names, table and column names, and file paths. \r\n",
					"    The second dataframe contains information schema metadata, such as column names, data types, and character maximum lengths.\r\n",
					"\r\n",
					"    Parameters:\r\n",
					"    - odbc_hostname (str): the hostname or IP address of the SQL Server instance\r\n",
					"    - odbc_database (str): the name of the database to connect to\r\n",
					"    - odbc_port (str): the port number on which the SQL Server instance is listening\r\n",
					"    - odbc_options (str): any options to add to the connection\r\n",
					"    - odbc_token (str): the managed identity token to authenticate with\r\n",
					"\r\n",
					"    Returns:\r\n",
					"    (df_objects_metadata_extracted (pyspark.sql.dataframe.DataFrame), df_info_schema (pyspark.sql.dataframe.DataFrame)) (tuple):\r\n",
					"    A tuple of two Spark dataframes. The first dataframe contains extracted object metadata, and the second dataframe contains information schema metadata.\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    # Define the schema for the JSON string\r\n",
					"    schema_adls_paths = \"array<struct<raw:array<struct<FP0:string,FP1:string>>,\" \\\r\n",
					"            \"staged:array<struct<FP0:string,FP1:string>>>>\"\r\n",
					"\r\n",
					"    # Define the schema for the OBJECT_PARAMETERS JSON strings from the metadata\r\n",
					"    schema_object_parameters = \"\"\"\r\n",
					"    array<\r\n",
					"    struct<\r\n",
					"        SOURCE_SYSTEM: string,\r\n",
					"        SOURCE_GROUPING_ID: int,\r\n",
					"        SOURCE_SYSTEM_CONNECTION_STRING: string,\r\n",
					"        DATABASE_NAME: string,\r\n",
					"        SCHEMA_NAME: string,\r\n",
					"        TABLE_NAME: string,\r\n",
					"        PRIMARY_KEYS: string,\r\n",
					"        SCHEMA_VERSION: int,\r\n",
					"        COLUMNS_META: string,\r\n",
					"        SERVERLESS_SQL_POOL_DATABASE: string,\r\n",
					"        SERVERLESS_SQL_POOL_SCHEMA: string,\r\n",
					"        INFORMATION_SCHEMA: array<\r\n",
					"        struct<\r\n",
					"            COLUMN_NAME: string,\r\n",
					"            IS_NULLABLE: string,\r\n",
					"            DATA_TYPE: string,\r\n",
					"            CHARACTER_MAXIMUM_LENGTH: bigint,\r\n",
					"            COLLATION_NAME: string\r\n",
					"        >\r\n",
					"        >\r\n",
					"    >\r\n",
					"    >\"\"\"\r\n",
					"\r\n",
					"    \r\n",
					"\r\n",
					"    pushdown_query_select_json_metadata = f\"\"\"EXEC [ETL].[usp_GetPipelineMetadataRetrievalFull] \r\n",
					"                                            @PARAM_DATETIME = '{PARAM_TRIGGER_TIME}'\r\n",
					"                                            ,@PARAM_SOURCE_TYPE = '{PARAM_SOURCE_TYPE}'\r\n",
					"                                            ,@PARAM_SOURCE_SYSTEM = '{PARAM_SOURCE_SYSTEM}'\r\n",
					"                                            ,@PARAM_SOURCE_GROUPING_ID = '{PARAM_SOURCE_GROUPING_ID}'\"\"\"\r\n",
					"\r\n",
					"    df_json_metadata = query_db_odbc(odbc_url, odbc_token, pushdown_query_select_json_metadata)\r\n",
					"\r\n",
					"    # Parse the JSON string into a struct column\r\n",
					"    df = df_json_metadata.select(col(\"OBJECT_NAME\"), \\\r\n",
					"                                col(\"SOURCE_TYPE\"), \\\r\n",
					"                                col(\"LOAD_TYPE\") ,\\\r\n",
					"                                from_json(col(\"ADLS_PATHS\"), schema_adls_paths).alias(\"ADLS_PATHS\"), \\\r\n",
					"                                from_json(col(\"OBJECT_PARAMETERS\"), schema_object_parameters).alias(\"OBJECT_PARAMETERS\"),\\\r\n",
					"                                col(\"HNS\"))\r\n",
					"    # Extract the values of FP0 and FP1 from the raw array\r\n",
					"    df_objects_metadata_extracted = df.selectExpr(\"OBJECT_NAME\",\\\r\n",
					"                                \"OBJECT_PARAMETERS[0].SOURCE_SYSTEM AS SOURCE_SYSTEM\",\\\r\n",
					"                                \"OBJECT_PARAMETERS[0].DATABASE_NAME AS DATABASE_NAME\",\\\r\n",
					"                                \"OBJECT_PARAMETERS[0].SCHEMA_NAME AS SCHEMA_NAME\",\\\r\n",
					"                                \"OBJECT_PARAMETERS[0].TABLE_NAME AS TABLE_NAME\",\\\r\n",
					"                                \"OBJECT_PARAMETERS[0].SERVERLESS_SQL_POOL_DATABASE AS SERVERLESS_SQL_POOL_DATABASE\",\\\r\n",
					"                                \"OBJECT_PARAMETERS[0].SERVERLESS_SQL_POOL_SCHEMA AS SERVERLESS_SQL_POOL_SCHEMA\",\\\r\n",
					"                                \"LOAD_TYPE\",\\\r\n",
					"                                \"OBJECT_PARAMETERS[0].INFORMATION_SCHEMA AS INFORMATION_SCHEMA\",\\\r\n",
					"                                \"CASE WHEN LOAD_TYPE = 'FULL_LOAD' THEN NULL ELSE OBJECT_PARAMETERS[0].PRIMARY_KEYS END AS PRIMARY_KEYS \", \\\r\n",
					"                                \"ADLS_PATHS[0].raw[0].FP0 AS FP0_RAW \", \\\r\n",
					"                                \"concat(ADLS_PATHS.raw[0].FP1[0], HNS) as FP1_RAW\", \\\r\n",
					"                                \"ADLS_PATHS[0].staged[0].FP0 as FP0_STAGED\", \\\r\n",
					"                                \"ADLS_PATHS.staged[0].FP1[0] as FP1_STAGED\")\r\n",
					"\r\n",
					"    from pyspark.sql.functions import explode,arrays_zip\r\n",
					"    df_info_schema = df.select(df.OBJECT_NAME,\\\r\n",
					"                                df.OBJECT_PARAMETERS.SOURCE_SYSTEM[0].alias(\"SOURCE_SYSTEM\"),\\\r\n",
					"                                df.OBJECT_PARAMETERS.DATABASE_NAME[0].alias(\"DATABASE_NAME\"),\\\r\n",
					"                                df.OBJECT_PARAMETERS.SCHEMA_NAME[0].alias(\"SCHEMA_NAME\"),\\\r\n",
					"                                df.OBJECT_PARAMETERS.TABLE_NAME[0].alias(\"TABLE_NAME\"),\\\r\n",
					"                                explode(df.OBJECT_PARAMETERS.INFORMATION_SCHEMA[0]).alias(\"COLUMN_INFO\"))\r\n",
					"\r\n",
					"    df_info_schema = df_info_schema.select(\r\n",
					"                                        col(\"OBJECT_NAME\"),\\\r\n",
					"                                        col(\"SOURCE_SYSTEM\"),\\\r\n",
					"                                        col(\"DATABASE_NAME\"),\\\r\n",
					"                                        col(\"SCHEMA_NAME\"),\\\r\n",
					"                                        col(\"TABLE_NAME\"),\\\r\n",
					"                                        col(\"COLUMN_INFO.COLUMN_NAME\"), \\\r\n",
					"                                        col(\"COLUMN_INFO.DATA_TYPE\"), \\\r\n",
					"                                        col(\"COLUMN_INFO.CHARACTER_MAXIMUM_LENGTH\"),\\\r\n",
					"                                        col(\"COLUMN_INFO.IS_NULLABLE\"))\r\n",
					"    return df_objects_metadata_extracted, df_info_schema\r\n",
					"    #display(df_info_schema)\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#secondary\r\n",
					"def create_query_successful_etl_log(trigger_time:str, trigger_type:str, pipeline_run_id:str, pipeline_name:str, notebook_name:str, source_system_path:str, target_system_path:str, object_name:str, database_name:str, schema_name:str, table:str, file_type:str, operation:str, start_time:datetime, merge_type:str) -> str:\r\n",
					"    \"\"\"Creates a SQL query to insert a log of a successful ETL operation into a database.\r\n",
					"\r\n",
					"    Parameters:\r\n",
					"        trigger_time (datetime): The date and time the ETL operation was triggered.\r\n",
					"        trigger_type (str): The type of trigger that started the ETL operation.\r\n",
					"        pipeline_run_id (str): The unique ID of the pipeline run that triggered the ETL operation.\r\n",
					"        pipeline_name (str): The name of the pipeline that triggered the ETL operation.\r\n",
					"        notebook_name (str): The name of the notebook or script that performed the ETL operation.\r\n",
					"        source_system_path (str): The path to the source system data.\r\n",
					"        target_system_path (str): The path to the target system data.\r\n",
					"        object_name (str): The name of the object containing the target table.\r\n",
					"        database_name (str): The name of the database containing the target table.\r\n",
					"        schema_name (str): The name of the schema containing the target table.\r\n",
					"        table (str): The name of the target table.\r\n",
					"        file_type (str): The type of file used for the target system data.\r\n",
					"        operation (str): The type of operation performed on the target system data.\r\n",
					"        start_time (datetime): The date and time the ETL operation started.\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        pushdown_query (str): A SQL query to insert a log of the ETL operation into a database.\r\n",
					"\r\n",
					"    \"\"\"\r\n",
					"    if file_type == 'DELTA':\r\n",
					"        if operation == 'WRITE':\r\n",
					"            end_time = datetime.utcnow()\r\n",
					"            deltaTime = end_time-start_time\r\n",
					"            deltaTime = deltaTime.total_seconds()\r\n",
					"            deltaTime = int(deltaTime)\r\n",
					"            delta_table = DeltaTable.forPath(spark, f\"{staged_instructions[table]['STAGED_PATH']}\")\r\n",
					"            lastOperationDF = delta_table.history(1)\r\n",
					"            operation_metrics = lastOperationDF.select(F.explode(lastOperationDF.operationMetrics))\r\n",
					"            numOutputRows = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numOutputRows')\r\n",
					"            numOutputRows = numOutputRows.rdd.flatMap(lambda x:x).collect()[0]\r\n",
					"\r\n",
					"            pushdown_query = f\"\"\"EXEC [ETL].[usp_InsertLog]\r\n",
					"                                @SOURCE_TYPE = '{PARAM_SOURCE_TYPE}'\r\n",
					"                                ,@SOURCE_SYSTEM = '{PARAM_SOURCE_SYSTEM}'\r\n",
					"                                ,@SOURCE_GROUPING_ID = '{PARAM_SOURCE_GROUPING_ID}'\r\n",
					"                                ,@OBJECT_NAME = '{object_name}'\r\n",
					"                                ,@DATABASE_NAME = '{database_name}'\r\n",
					"                                ,@SCHEMA_NAME = '{schema_name}'\r\n",
					"                                ,@TABLE_NAME = '{table}'\r\n",
					"                                ,@TRIGGER_TIME = '{trigger_time}'\r\n",
					"                                ,@TRIGGER_TYPE = '{trigger_type}'\r\n",
					"                                ,@PIPELINE_RUN_ID = '{pipeline_run_id}'\r\n",
					"                                ,@PIPELINE_NAME = '{pipeline_name}'\r\n",
					"                                ,@TRIGGERING_PIPELINE_RUN_ID = '{PARAM_TRIGGERING_PIPELINE_RUN_ID}'\r\n",
					"                                ,@TRIGGERING_PIPELINE_NAME = '{PARAM_TRIGGERING_PIPELINE_NAME}'\r\n",
					"                                ,@OPERATION = '{operation}'\r\n",
					"                                ,@COMPONENT_NAME = '{notebook_name}'\r\n",
					"                                ,@TARGET_SYSTEM = '{target_system_path}'\r\n",
					"                                ,@START_TIME = '{start_time}'\r\n",
					"                                ,@END_TIME =  '{end_time}'\r\n",
					"                                ,@DURATION_SECONDS = {deltaTime}\r\n",
					"                                ,@INSERTS = {numOutputRows}\r\n",
					"                                ,@UPDATES = 0\r\n",
					"                                ,@DELETES = 0\r\n",
					"                                ,@ERROR_MESSAGE = NULL \r\n",
					"                                ,@NEW_INCREMENTAL_KEY_VALUE = NULL \r\n",
					"                                ,@PREVIOUS_INCREMENTAL_KEY_VALUE = NULL \"\"\"\r\n",
					"            return pushdown_query\r\n",
					"        elif operation == 'MERGE':\r\n",
					"            end_time = datetime.utcnow()\r\n",
					"            deltaTime = end_time-start_time\r\n",
					"            deltaTime = deltaTime.total_seconds()\r\n",
					"            deltaTime = int(deltaTime)\r\n",
					"            delta_table = DeltaTable.forPath(spark, f\"{staged_instructions[table]['STAGED_PATH']}\")\r\n",
					"            lastMergeOperationDF = delta_table.history(2)\r\n",
					"            operation_metrics = lastMergeOperationDF.select(F.explode(lastMergeOperationDF.operationMetrics), lastMergeOperationDF.version)\r\n",
					"            if merge_type == 'CHANGE_TRACKING':\r\n",
					"                numRowsInserted = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numTargetRowsInserted').rdd.flatMap(lambda x:x).collect()[1]\r\n",
					"                try:\r\n",
					"                    numRowsDeleted = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numTargetRowsDeleted').rdd.flatMap(lambda x:x).collect()[0]\r\n",
					"                except IndexError: \r\n",
					"                    numRowsDeleted = 0\r\n",
					"                numRowsUpdated = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numTargetRowsUpdated').rdd.flatMap(lambda x:x).collect()[1]\r\n",
					"            else:\r\n",
					"                numRowsInserted = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numTargetRowsInserted').rdd.flatMap(lambda x:x).collect()[0]\r\n",
					"                numRowsDeleted = 0\r\n",
					"                numRowsUpdated = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numTargetRowsUpdated').rdd.flatMap(lambda x:x).collect()[0]\r\n",
					"\r\n",
					"            pushdown_query = f\"\"\"EXEC [ETL].[usp_InsertLog]\r\n",
					"                                @SOURCE_TYPE = '{PARAM_SOURCE_TYPE}'\r\n",
					"                                ,@SOURCE_SYSTEM = '{PARAM_SOURCE_SYSTEM}'\r\n",
					"                                ,@SOURCE_GROUPING_ID = '{PARAM_SOURCE_GROUPING_ID}'\r\n",
					"                                ,@OBJECT_NAME = '{object_name}'\r\n",
					"                                ,@DATABASE_NAME = '{database_name}'\r\n",
					"                                ,@SCHEMA_NAME = '{schema_name}'\r\n",
					"                                ,@TABLE_NAME = '{table}'\r\n",
					"                                ,@TRIGGER_TIME = '{trigger_time}'\r\n",
					"                                ,@TRIGGER_TYPE = '{trigger_type}'\r\n",
					"                                ,@PIPELINE_RUN_ID = '{pipeline_run_id}'\r\n",
					"                                ,@PIPELINE_NAME = '{pipeline_name}'\r\n",
					"                                ,@TRIGGERING_PIPELINE_RUN_ID = '{PARAM_TRIGGERING_PIPELINE_RUN_ID}'\r\n",
					"                                ,@TRIGGERING_PIPELINE_NAME = '{PARAM_TRIGGERING_PIPELINE_NAME}'\r\n",
					"                                ,@OPERATION = '{operation}'\r\n",
					"                                ,@COMPONENT_NAME = '{notebook_name}'\r\n",
					"                                ,@TARGET_SYSTEM = '{target_system_path}'\r\n",
					"                                ,@START_TIME = '{start_time}'\r\n",
					"                                ,@END_TIME =  '{end_time}'\r\n",
					"                                ,@DURATION_SECONDS = {deltaTime}\r\n",
					"                                ,@INSERTS = {numRowsInserted}\r\n",
					"                                ,@UPDATES = {numRowsUpdated}\r\n",
					"                                ,@DELETES = {numRowsDeleted}\r\n",
					"                                ,@ERROR_MESSAGE = NULL \r\n",
					"                                ,@NEW_INCREMENTAL_KEY_VALUE = NULL \r\n",
					"                                ,@PREVIOUS_INCREMENTAL_KEY_VALUE = NULL \"\"\"\r\n",
					"            return pushdown_query\r\n",
					"    \r\n",
					"    elif file_type == 'PARQUET':\r\n",
					"        if operation == 'WRITE':\r\n",
					"            # Gathering Successful WRITE use-case specific parameters for the Stored Procedure string\r\n",
					"            end_time = datetime.utcnow()\r\n",
					"            deltaTime = end_time-start_time\r\n",
					"            deltaTime = deltaTime.total_seconds()\r\n",
					"            deltaTime = int(deltaTime)\r\n",
					"            parquet_file = spark.read.parquet(f\"{target_system_path}\")\r\n",
					"            numOutputRows = parquet_file.count()\r\n",
					"\r\n",
					"            # Passing the parameters to the stored procedure string\r\n",
					"            pushdown_query = f\"\"\"EXEC [ETL].[usp_InsertLog]\r\n",
					"                                @SOURCE_TYPE = '{PARAM_SOURCE_TYPE}'\r\n",
					"                                ,@SOURCE_SYSTEM = '{PARAM_SOURCE_SYSTEM}'\r\n",
					"                                ,@SOURCE_GROUPING_ID = '{PARAM_SOURCE_GROUPING_ID}'\r\n",
					"                                ,@OBJECT_NAME = '{object_name}'\r\n",
					"                                ,@DATABASE_NAME = '{database_name}'\r\n",
					"                                ,@SCHEMA_NAME = '{schema_name}'\r\n",
					"                                ,@TABLE_NAME = '{table}'\r\n",
					"                                ,@TRIGGER_TIME = '{trigger_time}'\r\n",
					"                                ,@TRIGGER_TYPE = '{trigger_type}'\r\n",
					"                                ,@PIPELINE_RUN_ID = '{pipeline_run_id}'\r\n",
					"                                ,@PIPELINE_NAME = '{pipeline_name}'\r\n",
					"                                ,@TRIGGERING_PIPELINE_RUN_ID = '{PARAM_TRIGGERING_PIPELINE_RUN_ID}'\r\n",
					"                                ,@TRIGGERING_PIPELINE_NAME = '{PARAM_TRIGGERING_PIPELINE_NAME}'\r\n",
					"                                ,@OPERATION = '{operation}'\r\n",
					"                                ,@COMPONENT_NAME = '{notebook_name}'\r\n",
					"                                ,@TARGET_SYSTEM = '{target_system_path}'\r\n",
					"                                ,@START_TIME = '{start_time}'\r\n",
					"                                ,@END_TIME =  '{end_time}'\r\n",
					"                                ,@DURATION_SECONDS = {deltaTime}\r\n",
					"                                ,@INSERTS = {numOutputRows}\r\n",
					"                                ,@UPDATES = 0\r\n",
					"                                ,@DELETES = 0\r\n",
					"                                ,@ERROR_MESSAGE = NULL \r\n",
					"                                ,@NEW_INCREMENTAL_KEY_VALUE = NULL \r\n",
					"                                ,@PREVIOUS_INCREMENTAL_KEY_VALUE = NULL \"\"\"\r\n",
					"            # Returning the query as a string\r\n",
					"            return pushdown_query\r\n",
					"\r\n",
					"#secondary\r\n",
					"def create_query_failed_etl_log(trigger_time:str, trigger_type:str, pipeline_run_id:str, pipeline_name:str, notebook_name:str, source_system_path:str, target_system_path:str, object_name:str, database_name:str, schema_name:str, table:str, file_type:str, operation:str, start_time:datetime, error_msg:str) -> str:\r\n",
					"    \"\"\"\r\n",
					"    Creates a stored procedure query to insert ETL log information into the database.\r\n",
					"\r\n",
					"    Parameters:\r\n",
					"        trigger_time (str): The time at which the trigger occurred.\r\n",
					"        trigger_type (str): The type of the trigger that caused the ETL process.\r\n",
					"        pipeline_run_id (str): The ID of the pipeline run.\r\n",
					"        pipeline_name (str): The name of the pipeline that was executed.\r\n",
					"        notebook_name (str): The name of the notebook that ran the ETL process.\r\n",
					"        source_system_path (str): The path to the source system.\r\n",
					"        target_system_path (str): The path to the target system.\r\n",
					"        object_name (str): The name of the object containing the target table.\r\n",
					"        database_name (str): The name of the database containing the target table.\r\n",
					"        schema_name (str): The name of the schema containing the target table.\r\n",
					"        table (str): The name of the target table.\r\n",
					"        file_type (str): The type of file that was processed (e.g. PARQUET, DELTA).\r\n",
					"        operation (str): The type of operation that was performed (e.g. WRITE, MERGE).\r\n",
					"        start_time (datetime.datetime): The time at which the ETL process began.\r\n",
					"        error_msg (str): The error message that was encountered during the ETL process.\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        pushdown_query (str): The stored procedure query as a string.\r\n",
					"    \"\"\"\r\n",
					"    if file_type == 'DELTA':\r\n",
					"        if operation == 'WRITE':\r\n",
					"            end_time = datetime.utcnow()\r\n",
					"            deltaTime = end_time-start_time\r\n",
					"            deltaTime = deltaTime.total_seconds()\r\n",
					"            deltaTime = int(deltaTime)\r\n",
					"\r\n",
					"            pushdown_query = f\"\"\"EXEC [ETL].[usp_InsertLog]\r\n",
					"                                @SOURCE_TYPE = '{PARAM_SOURCE_TYPE}'\r\n",
					"                                ,@SOURCE_SYSTEM = '{PARAM_SOURCE_SYSTEM}'\r\n",
					"                                ,@SOURCE_GROUPING_ID = '{PARAM_SOURCE_GROUPING_ID}'\r\n",
					"                                ,@OBJECT_NAME = '{object_name}'\r\n",
					"                                ,@DATABASE_NAME = '{database_name}'\r\n",
					"                                ,@SCHEMA_NAME = '{schema_name}'\r\n",
					"                                ,@TABLE_NAME = '{table}'\r\n",
					"                                ,@TRIGGER_TIME = '{trigger_time}'\r\n",
					"                                ,@TRIGGER_TYPE = '{trigger_type}'\r\n",
					"                                ,@PIPELINE_RUN_ID = '{pipeline_run_id}'\r\n",
					"                                ,@PIPELINE_NAME = '{pipeline_name}'\r\n",
					"                                ,@TRIGGERING_PIPELINE_RUN_ID = '{PARAM_TRIGGERING_PIPELINE_RUN_ID}'\r\n",
					"                                ,@TRIGGERING_PIPELINE_NAME = '{PARAM_TRIGGERING_PIPELINE_NAME}'\r\n",
					"                                ,@OPERATION = '{operation}'\r\n",
					"                                ,@COMPONENT_NAME = '{notebook_name}'\r\n",
					"                                ,@TARGET_SYSTEM = '{target_system_path}'\r\n",
					"                                ,@START_TIME = '{start_time}'\r\n",
					"                                ,@END_TIME =  '{end_time}'\r\n",
					"                                ,@DURATION_SECONDS = {deltaTime}\r\n",
					"                                ,@INSERTS = 0\r\n",
					"                                ,@UPDATES = 0\r\n",
					"                                ,@DELETES = 0\r\n",
					"                                ,@ERROR_MESSAGE = '{error_msg}'\r\n",
					"                                ,@NEW_INCREMENTAL_KEY_VALUE = NULL\r\n",
					"                                ,@PREVIOUS_INCREMENTAL_KEY_VALUE = NULL \"\"\"\r\n",
					"            return pushdown_query\r\n",
					"        elif operation == 'MERGE':\r\n",
					"            end_time = datetime.utcnow()\r\n",
					"            deltaTime = end_time-start_time\r\n",
					"            \r\n",
					"            deltaTime = deltaTime.total_seconds()\r\n",
					"            deltaTime = int(deltaTime)\r\n",
					"\r\n",
					"            pushdown_query = f\"\"\"EXEC [ETL].[usp_InsertLog]\r\n",
					"                                @SOURCE_TYPE = '{PARAM_SOURCE_TYPE}'\r\n",
					"                                ,@SOURCE_SYSTEM = '{PARAM_SOURCE_SYSTEM}'\r\n",
					"                                ,@SOURCE_GROUPING_ID = '{PARAM_SOURCE_GROUPING_ID}'\r\n",
					"                                ,@OBJECT_NAME = '{object_name}'\r\n",
					"                                ,@DATABASE_NAME = '{database_name}'\r\n",
					"                                ,@SCHEMA_NAME = '{schema_name}'\r\n",
					"                                ,@TABLE_NAME = '{table}'\r\n",
					"                                ,@TRIGGER_TIME = '{trigger_time}'\r\n",
					"                                ,@TRIGGER_TYPE = '{trigger_type}'\r\n",
					"                                ,@PIPELINE_RUN_ID = '{pipeline_run_id}'\r\n",
					"                                ,@PIPELINE_NAME = '{pipeline_name}'\r\n",
					"                                ,@TRIGGERING_PIPELINE_RUN_ID = '{PARAM_TRIGGERING_PIPELINE_RUN_ID}'\r\n",
					"                                ,@TRIGGERING_PIPELINE_NAME = '{PARAM_TRIGGERING_PIPELINE_NAME}'\r\n",
					"                                ,@OPERATION = '{operation}'\r\n",
					"                                ,@COMPONENT_NAME = '{notebook_name}'\r\n",
					"                                ,@TARGET_SYSTEM = '{target_system_path}'\r\n",
					"                                ,@START_TIME = '{start_time}'\r\n",
					"                                ,@END_TIME =  '{end_time}'\r\n",
					"                                ,@DURATION_SECONDS = {deltaTime}\r\n",
					"                                ,@INSERTS = 0\r\n",
					"                                ,@UPDATES = 0\r\n",
					"                                ,@DELETES = 0\r\n",
					"                                ,@ERROR_MESSAGE = '{error_msg}'\r\n",
					"                                ,@NEW_INCREMENTAL_KEY_VALUE = NULL \r\n",
					"                                ,@PREVIOUS_INCREMENTAL_KEY_VALUE = NULL \"\"\"\r\n",
					"            return pushdown_query\r\n",
					"\r\n",
					"    elif file_type == 'PARQUET':\r\n",
					"        if operation == 'WRITE':\r\n",
					"            # Gathering Successful WRITE use-case specific parameters for the Stored Procedure string\r\n",
					"            end_time = datetime.utcnow()\r\n",
					"            deltaTime = end_time-start_time\r\n",
					"            deltaTime = deltaTime.total_seconds()\r\n",
					"            deltaTime = int(deltaTime)\r\n",
					"            parquet_file = spark.read.parquet(f\"{target_system_path}\")\r\n",
					"            numOutputRows = parquet_file.count()\r\n",
					"\r\n",
					"            # Passing the parameters to the stored procedure string\r\n",
					"            pushdown_query = f\"\"\"EXEC [ETL].[usp_InsertLog]\r\n",
					"                                @SOURCE_TYPE = '{PARAM_SOURCE_TYPE}'\r\n",
					"                                ,@SOURCE_SYSTEM = '{PARAM_SOURCE_SYSTEM}'\r\n",
					"                                ,@SOURCE_GROUPING_ID = '{PARAM_SOURCE_GROUPING_ID}'\r\n",
					"                                ,@OBJECT_NAME = '{object_name}'\r\n",
					"                                ,@DATABASE_NAME = '{database_name}'\r\n",
					"                                ,@SCHEMA_NAME = '{schema_name}'\r\n",
					"                                ,@TABLE_NAME = '{table}'\r\n",
					"                                ,@TRIGGER_TIME = '{trigger_time}'\r\n",
					"                                ,@TRIGGER_TYPE = '{trigger_type}'\r\n",
					"                                ,@PIPELINE_RUN_ID = '{pipeline_run_id}'\r\n",
					"                                ,@PIPELINE_NAME = '{pipeline_name}'\r\n",
					"                                ,@TRIGGERING_PIPELINE_RUN_ID = '{PARAM_TRIGGERING_PIPELINE_RUN_ID}'\r\n",
					"                                ,@TRIGGERING_PIPELINE_NAME = '{PARAM_TRIGGERING_PIPELINE_NAME}'\r\n",
					"                                ,@OPERATION = '{operation}'\r\n",
					"                                ,@COMPONENT_NAME = '{notebook_name}'\r\n",
					"                                ,@TARGET_SYSTEM = '{target_system_path}'\r\n",
					"                                ,@START_TIME = '{start_time}'\r\n",
					"                                ,@END_TIME =  '{end_time}'\r\n",
					"                                ,@DURATION_SECONDS = {deltaTime}\r\n",
					"                                ,@INSERTS = 0\r\n",
					"                                ,@UPDATES = 0\r\n",
					"                                ,@DELETES = 0\r\n",
					"                                ,@ERROR_MESSAGE = NULL\r\n",
					"                                ,@NEW_INCREMENTAL_KEY_VALUE = NULL \r\n",
					"                                ,@PREVIOUS_INCREMENTAL_KEY_VALUE = NULL \"\"\"\r\n",
					"            # Returning the query as a string\r\n",
					"            return pushdown_query"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def run_tasks(function, q):\r\n",
					"    while not q.empty():\r\n",
					"        value = q.get()\r\n",
					"        function(value)\r\n",
					"        q.task_done()\r\n",
					"    q.task_done()\r\n",
					"    print(\"Tables Migrated\")\r\n",
					"\r\n",
					"def load_table(table):\r\n",
					"    \"\"\"\r\n",
					"    Function that runs the raw_to_staged_etl_plus_logging() function for each table in the ETL queue. This is done so that\r\n",
					"    the load can be split across multiple worker nodes\r\n",
					"    \"\"\"\r\n",
					"    raw_to_staged_etl_plus_logging(table, PARAM_STORAGE_QUEUE_NAME, PARAM_TRIGGER_TIME, PARAM_TRIGGER_TYPE, PARAM_PIPELINE_NAME, PARAM_PIPELINE_RUN_ID, staged_instructions)\r\n",
					"    print(table)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_delta_versions(tables_list:list, staged_instructions: dict) -> dict:\r\n",
					"    \"\"\"Gets a dict of all delta tables and their current version.\"\"\"\r\n",
					"    delta_versions = {}\r\n",
					"    \r\n",
					"    for table in etl_queue:\r\n",
					"        table_path = staged_instructions[table][\"STAGED_PATH\"]   #f\"{delta_path}{table_name}\"\r\n",
					"        \r\n",
					"        # Check if the target file already exists in delta format\r\n",
					"        is_delta = DeltaTable.isDeltaTable(spark, table_path)\r\n",
					"\r\n",
					"        # Exit the current iteration if the file is not in delta format\r\n",
					"        if not is_delta:\r\n",
					"            print(f\"table {table} is not a delta table\")\r\n",
					"            continue\r\n",
					"\r\n",
					"        # Read delta table into a dataframe for processing\r\n",
					"        delta_table = DeltaTable.forPath(spark, table_path)\r\n",
					"\r\n",
					"        # Get the current version number        \r\n",
					"        version_num = delta_table.history().collect()[0][\"version\"]\r\n",
					"\r\n",
					"        # Append the version number to the table name list ready to rollback if required        \r\n",
					"        delta_versions[table] = {\"SCHEMA_VERSION\": version_num, \"DELTA_TABLE_PATH\": staged_instructions[table][\"STAGED_PATH\"]}\r\n",
					"        \r\n",
					"    \r\n",
					"    return delta_versions\r\n",
					"\r\n",
					"\t\r\n",
					"def rollback_deltatables(rollback_versions: dict, successful_ingestions:list) -> None:\r\n",
					"    \"\"\"Rolls back modified delta tables to the previous version.\r\n",
					"    \r\n",
					"    Accepts a dictionary containing the names of any delta files which have\r\n",
					"     been modified and their version number at the start of the notebook.\r\n",
					"     Each delta table previously altered is then restored to the specified\r\n",
					"     version and redunandant metadata vacuumed.\r\n",
					"\r\n",
					"    Args:\r\n",
					"        rollback_versions(dict): Dict containing entity names and versions.\r\n",
					"    \r\n",
					"    Returns:\r\n",
					"        None\r\n",
					"    \"\"\"\r\n",
					"    for table in successful_ingestions:\r\n",
					"\r\n",
					"        version_num = rollback_versions[table][\"SCHEMA_VERSION\"]\r\n",
					"        delta_table_path = rollback_versions[table]['DELTA_TABLE_PATH']\r\n",
					"\r\n",
					"        delta_df = DeltaTable.forPath(spark, f\"{delta_table_path}\")\r\n",
					"        delta_df.restoreToVersion(version_num)\r\n",
					"        \r\n",
					"        delta_df.vacuum()\r\n",
					"        \r\n",
					"    \r\n",
					"    return\r\n",
					"\r\n",
					"\t\r\n",
					"def cast_timestamps_to_date(entity_df: pyspark.sql.dataframe.DataFrame) -> pyspark.sql.dataframe.DataFrame:\r\n",
					"    \"\"\"Casts timestamp columns to date preserving null values.\r\n",
					"    \r\n",
					"    Accepts a Spark dataframe and casts any columns of data type \"timestamp\"\r\n",
					"     to \"date\". This preserves null values when writing to delta format.\r\n",
					"\r\n",
					"    Args:\r\n",
					"        entity_df(DataFrame): A Spark dataframe containing timestamp columns.\r\n",
					"    \r\n",
					"    Returns:\r\n",
					"        entity_df(DataFrame): A Spark dataframe with the timestamp data types\r\n",
					"         cast as date.\r\n",
					"    \"\"\"\r\n",
					"    # Get a list of all timestamp columns\r\n",
					"    date_cols = [x[0] for x in entity_df.dtypes if x[1] == \"timestamp\"]\r\n",
					"    \r\n",
					"    # Cast each timestamp column as date\r\n",
					"    for date_col in date_cols:\r\n",
					"        entity_df = entity_df.withColumn(date_col, F.col(date_col).cast(\"date\"))    \r\n",
					"\r\n",
					"\r\n",
					"    return entity_df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def raw_to_staged_etl_plus_logging(table:str, storage_queue:str, trigger_time:str, trigger_type:str, pipeline_name:str, pipeline_run_id:str, staged_instructions:dict):\r\n",
					"    \"\"\"\r\n",
					"    This function extracts data from a RAW source, transforms the data, and writes/merges it to a Delta table in Staged. \r\n",
					"\r\n",
					"    Parameters:\r\n",
					"    - trigger_time (str): The time at which the function was triggered.\r\n",
					"    - trigger_type (str): The type of trigger that initiated the function (e.g. event-based, scheduled, manual).\r\n",
					"    - pipeline_run_id (str): A unique ID for the current pipeline run.\r\n",
					"    - migrated_to_raw (list): A list of table names that have already been migrated from the RAW layer to the Staged layer.\r\n",
					"    - migrated_to_staged (list): A list of table names that have already been migrated from the Staged layer to the Curated layer.\r\n",
					"    - staged_existing_files (list): A list of table names that already exist in the Staged layer.\r\n",
					"    - dimensions_list (list): A list of table names that are dimensions.\r\n",
					"    - staged_instructions (dict): A dictionary where keys are table names and values are the file paths in the Staged layer.\r\n",
					"\r\n",
					"    Returns:\r\n",
					"    - None\r\n",
					"    \"\"\"\r\n",
					"    \r\n",
					"    staged_failures = []\r\n",
					"    date_now = datetime.utcnow().date()\r\n",
					"    JOB_ID = mssparkutils.env.getJobId()\r\n",
					"\r\n",
					"    # Reading the RAW parquet file into a DataFrame\r\n",
					"    schema = f\"{staged_instructions[table]['SCHEMA_NAME']}\"\r\n",
					"    load_type = f\"{staged_instructions[table]['LOAD_TYPE']}\"\r\n",
					"    serverless_sql_database = f\"{staged_instructions[table]['SERVERLESS_SQL_POOL_DATABASE']}\"\r\n",
					"    serverless_sql_schema = f\"{staged_instructions[table]['SERVERLESS_SQL_POOL_SCHEMA']}\"\r\n",
					"    staged_root_path = f\"synfs:/{JOB_ID}/mount/{staged_instructions[table]['SERVERLESS_SQL_POOL_DATABASE']}/{staged_instructions[table]['SERVERLESS_SQL_POOL_SCHEMA']}\" #'synfs:/125/mount/synw_data_uks_dna_lakehouse/IHHO001/ab_asbestos_dtl/'},\r\n",
					"    staged_existing_files = []\r\n",
					"    start_time = datetime.utcnow()\r\n",
					"    try:\r\n",
					"        df_to_merge = spark.read.parquet(f\"{staged_instructions[table]['RAW_PATH']}\").withColumn('etl_timestamp', lit(trigger_time).cast(TimestampType()))\r\n",
					"    except Exception as err_message:\r\n",
					"        \r\n",
					"        pushdown_query_fail = create_query_failed_etl_log(trigger_time = trigger_time,\\\r\n",
					"                                                            trigger_type = trigger_type,\\\r\n",
					"                                                            pipeline_run_id = pipeline_run_id ,\\\r\n",
					"                                                            pipeline_name = pipeline_name,\\\r\n",
					"                                                            notebook_name = 'nb_lakehouse_orchestration_py',\\\r\n",
					"                                                            source_system_path = f\"{staged_instructions[table]['RAW_PATH']}\",\\\r\n",
					"                                                            target_system_path = f\"{staged_instructions[table]['STAGED_PATH']}\",\\\r\n",
					"                                                            object_name = f\"{staged_instructions[table]['OBJECT_NAME']}\",\\\r\n",
					"                                                            database_name = f\"{staged_instructions[table]['DATABASE_NAME']}\",\\\r\n",
					"                                                            schema_name = f\"{staged_instructions[table]['SCHEMA_NAME']}\",\\\r\n",
					"                                                            table = f\"{staged_instructions[table]['TABLE_NAME']}\",\\\r\n",
					"                                                            file_type = 'DELTA',\\\r\n",
					"                                                            operation = 'MERGE',\\\r\n",
					"                                                            start_time = start_time,\\\r\n",
					"                                                            error_msg = err_message)\r\n",
					"        cursor_etl_sql.execute(pushdown_query_fail)\r\n",
					"        staged_failures.append(table)\r\n",
					"        raise err_message\r\n",
					"\r\n",
					"    try:\r\n",
					"        staged_files_list = mssparkutils.fs.ls(staged_root_path)\r\n",
					"        for file in staged_files_list:\r\n",
					"            staged_existing_files.append(file.name)\r\n",
					"    except Exception as e:\r\n",
					"        staged_existing_files = []\r\n",
					"\r\n",
					"        #logging.info(\"The path doesn't exist - meaning there are no existing staged files corresponding to this datasource.\")\r\n",
					"\r\n",
					"\r\n",
					"    #If the file from the current run hasn't been migrated (written/merged) to staged:\r\n",
					"        #If the file doesn't already exist in staged: WRITE\r\n",
					"    if table not in staged_existing_files or load_type == 'FULL_LOAD':\r\n",
					"\r\n",
					"        start_time = datetime.utcnow()\r\n",
					"        try:\r\n",
					"            # Try WRITE FACT on ETLDate \r\n",
					"\r\n",
					"            # vv Workaround if there is no ModifiedDate present in the file to assist MERGE operations. vv\r\n",
					"            #           df_to_merge= df_to_merge.withColumn('ETLDate',lit(date_now))\r\n",
					"\r\n",
					"            df_to_merge.write\\\r\n",
					"                .mode(\"overwrite\")\\\r\n",
					"                .format(\"delta\")\\\r\n",
					"                .option(\"overwriteSchema\", \"true\")\\\r\n",
					"                .save(f\"{staged_instructions[table]['STAGED_PATH']}\")\r\n",
					"            \r\n",
					"\r\n",
					"            \r\n",
					"            # Log successful ETL WRITE for FACT\r\n",
					"            pushdown_query_success= create_query_successful_etl_log(trigger_time = trigger_time,\\\r\n",
					"                                                trigger_type = trigger_type,\\\r\n",
					"                                                pipeline_run_id = pipeline_run_id ,\\\r\n",
					"                                                pipeline_name = pipeline_name,\\\r\n",
					"                                                notebook_name = 'nb_lakehouse_orchestration_py',\\\r\n",
					"                                                source_system_path = f\"{staged_instructions[table]['RAW_PATH']}\",\\\r\n",
					"                                                target_system_path = f\"{staged_instructions[table]['STAGED_PATH']}\",\\\r\n",
					"                                                object_name = f\"{staged_instructions[table]['OBJECT_NAME']}\",\\\r\n",
					"                                                database_name = f\"{staged_instructions[table]['DATABASE_NAME']}\",\\\r\n",
					"                                                schema_name = f\"{staged_instructions[table]['SCHEMA_NAME']}\",\\\r\n",
					"                                                table = f\"{staged_instructions[table]['TABLE_NAME']}\",\\\r\n",
					"                                                file_type = 'DELTA',\\\r\n",
					"                                                operation = 'WRITE',\\\r\n",
					"                                                start_time = start_time,\\\r\n",
					"                                                merge_type = 'N/A')\r\n",
					"\r\n",
					"            cursor_etl_sql.execute(pushdown_query_success)\r\n",
					"\r\n",
					"\r\n",
					"        # Log Failed ETL WRITE\r\n",
					"        except Exception as err_message:\r\n",
					"            pushdown_query_fail = create_query_failed_etl_log(trigger_time = trigger_time,\\\r\n",
					"                                                            trigger_type = trigger_type,\\\r\n",
					"                                                            pipeline_run_id = pipeline_run_id ,\\\r\n",
					"                                                            pipeline_name = pipeline_name,\\\r\n",
					"                                                            notebook_name = 'nb_lakehouse_orchestration_py',\\\r\n",
					"                                                            source_system_path = f\"{staged_instructions[table]['RAW_PATH']}\",\\\r\n",
					"                                                            target_system_path = f\"{staged_instructions[table]['STAGED_PATH']}\",\\\r\n",
					"                                                            object_name = f\"{staged_instructions[table]['OBJECT_NAME']}\",\\\r\n",
					"                                                            database_name = f\"{staged_instructions[table]['DATABASE_NAME']}\",\\\r\n",
					"                                                            schema_name = f\"{staged_instructions[table]['SCHEMA_NAME']}\",\\\r\n",
					"                                                            table = f\"{staged_instructions[table]['TABLE_NAME']}\",\\\r\n",
					"                                                            file_type = 'DELTA',\\\r\n",
					"                                                            operation = 'WRITE', \\\r\n",
					"                                                            start_time = start_time,\\\r\n",
					"                                                            error_msg = err_message)\r\n",
					"            cursor_etl_sql.execute(pushdown_query_fail)\r\n",
					"            staged_failures.append(table)\r\n",
					"            raise err_message\r\n",
					"\r\n",
					"        \r\n",
					"\r\n",
					"\r\n",
					"    #Else if the file already exist in staged: MERGE\r\n",
					"    elif table in staged_existing_files:\r\n",
					"        \r\n",
					"        delta_table = DeltaTable.forPath(spark, f\"{staged_instructions[table]['STAGED_PATH']}\")\r\n",
					"        \r\n",
					"        # Get the Primary Key statements for the specific table\r\n",
					"        primary_key_statement = staged_instructions[table]['PRIMARY_KEY_STATEMENTS']\r\n",
					"        # Record start_date for the stored procedure\r\n",
					"        start_time = datetime.utcnow()\r\n",
					"        #primary_key = staged_instructions[table]['PRIMARY_KEYS'].lower()\r\n",
					"        try:\r\n",
					"            if load_type == 'CHANGE_TRACKING':\r\n",
					"                try:\r\n",
					"                    df_deletes = df_to_merge.filter(df_to_merge['etl_action'] == 'D')\r\n",
					"                    df_to_merge = df_to_merge.filter(df_to_merge['etl_action'] != 'D')\r\n",
					"                    delta_table.alias(f\"{table}_delta\").merge(\r\n",
					"                    df_to_merge.alias(f\"{table}_parquet\"),\r\n",
					"                    f\"{primary_key_statement}\")\\\r\n",
					"                    .whenNotMatchedInsertAll()\\\r\n",
					"                    .whenMatchedUpdateAll()\\\r\n",
					"                    .execute()\r\n",
					"                except Exception as e:\r\n",
					"                    raise e\r\n",
					"                    #logging.info(\"Attempting to delete records failed\")\r\n",
					"                    #logging.error(f\" *** {table} has failed while attempting to merge using CHANGE TRACKING ***\")\r\n",
					"                try:\r\n",
					"                    delta_table.alias(f\"{table}_delta\").merge(\r\n",
					"                    df_deletes.alias(f\"{table}_parquet\"),\r\n",
					"                    f\"{primary_key_statement}\")\\\r\n",
					"                    .whenMatchedDelete()\\\r\n",
					"                    .execute() \r\n",
					"                except Exception as e:\r\n",
					"                    #logging.info(\"Attempting to delete records failed\")\r\n",
					"                    #logging.error(f\" *** {table} has failed while attempting to delete using CHANGE TRACKING ***\")\r\n",
					"                    raise e\r\n",
					"            else:\r\n",
					"                try:\r\n",
					"                    # Try to do a merge on primary key for dimension\r\n",
					"                    delta_table.alias(f\"{table}_delta\").merge(\r\n",
					"                    df_to_merge.alias(f\"{table}_parquet\"),\r\n",
					"                    f\"{primary_key_statement}\")\\\r\n",
					"                    .whenNotMatchedInsertAll()\\\r\n",
					"                    .whenMatchedUpdateAll()\\\r\n",
					"                    .execute()\r\n",
					"                except Exception as e:\r\n",
					"                    #logging.info(\"Attempting to delete records failed\")\r\n",
					"                    #logging.error(f\" *** {table} has failed while attempting to merge normally***\")\r\n",
					"                    raise e\r\n",
					"\r\n",
					"            # Log Successful ETL MERGE for FACT\r\n",
					"            pushdown_query_success= create_query_successful_etl_log(trigger_time = trigger_time,\\\r\n",
					"                                                trigger_type = trigger_type,\\\r\n",
					"                                                pipeline_run_id = pipeline_run_id ,\\\r\n",
					"                                                pipeline_name = pipeline_name,\\\r\n",
					"                                                notebook_name = 'nb_lakehouse_orchestration_py',\\\r\n",
					"                                                source_system_path = f\"{staged_instructions[table]['RAW_PATH']}\",\\\r\n",
					"                                                target_system_path = f\"{staged_instructions[table]['STAGED_PATH']}\",\\\r\n",
					"                                                object_name = f\"{staged_instructions[table]['OBJECT_NAME']}\",\\\r\n",
					"                                                database_name = f\"{staged_instructions[table]['DATABASE_NAME']}\",\\\r\n",
					"                                                schema_name = f\"{staged_instructions[table]['SCHEMA_NAME']}\",\\\r\n",
					"                                                table = f\"{staged_instructions[table]['TABLE_NAME']}\",\\\r\n",
					"                                                file_type = 'DELTA',\\\r\n",
					"                                                operation = 'MERGE',\\\r\n",
					"                                                start_time = start_time,\\\r\n",
					"                                                merge_type= load_type)\r\n",
					"            cursor_etl_sql.execute(pushdown_query_success)            \r\n",
					"\r\n",
					"\r\n",
					"            #queue_client.update_message(table, pop_receipt=pop_receipt, visibility_timeout=3600)#queue_client.delete_message(table, pop_receipt=pop_receipt) #delete_message(table, pop_receipt=pop_receipt)\r\n",
					"\r\n",
					"        # Log failed ETL MERGE for FACT\r\n",
					"        except Exception as err_message:\r\n",
					"            pushdown_query_fail = create_query_failed_etl_log(trigger_time = trigger_time,\\\r\n",
					"                                                            trigger_type = trigger_type,\\\r\n",
					"                                                            pipeline_run_id = pipeline_run_id,\\\r\n",
					"                                                            pipeline_name = pipeline_name,\\\r\n",
					"                                                            notebook_name = 'nb_lakehouse_orchestration_py',\\\r\n",
					"                                                            source_system_path = f\"{staged_instructions[table]['RAW_PATH']}\",\\\r\n",
					"                                                            target_system_path = f\"{staged_instructions[table]['STAGED_PATH']}\",\\\r\n",
					"                                                            object_name = f\"{staged_instructions[table]['OBJECT_NAME']}\",\\\r\n",
					"                                                            database_name = f\"{staged_instructions[table]['DATABASE_NAME']}\",\\\r\n",
					"                                                            schema_name = f\"{staged_instructions[table]['SCHEMA_NAME']}\",\\\r\n",
					"                                                            table = f\"{staged_instructions[table]['TABLE_NAME']}\",\\\r\n",
					"                                                            file_type = 'DELTA',\\\r\n",
					"                                                            operation = 'MERGE',\\\r\n",
					"                                                            start_time = start_time,\\\r\n",
					"                                                            error_msg = err_message)\r\n",
					"            cursor_etl_sql.execute(pushdown_query_fail)\r\n",
					"            staged_failures.append(table)\r\n",
					"            #logging.info(table + ' has failed to merge ')\r\n",
					"            #logging.info(err_message)\r\n",
					"            raise err_message"
				],
				"execution_count": null
			}
		]
	}
}