{
	"name": "nb_util_sql_staged_etl_and_logging_py",
	"properties": {
		"folder": {
			"name": "utils"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "012fa9dc-fd32-4a84-a99f-59c22acd61d5"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## SDLH (Strategic Data Lakehouse) Utility\n",
					"<p><b>Description: </b>Enter notebook decription</p>\n",
					"<b>Parent Process: </b>SDLH Pipelines</p>\n",
					"<table align=\"left\">\n",
					" <thead>\n",
					"  <tr>\n",
					"   <th>Contributor</th>\n",
					"   <th>Date</th>\n",
					"   <th>Version</th>\n",
					"   <th>Comment</th>\n",
					"   <th>WorkItem No</th>\n",
					"  </tr>\n",
					" </thead>\n",
					" <tbody>\n",
					"  <tr>\n",
					"   <td>Andrei Dumitru</td>\n",
					"   <td>2022-12-09</td>\n",
					"   <td>1.0</td>\n",
					"   <td>Create initial release</td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Darren Price</td>\n",
					"   <td>2023-10-23</td>\n",
					"   <td>2.0</td>\n",
					"   <td>Updated for SDLH v2.0</td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Darren Price</td>\n",
					"   <td>2024-01-09</td>\n",
					"   <td>2.1</td>\n",
					"   <td>Updated for SDLH v2.1</td>\n",
					"   <td></td>\n",
					"  <tr>\n",
					"   <td>Darren Price</td>\n",
					"   <td>2024-03-08</td>\n",
					"   <td>2.2.0</td>\n",
					"   <td>Updated for SDLH v2.2.0</td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Andrei Dumitru</td>\n",
					"   <td>2024-03-18</td>\n",
					"   <td>2.2.2</td>\n",
					"   <td>Updated for SDLH v2.2.2</td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Darren Price</td>\n",
					"   <td>2024-05-07</td>\n",
					"   <td>2.2.3</td>\n",
					"   <td>Column INFO schema removed reference to IS_NULLABLE</td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Andrei Dumitru</td>\n",
					"   <td>2024-07-24</td>\n",
					"   <td>2.2.14</td>\n",
					"   <td>Updated config for etl_action column to decide correct action for <br>FULL and WATERMARK loads, no update to CHANGE_TRACKING.<br>  This feature ensures we provide and acurate CHAR I=Insert or U=UPDATE</td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Andrei Dumitru</td>\n",
					"   <td>2024-08-16</td>\n",
					"   <td>2.2.4</td>\n",
					"   <td>Added an extra parameter \"spark_version:float\" to the \"get_raw_staged_etl_instructions\" function.<br> This function does mounting, which is slightly different between Spark 3.3 and 3.4. <br>It now handles both, by dynamically receiving the parameter based on the Spark pool. <br>The notebook is now compatible with Spark 3.4 on Synapse.</td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Andrei Dumitru</td>\n",
					"   <td>2024-10-01</td>\n",
					"   <td>2.2.15</td>\n",
					"   <td>Added an extra parameter \"spark_version:float\" to the \"raw_to_staged_etl_plus_logging\" function.<br> Updated the function to produce etl_action before etl_timestamp columns. <br> Added SOURCE_TYPE column as a result of the information schema metadata unpacking.</td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Darren Price</td>\n",
					"   <td>2024-10-11</td>\n",
					"   <td>2.3.0</td>\n",
					"   <td>Updated notebook with 4 new parameters to support: <p>(a) Capibility to support different datalake container names.<p>(b) Capibility to support recreating external tables.\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Andrei Dumitru</td>\n",
					"   <td>2024-10-15</td>\n",
					"   <td>2.3.2</td>\n",
					"   <td>Updated notebook with a series of functions to support CDC data: <p>(a) rename_etl_action_column - in case the action column can't be renamed from the source to etl_action, <br>it's done through the metadata and this function.<p>(b) keep_latest_update - takes in 1 or more PK's and 1 or more sequencing (watermark) columns <br>to determine and keep only the latest update per primary key for the incremental data ingested.<p>(c) Updated the main ETL function to use these functions to deal with CDC data.\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Andrei Dumitru</td>\n",
					"   <td>2024-10-29</td>\n",
					"   <td>2.3.4</td>\n",
					"   <td>Updated the metadata and ETL functions to add support for schema evolution where required.\n",
					"   <td></td>\n",
					"  </tr>\n",
					" </tbody>\n",
					"</table>"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#not logged\n",
					"def query_db_jdbc(jdbc_url:str, jdbc_token:str, pushdown_query:str) -> pyspark.sql.dataframe.DataFrame:\n",
					"    \"\"\"\n",
					"    Queries a SQL Server database using JDBC and returns a PySpark DataFrame.\n",
					"\n",
					"    Parameters:\n",
					"    - jdbc_hostname (str): the hostname or IP address of the SQL Server instance\n",
					"    - jdbc_database (str): the name of the database to connect to\n",
					"    - jdbc_port (str): the port number on which the SQL Server instance is listening\n",
					"    - jdbc_options (str): any options to add to the connection\n",
					"    - jdbc_token (str): the managed identity token to authenticate with\n",
					"    - pushdown_query (str): a SQL query to execute in the database (with optional filters and transformations)\n",
					"\n",
					"    Returns:\n",
					"    - df (pyspark.sql.dataframe.DataFrame): a PySpark DataFrame containing the result set of the query, with an additional \"current_date\" column that contains the current date.\n",
					"\n",
					"    \"\"\"\n",
					"    # Create connection properties, uses generated managed identity token\n",
					"    connection_properties = {\n",
					"      \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
					"      \"accessToken\" : jdbc_token\n",
					"    }\n",
					"\n",
					"    # Use PySpark's built-in JDBC reader to execute the query and create a DataFrame and add current_date column\n",
					"    df = spark.read.jdbc(url=jdbc_url, table=pushdown_query, properties=connection_properties).withColumn(\"current_date\", current_date())\n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def query_db_odbc(odbc_url:str, odbc_token:str, pushdown_query:str) -> pyspark.sql.dataframe.DataFrame:\n",
					"    \"\"\"\n",
					"    Queries a SQL Server database using ODBC and returns a PySpark DataFrame.\n",
					"\n",
					"    Parameters:\n",
					"    - odbc_hostname (str): the hostname or IP address of the SQL Server instance\n",
					"    - odbc_database (str): the name of the database to connect to\n",
					"    - odbc_port (str): the port number on which the SQL Server instance is listening\n",
					"    - odbc_options (str): any options to add to the connection\n",
					"    - odbc_token (str): the managed identity token to authenticate with\n",
					"    - pushdown_query (str): a SQL query to execute in the database (with optional filters and transformations)\n",
					"\n",
					"    Returns:\n",
					"    - df (pyspark.sql.dataframe.DataFrame): a PySpark DataFrame containing the result set of the stored procedure.\n",
					"\n",
					"    \"\"\"\n",
					"\n",
					"    # Create connection properties, uses generated managed identity token\n",
					"    connection_properties = pyodbc.connect(odbc_url, attrs_before = { SQL_COPT_SS_ACCESS_TOKEN:odbc_token })\n",
					"\n",
					"    # Read the data into a Pandas dataframe\n",
					"    df_pd = pd.read_sql(pushdown_query, connection_properties)\n",
					"\n",
					"    # Create a SparkSession\n",
					"    spark = SparkSession.builder.appName(\"Pandas to Spark\").getOrCreate()\n",
					"\n",
					"    # Convert the Pandas dataframe to a Spark dataframe\n",
					"    df_spark = spark.createDataFrame(df_pd)\n",
					"\n",
					"    # Show the Spark dataframe\n",
					"    return df_spark"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_raw_staged_etl_instructions(adls_name:str, adls_container_raw:str, adls_container_enriched:str, spark_version:float) -> dict:\n",
					"    \"\"\"\n",
					"    Given the ADLS container and name, it'll take the df_objects_metadata_extracted dataframe and parse all the info into a python\n",
					"    dictionary staged_instructions. This dictionary contains all the info needed to perform ETL from raw to enriched.\n",
					"\n",
					"    Parameters:\n",
					"    - adls_container_enriched (str): The enriched storage account container name as a string.\n",
					"    - adls_name (str): The storage account name as a string.\n",
					"\n",
					"    Returns:\n",
					"    - staged_instructions (dict): Dictionary containing raw to enriched ETL instructions.\n",
					"\n",
					"    \"\"\"\n",
					"    # Mount onto the enriched container\n",
					"    JOB_ID = synapse_adls_remount(adls_container_enriched, adls_name, \"datalake_data_azir\")\n",
					"\n",
					"    # Get the mounting prefix based on Spark version\n",
					"    if spark_version <= 3.3:\n",
					"        mounting_prefix = 'synfs:'\n",
					"    elif spark_version >= 3.4:\n",
					"        mounting_prefix = 'synfs:/notebook'\n",
					"\n",
					"    # Get tables that have already been migrated from the raw_migrated_tables_log DataFrame\n",
					"    tables_migrated_raw = df_objects_metadata_extracted.selectExpr(\"OBJECT_NAME\", \\\n",
					"                                                        \"SOURCE_SYSTEM\", \\\n",
					"                                                        \"DATABASE_NAME\",\\\n",
					"                                                        \"SCHEMA_NAME\",\\\n",
					"                                                        \"TABLE_NAME\",  \\\n",
					"                                                        \"SERVERLESS_SQL_POOL_DATABASE\",  \\\n",
					"                                                        \"SERVERLESS_SQL_POOL_SCHEMA\",  \\\n",
					"                                                        \"LOAD_TYPE\", \\\n",
					"                                                        \"PRIMARY_KEYS\", \\\n",
					"                                                        \"WATERMARK_COLUMN\", \\\n",
					"                                                        \"ETL_ACTION_COLUMN\", \\\n",
					"                                                        f\"CONCAT('abfss://{adls_container_raw}@{adls_name}.dfs.core.windows.net/', FP1_RAW) AS RAW_PATH\",\\\n",
					"                                                        f\"CONCAT('{mounting_prefix}/{JOB_ID}/mount/', FP1_STAGED) AS STAGED_PATH\")\\\n",
					"                                  .rdd.map(lambda x: (x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7], x[8], x[9], x[10], x[11], x[12])).collect()\n",
					"    tables_migrated_raw=list(tables_migrated_raw)\n",
					"\n",
					"    # Construct the data path for each table and store it in a dictionary\n",
					"    staged_instructions = {}\n",
					"    primary_key_lookup = {}\n",
					"    # Compose primary key statement for delta merges\n",
					"    for row in df_objects_metadata_extracted.select(\"TABLE_NAME\", \"PRIMARY_KEYS\").filter(df_objects_metadata_extracted.PRIMARY_KEYS.isNotNull()).distinct().collect():\n",
					"        primary_key_lookup[row.TABLE_NAME] = row.PRIMARY_KEYS.split(\",\")\n",
					"\n",
					"    # Parse and populate the python dictionary\n",
					"    for OBJECT_NAME, SOURCE_SYSTEM, DATABASE_NAME, SCHEMA_NAME, TABLE_NAME, SERVERLESS_SQL_POOL_DATABASE, SERVERLESS_SQL_POOL_SCHEMA, LOAD_TYPE, PRIMARY_KEYS, WATERMARK_COLUMN, ETL_ACTION_COLUMN, RAW_PATH, STAGED_PATH in tables_migrated_raw:\n",
					"        data_path = f\"{RAW_PATH}\"\n",
					"        if LOAD_TYPE == \"FULL_LOAD\":\n",
					"            table_primary_key_statements = [\"NULL\"]\n",
					"        else:\n",
					"            # Simplify string concatenation using f-strings\n",
					"            primary_keys = primary_key_lookup[TABLE_NAME]\n",
					"            table_primary_key_statements = [f\"{TABLE_NAME}_parquet.{pk} = {TABLE_NAME}_delta.{pk}\".lower() for pk in primary_keys]\n",
					"            table_primary_key_statements = ' and '.join(table_primary_key_statements)\n",
					"            \n",
					"        staged_instructions[TABLE_NAME] = {\"OBJECT_NAME\":OBJECT_NAME, \"SOURCE_SYSTEM\":SOURCE_SYSTEM, \"DATABASE_NAME\": DATABASE_NAME, \"SCHEMA_NAME\": SCHEMA_NAME, \"TABLE_NAME\": TABLE_NAME, \"SERVERLESS_SQL_POOL_DATABASE\":SERVERLESS_SQL_POOL_DATABASE, \"SERVERLESS_SQL_POOL_SCHEMA\":SERVERLESS_SQL_POOL_SCHEMA, \"LOAD_TYPE\":LOAD_TYPE, \"PRIMARY_KEYS\":PRIMARY_KEYS , \"WATERMARK_COLUMN\":WATERMARK_COLUMN , \"ETL_ACTION_COLUMN\":ETL_ACTION_COLUMN , \"PRIMARY_KEY_STATEMENTS\": table_primary_key_statements , \"RAW_PATH\":RAW_PATH, \"STAGED_PATH\":STAGED_PATH}\n",
					"    return staged_instructions"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def synapse_adls_remount(file_system_name:str, datalake_name:str, linked_service_name:str)->str:\n",
					"    \"\"\"\n",
					"    Mounts an Azure Data Lake Storage Gen2 filesystem using the specified file_system_name, datalake_name, and \n",
					"    linked_service_name. The mounted filesystem will be available at \"/mount\" in the current workspace.\n",
					"    \n",
					"    Parameters:\n",
					"    - file_system_name (str): The name of the filesystem to mount.\n",
					"    - datalake_name (str): The name of the Azure Data Lake Storage Gen2 account.\n",
					"    - linked_service_name (str): The name of the Azure Synapse linked service to use for mounting the filesystem.\n",
					"    \n",
					"    Returns:\n",
					"    - str: The job ID of the current Synapse Analytics job.\n",
					"    \n",
					"    \"\"\"\n",
					"    # Unmounting any existing mount points\n",
					"    mssparkutils.fs.unmount(\"/mount\")\n",
					"\n",
					"    # Mounting the specified filesystem\n",
					"    mssparkutils.fs.mount( \n",
					"        f\"abfss://{file_system_name}@{datalake_name}.dfs.core.windows.net\", \n",
					"        \"/mount\", \n",
					"        {\"linkedService\":f\"{linked_service_name}\"} \n",
					"    )\n",
					"    \n",
					"    # Returning the current job ID\n",
					"    JOB_ID = mssparkutils.env.getJobId()\n",
					"    return JOB_ID"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def deserialize_json_meta_to_notebook_etl_relational_meta(odbc_url:str, odbc_token:str) -> pyspark.sql.dataframe.DataFrame:\n",
					"    \"\"\"\n",
					"    Deserializes JSON metadata from a specified database using the provided parameters and returns two Spark dataframes. \n",
					"    The first dataframe contains extracted object metadata, such as server and database names, table and column names, and file paths. \n",
					"    The second dataframe contains information schema metadata, such as column names, data types, and character maximum lengths.\n",
					"\n",
					"    Parameters:\n",
					"    - odbc_hostname (str): the hostname or IP address of the SQL Server instance\n",
					"    - odbc_database (str): the name of the database to connect to\n",
					"    - odbc_port (str): the port number on which the SQL Server instance is listening\n",
					"    - odbc_options (str): any options to add to the connection\n",
					"    - odbc_token (str): the managed identity token to authenticate with\n",
					"\n",
					"    Returns:\n",
					"    (df_objects_metadata_extracted (pyspark.sql.dataframe.DataFrame), df_info_schema (pyspark.sql.dataframe.DataFrame)) (tuple):\n",
					"    A tuple of two Spark dataframes. The first dataframe contains extracted object metadata, and the second dataframe contains information schema metadata.\n",
					"    \"\"\"\n",
					"\n",
					"    # Define the schema for the JSON string\n",
					"    schema_adls_paths = \"array<struct<raw:array<struct<FP0:string,FP1:string>>,\" \\\n",
					"            \"staged:array<struct<FP0:string,FP1:string>>>>\"\n",
					"\n",
					"    # Define the schema for the OBJECT_PARAMETERS JSON strings from the metadata\n",
					"    schema_object_parameters = \"\"\"\n",
					"    array<\n",
					"    struct<\n",
					"        SOURCE_SYSTEM: string,\n",
					"        SOURCE_GROUPING_ID: int,\n",
					"        SOURCE_SYSTEM_CONNECTION_STRING: string,\n",
					"        DATABASE_NAME: string,\n",
					"        SCHEMA_NAME: string,\n",
					"        TABLE_NAME: string,\n",
					"        PRIMARY_KEYS: string,\n",
					"        WATERMARK_COLUMN: string,\n",
					"        ETL_ACTION_COLUMN: string,\n",
					"        SCHEMA_VERSION: int,\n",
					"        COLUMNS_META: string,\n",
					"        SERVERLESS_SQL_POOL_DATABASE: string,\n",
					"        SERVERLESS_SQL_POOL_SCHEMA: string,\n",
					"        INFORMATION_SCHEMA: array<\n",
					"        struct<\n",
					"            COLUMN_NAME: string,\n",
					"            IS_NULLABLE: string,\n",
					"            DATA_TYPE: string,\n",
					"            CHARACTER_MAXIMUM_LENGTH: bigint,\n",
					"            COLLATION_NAME: string\n",
					"        >\n",
					"        >\n",
					"    >\n",
					"    >\"\"\"\n",
					"\n",
					"    \n",
					"\n",
					"    pushdown_query_select_json_metadata = f\"\"\"EXEC [ETL].[usp_GetPipelineMetadataRetrievalFull] \n",
					"                                            @PARAM_DATETIME = '{PARAM_TRIGGER_TIME}'\n",
					"                                            ,@PARAM_SOURCE_TYPE = '{PARAM_SOURCE_TYPE}'\n",
					"                                            ,@PARAM_SOURCE_SYSTEM = '{PARAM_SOURCE_SYSTEM}'\n",
					"                                            ,@PARAM_SOURCE_GROUPING_ID = '{PARAM_SOURCE_GROUPING_ID}'\"\"\"\n",
					"\n",
					"    df_json_metadata = query_db_odbc(odbc_url, odbc_token, pushdown_query_select_json_metadata)\n",
					"\n",
					"    # Parse the JSON string into a struct column\n",
					"    df = df_json_metadata.select(col(\"OBJECT_NAME\"), \\\n",
					"                                col(\"SOURCE_TYPE\"), \\\n",
					"                                col(\"LOAD_TYPE\") ,\\\n",
					"                                from_json(col(\"ADLS_PATHS\"), schema_adls_paths).alias(\"ADLS_PATHS\"), \\\n",
					"                                from_json(col(\"OBJECT_PARAMETERS\"), schema_object_parameters).alias(\"OBJECT_PARAMETERS\"),\\\n",
					"                                col(\"HNS\"))\n",
					"    # Extract the values of FP0 and FP1 from the raw array\n",
					"    df_objects_metadata_extracted = df.selectExpr(\"OBJECT_NAME\",\\\n",
					"                                \"OBJECT_PARAMETERS[0].SOURCE_SYSTEM AS SOURCE_SYSTEM\",\\\n",
					"                                \"OBJECT_PARAMETERS[0].DATABASE_NAME AS DATABASE_NAME\",\\\n",
					"                                \"OBJECT_PARAMETERS[0].SCHEMA_NAME AS SCHEMA_NAME\",\\\n",
					"                                \"OBJECT_PARAMETERS[0].TABLE_NAME AS TABLE_NAME\",\\\n",
					"                                \"OBJECT_PARAMETERS[0].SERVERLESS_SQL_POOL_DATABASE AS SERVERLESS_SQL_POOL_DATABASE\",\\\n",
					"                                \"OBJECT_PARAMETERS[0].SERVERLESS_SQL_POOL_SCHEMA AS SERVERLESS_SQL_POOL_SCHEMA\",\\\n",
					"                                \"LOAD_TYPE\",\\\n",
					"                                \"OBJECT_PARAMETERS[0].INFORMATION_SCHEMA AS INFORMATION_SCHEMA\",\\\n",
					"                                \"CASE WHEN LOAD_TYPE = 'FULL_LOAD' THEN NULL ELSE OBJECT_PARAMETERS[0].PRIMARY_KEYS END AS PRIMARY_KEYS \", \\\n",
					"                                \"CASE WHEN LOAD_TYPE = 'FULL_LOAD' THEN NULL ELSE OBJECT_PARAMETERS[0].WATERMARK_COLUMN END AS WATERMARK_COLUMN \", \\\n",
					"                                \"OBJECT_PARAMETERS[0].ETL_ACTION_COLUMN AS ETL_ACTION_COLUMN\",\\\n",
					"                                \"ADLS_PATHS[0].raw[0].FP0 AS FP0_RAW \", \\\n",
					"                                \"concat(ADLS_PATHS.raw[0].FP1[0], HNS) as FP1_RAW\", \\\n",
					"                                \"ADLS_PATHS[0].staged[0].FP0 as FP0_STAGED\", \\\n",
					"                                \"ADLS_PATHS.staged[0].FP1[0] as FP1_STAGED\")\n",
					"\n",
					"    from pyspark.sql.functions import explode,arrays_zip\n",
					"    df_info_schema = df.select(df.OBJECT_NAME,\\\n",
					"                                df.SOURCE_TYPE,\\\n",
					"                                df.OBJECT_PARAMETERS.SOURCE_SYSTEM[0].alias(\"SOURCE_SYSTEM\"),\\\n",
					"                                df.OBJECT_PARAMETERS.DATABASE_NAME[0].alias(\"DATABASE_NAME\"),\\\n",
					"                                df.OBJECT_PARAMETERS.SCHEMA_NAME[0].alias(\"SCHEMA_NAME\"),\\\n",
					"                                df.OBJECT_PARAMETERS.TABLE_NAME[0].alias(\"TABLE_NAME\"),\\\n",
					"                                explode(df.OBJECT_PARAMETERS.INFORMATION_SCHEMA[0]).alias(\"COLUMN_INFO\"))\n",
					"\n",
					"    df_info_schema = df_info_schema.select(\n",
					"                                        col(\"OBJECT_NAME\"),\\\n",
					"                                        col(\"SOURCE_SYSTEM\"),\\\n",
					"                                        col(\"SOURCE_TYPE\"),\\\n",
					"                                        col(\"DATABASE_NAME\"),\\\n",
					"                                        col(\"SCHEMA_NAME\"),\\\n",
					"                                        col(\"TABLE_NAME\"),\\\n",
					"                                        col(\"COLUMN_INFO.COLUMN_NAME\"), \\\n",
					"                                        col(\"COLUMN_INFO.DATA_TYPE\"), \\\n",
					"                                        col(\"COLUMN_INFO.CHARACTER_MAXIMUM_LENGTH\"))\n",
					"                                        \n",
					"    return df_objects_metadata_extracted, df_info_schema"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#secondary\n",
					"def create_query_successful_etl_log(trigger_time:str, trigger_type:str, pipeline_run_id:str, pipeline_name:str, notebook_name:str, source_system_path:str, target_system_path:str, object_name:str, database_name:str, schema_name:str, table:str, file_type:str, operation:str, start_time:datetime, merge_type:str) -> str:\n",
					"    \"\"\"Creates a SQL query to insert a log of a successful ETL operation into a database.\n",
					"\n",
					"    Parameters:\n",
					"        trigger_time (datetime): The date and time the ETL operation was triggered.\n",
					"        trigger_type (str): The type of trigger that started the ETL operation.\n",
					"        pipeline_run_id (str): The unique ID of the pipeline run that triggered the ETL operation.\n",
					"        pipeline_name (str): The name of the pipeline that triggered the ETL operation.\n",
					"        notebook_name (str): The name of the notebook or script that performed the ETL operation.\n",
					"        source_system_path (str): The path to the source system data.\n",
					"        target_system_path (str): The path to the target system data.\n",
					"        object_name (str): The name of the object containing the target table.\n",
					"        database_name (str): The name of the database containing the target table.\n",
					"        schema_name (str): The name of the schema containing the target table.\n",
					"        table (str): The name of the target table.\n",
					"        file_type (str): The type of file used for the target system data.\n",
					"        operation (str): The type of operation performed on the target system data.\n",
					"        start_time (datetime): The date and time the ETL operation started.\n",
					"\n",
					"    Returns:\n",
					"        pushdown_query (str): A SQL query to insert a log of the ETL operation into a database.\n",
					"\n",
					"    \"\"\"\n",
					"    if file_type == 'DELTA':\n",
					"        if operation == 'WRITE':\n",
					"            end_time = datetime.utcnow()\n",
					"            deltaTime = end_time-start_time\n",
					"            deltaTime = deltaTime.total_seconds()\n",
					"            deltaTime = int(deltaTime)\n",
					"            delta_table = DeltaTable.forPath(spark, f\"{staged_instructions[table]['STAGED_PATH']}\")\n",
					"            lastOperationDF = delta_table.history(1)\n",
					"            operation_metrics = lastOperationDF.select(F.explode(lastOperationDF.operationMetrics))\n",
					"            numOutputRows = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numOutputRows')\n",
					"            numOutputRows = numOutputRows.rdd.flatMap(lambda x:x).collect()[0]\n",
					"\n",
					"            pushdown_query = f\"\"\"EXEC [ETL].[usp_InsertLog]\n",
					"                                @SOURCE_TYPE = '{PARAM_SOURCE_TYPE}'\n",
					"                                ,@SOURCE_SYSTEM = '{PARAM_SOURCE_SYSTEM}'\n",
					"                                ,@SOURCE_GROUPING_ID = '{PARAM_SOURCE_GROUPING_ID}'\n",
					"                                ,@OBJECT_NAME = '{object_name}'\n",
					"                                ,@DATABASE_NAME = '{database_name}'\n",
					"                                ,@SCHEMA_NAME = '{schema_name}'\n",
					"                                ,@TABLE_NAME = '{table}'\n",
					"                                ,@TRIGGER_TIME = '{trigger_time}'\n",
					"                                ,@TRIGGER_TYPE = '{trigger_type}'\n",
					"                                ,@PIPELINE_RUN_ID = '{pipeline_run_id}'\n",
					"                                ,@PIPELINE_NAME = '{pipeline_name}'\n",
					"                                ,@TRIGGERING_PIPELINE_RUN_ID = '{PARAM_TRIGGERING_PIPELINE_RUN_ID}'\n",
					"                                ,@TRIGGERING_PIPELINE_NAME = '{PARAM_TRIGGERING_PIPELINE_NAME}'\n",
					"                                ,@OPERATION = '{operation}'\n",
					"                                ,@COMPONENT_NAME = '{notebook_name}'\n",
					"                                ,@TARGET_SYSTEM = '{target_system_path}'\n",
					"                                ,@START_TIME = '{start_time}'\n",
					"                                ,@END_TIME =  '{end_time}'\n",
					"                                ,@DURATION_SECONDS = {deltaTime}\n",
					"                                ,@INSERTS = {numOutputRows}\n",
					"                                ,@UPDATES = 0\n",
					"                                ,@DELETES = 0\n",
					"                                ,@ERROR_MESSAGE = NULL \n",
					"                                ,@NEW_INCREMENTAL_KEY_VALUE = NULL \n",
					"                                ,@PREVIOUS_INCREMENTAL_KEY_VALUE = NULL \"\"\"\n",
					"            return pushdown_query\n",
					"        elif operation == 'MERGE':\n",
					"            end_time = datetime.utcnow()\n",
					"            deltaTime = end_time-start_time\n",
					"            deltaTime = deltaTime.total_seconds()\n",
					"            deltaTime = int(deltaTime)\n",
					"            delta_table = DeltaTable.forPath(spark, f\"{staged_instructions[table]['STAGED_PATH']}\")\n",
					"            lastMergeOperationDF = delta_table.history(2)\n",
					"            operation_metrics = lastMergeOperationDF.select(F.explode(lastMergeOperationDF.operationMetrics), lastMergeOperationDF.version)\n",
					"            if merge_type == 'CHANGE_TRACKING':\n",
					"                numRowsInserted = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numTargetRowsInserted').rdd.flatMap(lambda x:x).collect()[1]\n",
					"                try:\n",
					"                    numRowsDeleted = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numTargetRowsDeleted').rdd.flatMap(lambda x:x).collect()[0]\n",
					"                except IndexError: \n",
					"                    numRowsDeleted = 0\n",
					"                numRowsUpdated = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numTargetRowsUpdated').rdd.flatMap(lambda x:x).collect()[1]\n",
					"            else:\n",
					"                numRowsInserted = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numTargetRowsInserted').rdd.flatMap(lambda x:x).collect()[0]\n",
					"                numRowsDeleted = 0\n",
					"                numRowsUpdated = operation_metrics.select(operation_metrics.value).where(operation_metrics.key == 'numTargetRowsUpdated').rdd.flatMap(lambda x:x).collect()[0]\n",
					"\n",
					"            pushdown_query = f\"\"\"EXEC [ETL].[usp_InsertLog]\n",
					"                                @SOURCE_TYPE = '{PARAM_SOURCE_TYPE}'\n",
					"                                ,@SOURCE_SYSTEM = '{PARAM_SOURCE_SYSTEM}'\n",
					"                                ,@SOURCE_GROUPING_ID = '{PARAM_SOURCE_GROUPING_ID}'\n",
					"                                ,@OBJECT_NAME = '{object_name}'\n",
					"                                ,@DATABASE_NAME = '{database_name}'\n",
					"                                ,@SCHEMA_NAME = '{schema_name}'\n",
					"                                ,@TABLE_NAME = '{table}'\n",
					"                                ,@TRIGGER_TIME = '{trigger_time}'\n",
					"                                ,@TRIGGER_TYPE = '{trigger_type}'\n",
					"                                ,@PIPELINE_RUN_ID = '{pipeline_run_id}'\n",
					"                                ,@PIPELINE_NAME = '{pipeline_name}'\n",
					"                                ,@TRIGGERING_PIPELINE_RUN_ID = '{PARAM_TRIGGERING_PIPELINE_RUN_ID}'\n",
					"                                ,@TRIGGERING_PIPELINE_NAME = '{PARAM_TRIGGERING_PIPELINE_NAME}'\n",
					"                                ,@OPERATION = '{operation}'\n",
					"                                ,@COMPONENT_NAME = '{notebook_name}'\n",
					"                                ,@TARGET_SYSTEM = '{target_system_path}'\n",
					"                                ,@START_TIME = '{start_time}'\n",
					"                                ,@END_TIME =  '{end_time}'\n",
					"                                ,@DURATION_SECONDS = {deltaTime}\n",
					"                                ,@INSERTS = {numRowsInserted}\n",
					"                                ,@UPDATES = {numRowsUpdated}\n",
					"                                ,@DELETES = {numRowsDeleted}\n",
					"                                ,@ERROR_MESSAGE = NULL \n",
					"                                ,@NEW_INCREMENTAL_KEY_VALUE = NULL \n",
					"                                ,@PREVIOUS_INCREMENTAL_KEY_VALUE = NULL \"\"\"\n",
					"            return pushdown_query\n",
					"    \n",
					"    elif file_type == 'PARQUET':\n",
					"        if operation == 'WRITE':\n",
					"            # Gathering Successful WRITE use-case specific parameters for the Stored Procedure string\n",
					"            end_time = datetime.utcnow()\n",
					"            deltaTime = end_time-start_time\n",
					"            deltaTime = deltaTime.total_seconds()\n",
					"            deltaTime = int(deltaTime)\n",
					"            parquet_file = spark.read.parquet(f\"{target_system_path}\")\n",
					"            numOutputRows = parquet_file.count()\n",
					"\n",
					"            # Passing the parameters to the stored procedure string\n",
					"            pushdown_query = f\"\"\"EXEC [ETL].[usp_InsertLog]\n",
					"                                @SOURCE_TYPE = '{PARAM_SOURCE_TYPE}'\n",
					"                                ,@SOURCE_SYSTEM = '{PARAM_SOURCE_SYSTEM}'\n",
					"                                ,@SOURCE_GROUPING_ID = '{PARAM_SOURCE_GROUPING_ID}'\n",
					"                                ,@OBJECT_NAME = '{object_name}'\n",
					"                                ,@DATABASE_NAME = '{database_name}'\n",
					"                                ,@SCHEMA_NAME = '{schema_name}'\n",
					"                                ,@TABLE_NAME = '{table}'\n",
					"                                ,@TRIGGER_TIME = '{trigger_time}'\n",
					"                                ,@TRIGGER_TYPE = '{trigger_type}'\n",
					"                                ,@PIPELINE_RUN_ID = '{pipeline_run_id}'\n",
					"                                ,@PIPELINE_NAME = '{pipeline_name}'\n",
					"                                ,@TRIGGERING_PIPELINE_RUN_ID = '{PARAM_TRIGGERING_PIPELINE_RUN_ID}'\n",
					"                                ,@TRIGGERING_PIPELINE_NAME = '{PARAM_TRIGGERING_PIPELINE_NAME}'\n",
					"                                ,@OPERATION = '{operation}'\n",
					"                                ,@COMPONENT_NAME = '{notebook_name}'\n",
					"                                ,@TARGET_SYSTEM = '{target_system_path}'\n",
					"                                ,@START_TIME = '{start_time}'\n",
					"                                ,@END_TIME =  '{end_time}'\n",
					"                                ,@DURATION_SECONDS = {deltaTime}\n",
					"                                ,@INSERTS = {numOutputRows}\n",
					"                                ,@UPDATES = 0\n",
					"                                ,@DELETES = 0\n",
					"                                ,@ERROR_MESSAGE = NULL \n",
					"                                ,@NEW_INCREMENTAL_KEY_VALUE = NULL \n",
					"                                ,@PREVIOUS_INCREMENTAL_KEY_VALUE = NULL \"\"\"\n",
					"            # Returning the query as a string\n",
					"            return pushdown_query\n",
					"\n",
					"#secondary\n",
					"def create_query_failed_etl_log(trigger_time:str, trigger_type:str, pipeline_run_id:str, pipeline_name:str, notebook_name:str, source_system_path:str, target_system_path:str, object_name:str, database_name:str, schema_name:str, table:str, file_type:str, operation:str, start_time:datetime, error_msg:str) -> str:\n",
					"    \"\"\"\n",
					"    Creates a stored procedure query to insert ETL log information into the database.\n",
					"\n",
					"    Parameters:\n",
					"        trigger_time (str): The time at which the trigger occurred.\n",
					"        trigger_type (str): The type of the trigger that caused the ETL process.\n",
					"        pipeline_run_id (str): The ID of the pipeline run.\n",
					"        pipeline_name (str): The name of the pipeline that was executed.\n",
					"        notebook_name (str): The name of the notebook that ran the ETL process.\n",
					"        source_system_path (str): The path to the source system.\n",
					"        target_system_path (str): The path to the target system.\n",
					"        object_name (str): The name of the object containing the target table.\n",
					"        database_name (str): The name of the database containing the target table.\n",
					"        schema_name (str): The name of the schema containing the target table.\n",
					"        table (str): The name of the target table.\n",
					"        file_type (str): The type of file that was processed (e.g. PARQUET, DELTA).\n",
					"        operation (str): The type of operation that was performed (e.g. WRITE, MERGE).\n",
					"        start_time (datetime.datetime): The time at which the ETL process began.\n",
					"        error_msg (str): The error message that was encountered during the ETL process.\n",
					"\n",
					"    Returns:\n",
					"        pushdown_query (str): The stored procedure query as a string.\n",
					"    \"\"\"\n",
					"    if file_type == 'DELTA':\n",
					"        if operation == 'WRITE':\n",
					"            end_time = datetime.utcnow()\n",
					"            deltaTime = end_time-start_time\n",
					"            deltaTime = deltaTime.total_seconds()\n",
					"            deltaTime = int(deltaTime)\n",
					"\n",
					"            pushdown_query = f\"\"\"EXEC [ETL].[usp_InsertLog]\n",
					"                                @SOURCE_TYPE = '{PARAM_SOURCE_TYPE}'\n",
					"                                ,@SOURCE_SYSTEM = '{PARAM_SOURCE_SYSTEM}'\n",
					"                                ,@SOURCE_GROUPING_ID = '{PARAM_SOURCE_GROUPING_ID}'\n",
					"                                ,@OBJECT_NAME = '{object_name}'\n",
					"                                ,@DATABASE_NAME = '{database_name}'\n",
					"                                ,@SCHEMA_NAME = '{schema_name}'\n",
					"                                ,@TABLE_NAME = '{table}'\n",
					"                                ,@TRIGGER_TIME = '{trigger_time}'\n",
					"                                ,@TRIGGER_TYPE = '{trigger_type}'\n",
					"                                ,@PIPELINE_RUN_ID = '{pipeline_run_id}'\n",
					"                                ,@PIPELINE_NAME = '{pipeline_name}'\n",
					"                                ,@TRIGGERING_PIPELINE_RUN_ID = '{PARAM_TRIGGERING_PIPELINE_RUN_ID}'\n",
					"                                ,@TRIGGERING_PIPELINE_NAME = '{PARAM_TRIGGERING_PIPELINE_NAME}'\n",
					"                                ,@OPERATION = '{operation}'\n",
					"                                ,@COMPONENT_NAME = '{notebook_name}'\n",
					"                                ,@TARGET_SYSTEM = '{target_system_path}'\n",
					"                                ,@START_TIME = '{start_time}'\n",
					"                                ,@END_TIME =  '{end_time}'\n",
					"                                ,@DURATION_SECONDS = {deltaTime}\n",
					"                                ,@INSERTS = 0\n",
					"                                ,@UPDATES = 0\n",
					"                                ,@DELETES = 0\n",
					"                                ,@ERROR_MESSAGE = '{error_msg}'\n",
					"                                ,@NEW_INCREMENTAL_KEY_VALUE = NULL\n",
					"                                ,@PREVIOUS_INCREMENTAL_KEY_VALUE = NULL \"\"\"\n",
					"            return pushdown_query\n",
					"        elif operation == 'MERGE':\n",
					"            end_time = datetime.utcnow()\n",
					"            deltaTime = end_time-start_time\n",
					"            \n",
					"            deltaTime = deltaTime.total_seconds()\n",
					"            deltaTime = int(deltaTime)\n",
					"\n",
					"            pushdown_query = f\"\"\"EXEC [ETL].[usp_InsertLog]\n",
					"                                @SOURCE_TYPE = '{PARAM_SOURCE_TYPE}'\n",
					"                                ,@SOURCE_SYSTEM = '{PARAM_SOURCE_SYSTEM}'\n",
					"                                ,@SOURCE_GROUPING_ID = '{PARAM_SOURCE_GROUPING_ID}'\n",
					"                                ,@OBJECT_NAME = '{object_name}'\n",
					"                                ,@DATABASE_NAME = '{database_name}'\n",
					"                                ,@SCHEMA_NAME = '{schema_name}'\n",
					"                                ,@TABLE_NAME = '{table}'\n",
					"                                ,@TRIGGER_TIME = '{trigger_time}'\n",
					"                                ,@TRIGGER_TYPE = '{trigger_type}'\n",
					"                                ,@PIPELINE_RUN_ID = '{pipeline_run_id}'\n",
					"                                ,@PIPELINE_NAME = '{pipeline_name}'\n",
					"                                ,@TRIGGERING_PIPELINE_RUN_ID = '{PARAM_TRIGGERING_PIPELINE_RUN_ID}'\n",
					"                                ,@TRIGGERING_PIPELINE_NAME = '{PARAM_TRIGGERING_PIPELINE_NAME}'\n",
					"                                ,@OPERATION = '{operation}'\n",
					"                                ,@COMPONENT_NAME = '{notebook_name}'\n",
					"                                ,@TARGET_SYSTEM = '{target_system_path}'\n",
					"                                ,@START_TIME = '{start_time}'\n",
					"                                ,@END_TIME =  '{end_time}'\n",
					"                                ,@DURATION_SECONDS = {deltaTime}\n",
					"                                ,@INSERTS = 0\n",
					"                                ,@UPDATES = 0\n",
					"                                ,@DELETES = 0\n",
					"                                ,@ERROR_MESSAGE = '{error_msg}'\n",
					"                                ,@NEW_INCREMENTAL_KEY_VALUE = NULL \n",
					"                                ,@PREVIOUS_INCREMENTAL_KEY_VALUE = NULL \"\"\"\n",
					"            return pushdown_query\n",
					"\n",
					"    elif file_type == 'PARQUET':\n",
					"        if operation == 'WRITE':\n",
					"            # Gathering Successful WRITE use-case specific parameters for the Stored Procedure string\n",
					"            end_time = datetime.utcnow()\n",
					"            deltaTime = end_time-start_time\n",
					"            deltaTime = deltaTime.total_seconds()\n",
					"            deltaTime = int(deltaTime)\n",
					"            parquet_file = spark.read.parquet(f\"{target_system_path}\")\n",
					"            numOutputRows = parquet_file.count()\n",
					"\n",
					"            # Passing the parameters to the stored procedure string\n",
					"            pushdown_query = f\"\"\"EXEC [ETL].[usp_InsertLog]\n",
					"                                @SOURCE_TYPE = '{PARAM_SOURCE_TYPE}'\n",
					"                                ,@SOURCE_SYSTEM = '{PARAM_SOURCE_SYSTEM}'\n",
					"                                ,@SOURCE_GROUPING_ID = '{PARAM_SOURCE_GROUPING_ID}'\n",
					"                                ,@OBJECT_NAME = '{object_name}'\n",
					"                                ,@DATABASE_NAME = '{database_name}'\n",
					"                                ,@SCHEMA_NAME = '{schema_name}'\n",
					"                                ,@TABLE_NAME = '{table}'\n",
					"                                ,@TRIGGER_TIME = '{trigger_time}'\n",
					"                                ,@TRIGGER_TYPE = '{trigger_type}'\n",
					"                                ,@PIPELINE_RUN_ID = '{pipeline_run_id}'\n",
					"                                ,@PIPELINE_NAME = '{pipeline_name}'\n",
					"                                ,@TRIGGERING_PIPELINE_RUN_ID = '{PARAM_TRIGGERING_PIPELINE_RUN_ID}'\n",
					"                                ,@TRIGGERING_PIPELINE_NAME = '{PARAM_TRIGGERING_PIPELINE_NAME}'\n",
					"                                ,@OPERATION = '{operation}'\n",
					"                                ,@COMPONENT_NAME = '{notebook_name}'\n",
					"                                ,@TARGET_SYSTEM = '{target_system_path}'\n",
					"                                ,@START_TIME = '{start_time}'\n",
					"                                ,@END_TIME =  '{end_time}'\n",
					"                                ,@DURATION_SECONDS = {deltaTime}\n",
					"                                ,@INSERTS = 0\n",
					"                                ,@UPDATES = 0\n",
					"                                ,@DELETES = 0\n",
					"                                ,@ERROR_MESSAGE = NULL\n",
					"                                ,@NEW_INCREMENTAL_KEY_VALUE = NULL \n",
					"                                ,@PREVIOUS_INCREMENTAL_KEY_VALUE = NULL \"\"\"\n",
					"            # Returning the query as a string\n",
					"            return pushdown_query"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def run_tasks(function, q):\n",
					"    while not q.empty():\n",
					"        value = q.get()\n",
					"        function(value)\n",
					"        q.task_done()\n",
					"    q.task_done()\n",
					"    print(\"Tables Migrated\")\n",
					"\n",
					"def load_table(table):\n",
					"    \"\"\"\n",
					"    Function that runs the raw_to_staged_etl_plus_logging() function for each table in the ETL queue. This is done so that\n",
					"    the load can be split across multiple worker nodes\n",
					"    \"\"\"\n",
					"    raw_to_staged_etl_plus_logging(table, PARAM_STORAGE_QUEUE_NAME, PARAM_TRIGGER_TIME, PARAM_TRIGGER_TYPE, PARAM_PIPELINE_NAME, PARAM_PIPELINE_RUN_ID, staged_instructions)\n",
					"    print(table)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_delta_versions(tables_list:list, staged_instructions: dict) -> dict:\n",
					"    \"\"\"Gets a dict of all delta tables and their current version.\"\"\"\n",
					"    delta_versions = {}\n",
					"    \n",
					"    for table in etl_queue:\n",
					"        table_path = staged_instructions[table][\"STAGED_PATH\"]   #f\"{delta_path}{table_name}\"\n",
					"        \n",
					"        # Check if the target file already exists in delta format\n",
					"        is_delta = DeltaTable.isDeltaTable(spark, table_path)\n",
					"\n",
					"        # Exit the current iteration if the file is not in delta format\n",
					"        if not is_delta:\n",
					"            print(f\"table {table} is not a delta table\")\n",
					"            continue\n",
					"\n",
					"        # Read delta table into a dataframe for processing\n",
					"        delta_table = DeltaTable.forPath(spark, table_path)\n",
					"\n",
					"        # Get the current version number        \n",
					"        version_num = delta_table.history().collect()[0][\"version\"]\n",
					"\n",
					"        # Append the version number to the table name list ready to rollback if required        \n",
					"        delta_versions[table] = {\"SCHEMA_VERSION\": version_num, \"DELTA_TABLE_PATH\": staged_instructions[table][\"STAGED_PATH\"]}\n",
					"        \n",
					"    \n",
					"    return delta_versions\n",
					"\n",
					"\t\n",
					"def rollback_deltatables(rollback_versions: dict, successful_ingestions:list) -> None:\n",
					"    \"\"\"Rolls back modified delta tables to the previous version.\n",
					"    \n",
					"    Accepts a dictionary containing the names of any delta files which have\n",
					"     been modified and their version number at the start of the notebook.\n",
					"     Each delta table previously altered is then restored to the specified\n",
					"     version and redunandant metadata vacuumed.\n",
					"\n",
					"    Args:\n",
					"        rollback_versions(dict): Dict containing entity names and versions.\n",
					"    \n",
					"    Returns:\n",
					"        None\n",
					"    \"\"\"\n",
					"    for table in successful_ingestions:\n",
					"\n",
					"        version_num = rollback_versions[table][\"SCHEMA_VERSION\"]\n",
					"        delta_table_path = rollback_versions[table]['DELTA_TABLE_PATH']\n",
					"\n",
					"        delta_df = DeltaTable.forPath(spark, f\"{delta_table_path}\")\n",
					"        delta_df.restoreToVersion(version_num)\n",
					"        \n",
					"        delta_df.vacuum()\n",
					"        \n",
					"    \n",
					"    return\n",
					"\n",
					"\t\n",
					"def cast_timestamps_to_date(entity_df: pyspark.sql.dataframe.DataFrame) -> pyspark.sql.dataframe.DataFrame:\n",
					"    \"\"\"Casts timestamp columns to date preserving null values.\n",
					"    \n",
					"    Accepts a Spark dataframe and casts any columns of data type \"timestamp\"\n",
					"     to \"date\". This preserves null values when writing to delta format.\n",
					"\n",
					"    Args:\n",
					"        entity_df(DataFrame): A Spark dataframe containing timestamp columns.\n",
					"    \n",
					"    Returns:\n",
					"        entity_df(DataFrame): A Spark dataframe with the timestamp data types\n",
					"         cast as date.\n",
					"    \"\"\"\n",
					"    # Get a list of all timestamp columns\n",
					"    date_cols = [x[0] for x in entity_df.dtypes if x[1] == \"timestamp\"]\n",
					"    \n",
					"    # Cast each timestamp column as date\n",
					"    for date_col in date_cols:\n",
					"        entity_df = entity_df.withColumn(date_col, F.col(date_col).cast(\"date\"))    \n",
					"\n",
					"\n",
					"    return entity_df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def rename_etl_action_column(cdc_df:pyspark.sql.dataframe.DataFrame, etl_action_column:str = 'etl_action') ->pyspark.sql.dataframe.DataFrame:\r\n",
					"    \"\"\"\r\n",
					"    Takes in the CDC action column that can only contain 3 literals ('I', 'U', 'D') \r\n",
					"    and it renames it to the standardized name etl_action for the main delta merge ETL function.\r\n",
					"\r\n",
					"    Args:\r\n",
					"        cdc_df (DataFrame): The DataFrame containing CDC data.\r\n",
					"        etl_action_column(str): The column name to be renamed to etl_action\r\n",
					"    Returns:\r\n",
					"        DataFrame: A new DataFrame with only the latest update for each primary key.\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    # Rename the action column to etl_action\r\n",
					"    result_df = cdc_df.withColumnRenamed(f\"{etl_action_column}\", \"etl_action\")\r\n",
					"\r\n",
					"    return result_df\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"def keep_latest_update(cdc_df:pyspark.sql.dataframe.DataFrame, primary_key:str, sequencing_columns:str) ->pyspark.sql.dataframe.DataFrame:\r\n",
					"    \"\"\"\r\n",
					"    Keeps the latest update in the dataframe to be merged, for each primary key based on the watermark column.\r\n",
					"\r\n",
					"    Args:\r\n",
					"        cdc_df (DataFrame): The DataFrame containing CDC data.\r\n",
					"        primary_key (str): The name of the primary key column.\r\n",
					"        sequencing_columns (str): The name of the column with timestamps.\r\n",
					"\r\n",
					"    Returns:\r\n",
					"        DataFrame: A new DataFrame with only the latest update for each primary key.\r\n",
					"    \"\"\"\r\n",
					"    # Split the primary_key parameter into a list of individual primary keys\r\n",
					"    primary_keys = primary_key.split(\",\")\r\n",
					"    sequencing_columns_list = sequencing_columns.split(\",\")\r\n",
					"\r\n",
					"    # Create a window specification partitioned by all primary keys and ordered by the watermark column\r\n",
					"    order_by_columns = [col(column).desc() for column in sequencing_columns_list]\r\n",
					"    \r\n",
					"    # Create a window function specification by providing the partition and ordering columns\r\n",
					"    # Partition by the primary keys provided\r\n",
					"    window_spec = Window.partitionBy(*primary_keys)\r\n",
					"    # Order by the sequencing columns provided\r\n",
					"    window_spec = window_spec.orderBy(*order_by_columns)\r\n",
					"\r\n",
					"    # Add a row number column to the DataFrame using the window function specification\r\n",
					"    df_with_row_number = cdc_df.withColumn(\"row_number\", row_number().over(window_spec))\r\n",
					"\r\n",
					"    # Filter rows where row number is 1 (i.e., the latest update)\r\n",
					"    result_df = df_with_row_number.filter(col(\"row_number\") == 1).drop(\"row_number\")\r\n",
					"\r\n",
					"    return result_df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def raw_to_staged_etl_plus_logging(table:str, schema_evolves:bool , storage_queue:str, trigger_time:str, trigger_type:str, pipeline_name:str, pipeline_run_id:str, staged_instructions:dict, spark_version:float):\n",
					"    \"\"\"\n",
					"    This function extracts data from a RAW source, transforms the data, and writes/merges it to a Delta table in Staged. \n",
					"\n",
					"    Parameters:\n",
					"    - trigger_time (str): The time at which the function was triggered.\n",
					"    - trigger_type (str): The type of trigger that initiated the function (e.g. event-based, scheduled, manual).\n",
					"    - pipeline_run_id (str): A unique ID for the current pipeline run.\n",
					"    - migrated_to_raw (list): A list of table names that have already been migrated from the RAW layer to the Staged layer.\n",
					"    - migrated_to_staged (list): A list of table names that have already been migrated from the Staged layer to the Curated layer.\n",
					"    - staged_existing_files (list): A list of table names that already exist in the Staged layer.\n",
					"    - dimensions_list (list): A list of table names that are dimensions.\n",
					"    - staged_instructions (dict): A dictionary where keys are table names and values are the file paths in the Staged layer.\n",
					"\n",
					"    Returns:\n",
					"    - None\n",
					"    \"\"\"\n",
					"    # IMPORTANT: Set schema evolution to false.\n",
					"    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"false\")\n",
					"\n",
					"    if spark_version <= 3.3:\n",
					"        mounting_prefix = 'synfs:'\n",
					"    elif spark_version >= 3.4:\n",
					"        mounting_prefix = 'synfs:/notebook'\n",
					"    \n",
					"    staged_failures = []\n",
					"    date_now = datetime.utcnow().date()\n",
					"    JOB_ID = mssparkutils.env.getJobId()\n",
					"\n",
					"    # Reading the RAW parquet file into a DataFrame\n",
					"    schema = f\"{staged_instructions[table]['SCHEMA_NAME']}\"\n",
					"    load_type = f\"{staged_instructions[table]['LOAD_TYPE']}\"\n",
					"    serverless_sql_database = f\"{staged_instructions[table]['SERVERLESS_SQL_POOL_DATABASE']}\"\n",
					"    serverless_sql_schema = f\"{staged_instructions[table]['SERVERLESS_SQL_POOL_SCHEMA']}\"\n",
					"    staged_root_path = f\"{mounting_prefix}/{JOB_ID}/mount/{staged_instructions[table]['SERVERLESS_SQL_POOL_DATABASE']}/{staged_instructions[table]['SERVERLESS_SQL_POOL_SCHEMA']}\" #'synfs:/125/mount/synw_data_uks_dna_lakehouse/IHHO001/ab_asbestos_dtl/'},\n",
					"    staged_existing_files = []\n",
					"    start_time = datetime.utcnow()\n",
					"\n",
					"    # Set schema evolution to true only if needed.\n",
					"    if schema_evolves == True:\n",
					"        spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
					"\n",
					"\n",
					"    try:\n",
					"        staged_files_list = mssparkutils.fs.ls(staged_root_path)\n",
					"        for file in staged_files_list:\n",
					"            staged_existing_files.append(file.name)\n",
					"    except Exception as e:\n",
					"        staged_existing_files = []\n",
					"\n",
					"        #logging.info(\"The path doesn't exist - meaning there are no existing staged files corresponding to this datasource.\")\n",
					"\n",
					"\n",
					"    #If the file from the current run hasn't been migrated (written/merged) to staged:\n",
					"        #If the file doesn't already exist in staged: WRITE\n",
					"    if table not in staged_existing_files or load_type == 'FULL_LOAD':\n",
					"\n",
					"        start_time = datetime.utcnow()\n",
					"\n",
					"        # Add new columns etl_timestamp, etl_action that shows when the ETL on a row level was performed and what kind of operation.\n",
					"        # It needs to go after the etl_action column for initial/full loads for Dedicated SQL Pool support.\n",
					"        try:\n",
					"            df_to_merge = spark.read.parquet(f\"{staged_instructions[table]['RAW_PATH']}\").withColumn(\"etl_action\", lit('I')).withColumn('etl_timestamp', lit(trigger_time).cast(TimestampType()))\n",
					"\n",
					"        except:\n",
					"            pushdown_query_fail = create_query_failed_etl_log(trigger_time = trigger_time,\\\n",
					"                                                                trigger_type = trigger_type,\\\n",
					"                                                                pipeline_run_id = pipeline_run_id ,\\\n",
					"                                                                pipeline_name = pipeline_name,\\\n",
					"                                                                notebook_name = 'nb_lakehouse_orchestration_py',\\\n",
					"                                                                source_system_path = f\"{staged_instructions[table]['RAW_PATH']}\",\\\n",
					"                                                                target_system_path = f\"{staged_instructions[table]['STAGED_PATH']}\",\\\n",
					"                                                                object_name = f\"{staged_instructions[table]['OBJECT_NAME']}\",\\\n",
					"                                                                database_name = f\"{staged_instructions[table]['DATABASE_NAME']}\",\\\n",
					"                                                                schema_name = f\"{staged_instructions[table]['SCHEMA_NAME']}\",\\\n",
					"                                                                table = f\"{staged_instructions[table]['TABLE_NAME']}\",\\\n",
					"                                                                file_type = 'PARQUET',\\\n",
					"                                                                operation = 'READ',\\\n",
					"                                                                start_time = start_time,\\\n",
					"                                                                error_msg = err_message)\n",
					"            cursor_etl_sql.execute(pushdown_query_fail)\n",
					"            staged_failures.append(table)\n",
					"            raise err_message\n",
					"\n",
					"        if load_type == 'CDC':\n",
					"            df_to_merge = rename_etl_action_column(cdc_df = df_to_merge, etl_action_column = staged_instructions[table]['ETL_ACTION_COLUMN'])\n",
					"            \n",
					"        try:\n",
					"            # Write full load\n",
					"            df_to_merge.write\\\n",
					"                .mode(\"overwrite\")\\\n",
					"                .format(\"delta\")\\\n",
					"                .option(\"overwriteSchema\", \"true\")\\\n",
					"                .save(f\"{staged_instructions[table]['STAGED_PATH']}\")\n",
					"            \n",
					"\n",
					"            \n",
					"            # Log successful ETL WRITE for FACT\n",
					"            pushdown_query_success= create_query_successful_etl_log(trigger_time = trigger_time,\\\n",
					"                                                trigger_type = trigger_type,\\\n",
					"                                                pipeline_run_id = pipeline_run_id ,\\\n",
					"                                                pipeline_name = pipeline_name,\\\n",
					"                                                notebook_name = 'nb_lakehouse_orchestration_py',\\\n",
					"                                                source_system_path = f\"{staged_instructions[table]['RAW_PATH']}\",\\\n",
					"                                                target_system_path = f\"{staged_instructions[table]['STAGED_PATH']}\",\\\n",
					"                                                object_name = f\"{staged_instructions[table]['OBJECT_NAME']}\",\\\n",
					"                                                database_name = f\"{staged_instructions[table]['DATABASE_NAME']}\",\\\n",
					"                                                schema_name = f\"{staged_instructions[table]['SCHEMA_NAME']}\",\\\n",
					"                                                table = f\"{staged_instructions[table]['TABLE_NAME']}\",\\\n",
					"                                                file_type = 'DELTA',\\\n",
					"                                                operation = 'WRITE',\\\n",
					"                                                start_time = start_time,\\\n",
					"                                                merge_type = 'N/A')\n",
					"\n",
					"            cursor_etl_sql.execute(pushdown_query_success)\n",
					"\n",
					"\n",
					"        # Log Failed ETL WRITE\n",
					"        except Exception as err_message:\n",
					"            pushdown_query_fail = create_query_failed_etl_log(trigger_time = trigger_time,\\\n",
					"                                                            trigger_type = trigger_type,\\\n",
					"                                                            pipeline_run_id = pipeline_run_id ,\\\n",
					"                                                            pipeline_name = pipeline_name,\\\n",
					"                                                            notebook_name = 'nb_lakehouse_orchestration_py',\\\n",
					"                                                            source_system_path = f\"{staged_instructions[table]['RAW_PATH']}\",\\\n",
					"                                                            target_system_path = f\"{staged_instructions[table]['STAGED_PATH']}\",\\\n",
					"                                                            object_name = f\"{staged_instructions[table]['OBJECT_NAME']}\",\\\n",
					"                                                            database_name = f\"{staged_instructions[table]['DATABASE_NAME']}\",\\\n",
					"                                                            schema_name = f\"{staged_instructions[table]['SCHEMA_NAME']}\",\\\n",
					"                                                            table = f\"{staged_instructions[table]['TABLE_NAME']}\",\\\n",
					"                                                            file_type = 'DELTA',\\\n",
					"                                                            operation = 'WRITE', \\\n",
					"                                                            start_time = start_time,\\\n",
					"                                                            error_msg = err_message)\n",
					"            cursor_etl_sql.execute(pushdown_query_fail)\n",
					"            staged_failures.append(table)\n",
					"            raise err_message\n",
					"\n",
					"        \n",
					"\n",
					"\n",
					"    #Else if the file already exist in staged: MERGE\n",
					"    elif table in staged_existing_files:\n",
					"        try:\n",
					"            # Read into dataframe the parquet file containing the data to be Merged.\n",
					"            # Add etl_timestamp column to the data that needs to be merged. \n",
					"            # etl_action doesn't need to be added to deltas, as the operation is determined from the delta merge.\n",
					"\n",
					"            df_to_merge = spark.read.parquet(f\"{staged_instructions[table]['RAW_PATH']}\").withColumn('etl_timestamp', lit(trigger_time).cast(TimestampType()))\n",
					"\n",
					"            if load_type == 'CDC':\n",
					"                df_to_merge = rename_etl_action_column(cdc_df = df_to_merge, etl_action_column = staged_instructions[table]['ETL_ACTION_COLUMN'])\n",
					"\n",
					"        except:\n",
					"            pushdown_query_fail = create_query_failed_etl_log(trigger_time = trigger_time,\\\n",
					"                                                                trigger_type = trigger_type,\\\n",
					"                                                                pipeline_run_id = pipeline_run_id ,\\\n",
					"                                                                pipeline_name = pipeline_name,\\\n",
					"                                                                notebook_name = 'nb_lakehouse_orchestration_py',\\\n",
					"                                                                source_system_path = f\"{staged_instructions[table]['RAW_PATH']}\",\\\n",
					"                                                                target_system_path = f\"{staged_instructions[table]['STAGED_PATH']}\",\\\n",
					"                                                                object_name = f\"{staged_instructions[table]['OBJECT_NAME']}\",\\\n",
					"                                                                database_name = f\"{staged_instructions[table]['DATABASE_NAME']}\",\\\n",
					"                                                                schema_name = f\"{staged_instructions[table]['SCHEMA_NAME']}\",\\\n",
					"                                                                table = f\"{staged_instructions[table]['TABLE_NAME']}\",\\\n",
					"                                                                file_type = 'PARQUET',\\\n",
					"                                                                operation = 'READ',\\\n",
					"                                                                start_time = start_time,\\\n",
					"                                                                error_msg = err_message)\n",
					"            cursor_etl_sql.execute(pushdown_query_fail)\n",
					"            staged_failures.append(table)\n",
					"            raise err_message\n",
					"\n",
					"        try:\n",
					"            # Read the target parquet delta file as a delta table. \n",
					"            delta_table = DeltaTable.forPath(spark, f\"{staged_instructions[table]['STAGED_PATH']}\")\n",
					"        except:\n",
					"            pushdown_query_fail = create_query_failed_etl_log(trigger_time = trigger_time,\\\n",
					"                                                                trigger_type = trigger_type,\\\n",
					"                                                                pipeline_run_id = pipeline_run_id ,\\\n",
					"                                                                pipeline_name = pipeline_name,\\\n",
					"                                                                notebook_name = 'nb_lakehouse_orchestration_py',\\\n",
					"                                                                source_system_path = f\"{staged_instructions[table]['RAW_PATH']}\",\\\n",
					"                                                                target_system_path = f\"{staged_instructions[table]['STAGED_PATH']}\",\\\n",
					"                                                                object_name = f\"{staged_instructions[table]['OBJECT_NAME']}\",\\\n",
					"                                                                database_name = f\"{staged_instructions[table]['DATABASE_NAME']}\",\\\n",
					"                                                                schema_name = f\"{staged_instructions[table]['SCHEMA_NAME']}\",\\\n",
					"                                                                table = f\"{staged_instructions[table]['TABLE_NAME']}\",\\\n",
					"                                                                file_type = 'DELTA',\\\n",
					"                                                                operation = 'READ',\\\n",
					"                                                                start_time = start_time,\\\n",
					"                                                                error_msg = err_message)\n",
					"            cursor_etl_sql.execute(pushdown_query_fail)\n",
					"            staged_failures.append(table)\n",
					"            raise err_message\n",
					"\n",
					"        # Get the Primary Key statements for the specific table\n",
					"        primary_key_statement = staged_instructions[table]['PRIMARY_KEY_STATEMENTS']\n",
					"\n",
					"        # Record start_date for the stored procedure\n",
					"        start_time = datetime.utcnow()\n",
					"\n",
					"        try:\n",
					"            if load_type == 'CHANGE_TRACKING' or load_type == 'CDC':\n",
					"                try:\n",
					"                    df_deletes = df_to_merge.filter(df_to_merge['etl_action'] == 'D')\n",
					"                    df_to_merge = df_to_merge.filter(df_to_merge['etl_action'] != 'D')\n",
					"\n",
					"                    if load_type == 'CDC':\n",
					"                        df_to_merge = keep_latest_update(cdc_df = df_to_merge, primary_key= staged_instructions[table]['PRIMARY_KEYS'], sequencing_columns = staged_instructions[table]['WATERMARK_COLUMN'])\n",
					"\n",
					"\n",
					"                    delta_table.alias(f\"{table}_delta\").merge(\n",
					"                    df_to_merge.alias(f\"{table}_parquet\"),\n",
					"                    f\"{primary_key_statement}\")\\\n",
					"                    .whenNotMatchedInsertAll()\\\n",
					"                    .whenMatchedUpdateAll()\\\n",
					"                    .execute()\n",
					"                except Exception as e:\n",
					"                    raise e\n",
					"                try:\n",
					"                    delta_table.alias(f\"{table}_delta\").merge(\n",
					"                    df_deletes.alias(f\"{table}_parquet\"),\n",
					"                    f\"{primary_key_statement}\")\\\n",
					"                    .whenMatchedDelete()\\\n",
					"                    .execute() \n",
					"                except Exception as e:\n",
					"                    raise e\n",
					"            else:\n",
					"                try:\n",
					"\n",
					"                    column_names = df_to_merge.columns\n",
					"                    \n",
					"                    # Create a dictionary based on the main column_mapping dictionary for the inserts.\n",
					"                    column_mapping_inserts = {f\"{table}_delta.{col}\": f\"{table}_parquet.{col}\" for col in column_names if col != 'etl_action'}\n",
					"                    column_mapping_inserts[\"etl_action\"] = \"'I'\"\n",
					"                    \n",
					"                    # Create a dictionary based on the main column_mapping dictionary for the updates.\n",
					"                    column_mapping_updates = {f\"{table}_delta.{col}\": f\"{table}_parquet.{col}\" for col in column_names if col != 'etl_action'}\n",
					"                    column_mapping_updates[\"etl_action\"] = \"'U'\"\n",
					"\n",
					"                    # Try to do a merge on primary key for dimension\n",
					"                    delta_table.alias(f\"{table}_delta\").merge(\n",
					"                    df_to_merge.alias(f\"{table}_parquet\"),\n",
					"                    f\"{primary_key_statement}\")\\\n",
					"                    .whenMatchedUpdate(set=column_mapping_updates)\\\n",
					"                    .whenNotMatchedInsert(values= column_mapping_inserts)\\\n",
					"                    .execute()\n",
					"\n",
					"                except Exception as e:\n",
					"                    raise e\n",
					"\n",
					"            # Log Successful ETL MERGE for FACT\n",
					"            pushdown_query_success= create_query_successful_etl_log(trigger_time = trigger_time,\\\n",
					"                                                trigger_type = trigger_type,\\\n",
					"                                                pipeline_run_id = pipeline_run_id ,\\\n",
					"                                                pipeline_name = pipeline_name,\\\n",
					"                                                notebook_name = 'nb_lakehouse_orchestration_py',\\\n",
					"                                                source_system_path = f\"{staged_instructions[table]['RAW_PATH']}\",\\\n",
					"                                                target_system_path = f\"{staged_instructions[table]['STAGED_PATH']}\",\\\n",
					"                                                object_name = f\"{staged_instructions[table]['OBJECT_NAME']}\",\\\n",
					"                                                database_name = f\"{staged_instructions[table]['DATABASE_NAME']}\",\\\n",
					"                                                schema_name = f\"{staged_instructions[table]['SCHEMA_NAME']}\",\\\n",
					"                                                table = f\"{staged_instructions[table]['TABLE_NAME']}\",\\\n",
					"                                                file_type = 'DELTA',\\\n",
					"                                                operation = 'MERGE',\\\n",
					"                                                start_time = start_time,\\\n",
					"                                                merge_type= load_type)\n",
					"            cursor_etl_sql.execute(pushdown_query_success)            \n",
					"\n",
					"        # Log failed ETL MERGE for FACT\n",
					"        except Exception as err_message:\n",
					"            pushdown_query_fail = create_query_failed_etl_log(trigger_time = trigger_time,\\\n",
					"                                                            trigger_type = trigger_type,\\\n",
					"                                                            pipeline_run_id = pipeline_run_id,\\\n",
					"                                                            pipeline_name = pipeline_name,\\\n",
					"                                                            notebook_name = 'nb_lakehouse_orchestration_py',\\\n",
					"                                                            source_system_path = f\"{staged_instructions[table]['RAW_PATH']}\",\\\n",
					"                                                            target_system_path = f\"{staged_instructions[table]['STAGED_PATH']}\",\\\n",
					"                                                            object_name = f\"{staged_instructions[table]['OBJECT_NAME']}\",\\\n",
					"                                                            database_name = f\"{staged_instructions[table]['DATABASE_NAME']}\",\\\n",
					"                                                            schema_name = f\"{staged_instructions[table]['SCHEMA_NAME']}\",\\\n",
					"                                                            table = f\"{staged_instructions[table]['TABLE_NAME']}\",\\\n",
					"                                                            file_type = 'DELTA',\\\n",
					"                                                            operation = 'MERGE',\\\n",
					"                                                            start_time = start_time,\\\n",
					"                                                            error_msg = err_message)\n",
					"            cursor_etl_sql.execute(pushdown_query_fail)\n",
					"            staged_failures.append(table)\n",
					"            raise err_message"
				],
				"execution_count": null
			}
		]
	}
}