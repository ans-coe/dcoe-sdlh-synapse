{
	"name": "nb_util_sql_query_py",
	"properties": {
		"folder": {
			"name": "utils"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "d6738db3-b365-4340-8d2e-6a3cb5b8b887"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## SDLH (Strategic Data Lakehouse) Utility\n",
					"<p><b>Description: </b>Enter notebook decription</p>\n",
					"<b>Parent Process: </b>SDLH Pipelines</p>\n",
					"<table align=\"left\">\n",
					" <thead>\n",
					"  <tr>\n",
					"   <th>Contributor</th>\n",
					"   <th>Date</th>\n",
					"   <th>Version</th>\n",
					"   <th>Comment</th>\n",
					"   <th>WorkItem No</th>\n",
					"  </tr>\n",
					" </thead>\n",
					" <tbody>\n",
					"  <tr>\n",
					"   <td>Andrei Dumitru</td>\n",
					"   <td>2022-12-09</td>\n",
					"   <td>1.0</td>\n",
					"   <td>Create initial release</td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Darren Price</td>\n",
					"   <td>2023-10-23</td>\n",
					"   <td>2.0</td>\n",
					"   <td>Updated for SDLH v2.0</td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Darren Price</td>\n",
					"   <td>2024-01-09</td>\n",
					"   <td>2.1</td>\n",
					"   <td>Updated for SDLH v2.1</td>\n",
					"   <td></td>\n",
					"  <tr>\n",
					"   <td>Darren Price</td>\n",
					"   <td>2024-03-08</td>\n",
					"   <td>2.2.0</td>\n",
					"   <td>Updated for SDLH v2.2.0</td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Andrei Dumitru</td>\n",
					"   <td>2024-03-18</td>\n",
					"   <td>2.2.2</td>\n",
					"   <td>Updated for SDLH v2.2.2</td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Darren Price</td>\n",
					"   <td>2024-05-07</td>\n",
					"   <td>2.2.3</td>\n",
					"   <td>Removed lower() from column name <br> Added data type swap for datetimeoffset to datetime2<br> Column INFO schema removed reference to IS_NULLABLE</td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Darren Price</td>\n",
					"   <td>2024-07-24</td>\n",
					"   <td>2.2.14</td>\n",
					"   <td>Updated create lakhouse database to use UTF8 collation</td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Andrei Dumitru</td>\n",
					"   <td>2024-08-16</td>\n",
					"   <td>2.2.15</td>\n",
					"   <td>Updated the create_external_table to commit after each query <br>instead of \"d.autocommit=True\", making it compatible with both<br> Spark 3.3 and 3.4.<br> The notebook is now compatible with Spark 3.4 on Synapse. </td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Andrei Dumitru</td>\n",
					"   <td>2024-09-09</td>\n",
					"   <td>2.2.15</td>\n",
					"   <td>Changed the data lakehouse datatype mapping for timestamp to varbinary(8).</td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Andrei Dumitru</td>\n",
					"   <td>2024-10-01</td>\n",
					"   <td>2.2.15</td>\n",
					"   <td>Modified the datatype mapping for timestamp to varbinary(8) but ONLY for SQL SERVER data sources. <br>Postgres and MySQL data sources map to datetime2. <br>Mapped character lengths >= 1000 for nvarchar data types to be mapped to double the original length and varchar data type. </td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					" </tbody>\n",
					"</table>"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def create_serverless_databases_and_schemas(odbc_url:str, odbc_token:str, datalake_name: str, container_name: str) ->str:\n",
					"    \"\"\"\n",
					"    Uses a batch of metadata from the Metadata DB. It works out the unique Serverless SQL DB names and Schema names that need creating. It then \n",
					"    creates the following external tables pre-requisites if they don't exist already:\n",
					"        - Serverless SQL Database/s\n",
					"        - Serverless SQL Schema/s\n",
					"        - Master Key Encryption\n",
					"        - Database Scoped Credential\n",
					"        - Enriched External Data Source\n",
					"        - Delimited, Parquet, Delta External File Formats\n",
					" \n",
					"    Args:\n",
					"        odbc_url (str): The Serverless SQL Pool ODBC connection details.\n",
					"        odbc_token (str): The Serverless SQL Pool ODBC connection credentials.\n",
					"        datalake_name (str): The name of the data lake to use.\n",
					"        container_name (str): The name of the container to use.\n",
					"\n",
					"    Returns:\n",
					"        str: Returns the External data source name created.\n",
					"    \"\"\"\n",
					"    ## Extract metadata \n",
					"    serverless_databases_to_create = (df_objects_metadata_extracted\n",
					"                                    .select(\"SERVERLESS_SQL_POOL_DATABASE\", \"SERVERLESS_SQL_POOL_SCHEMA\").distinct()\n",
					"                                    .groupBy(\"SERVERLESS_SQL_POOL_DATABASE\")\n",
					"                                    .agg(collect_list(\"SERVERLESS_SQL_POOL_SCHEMA\").alias(\"SERVERLESS_SQL_POOL_SCHEMA_LIST\"))\n",
					"                                    .rdd\n",
					"                                    .map(lambda row: (row[0], row[1]))\n",
					"                                    .collectAsMap())\n",
					"\n",
					"    d = pyodbc.connect(odbc_url, attrs_before = { SQL_COPT_SS_ACCESS_TOKEN:odbc_token })\n",
					"    d.autocommit = True\n",
					"    cursor = d.cursor()\n",
					"\n",
					"    for serverless_sql_pool_database in serverless_databases_to_create.keys():\n",
					"        create_database = f'''\n",
					"            IF NOT EXISTS (SELECT [name] FROM sys.databases WHERE [name] = '{serverless_sql_pool_database}')\n",
					"            BEGIN\n",
					"                EXEC('CREATE DATABASE [{serverless_sql_pool_database}] COLLATE Latin1_General_100_BIN2_UTF8;')\n",
					"            END;\n",
					"        '''\n",
					"\n",
					"        print('Database created: ', serverless_sql_pool_database)\n",
					"        cursor.execute(create_database)\n",
					"        serverless_sql_pool_schemas = serverless_databases_to_create.values()\n",
					"\n",
					"        for serverless_sql_pool_schema in serverless_databases_to_create[serverless_sql_pool_database]:\n",
					"            create_schema = f'''\n",
					"                USE [{serverless_sql_pool_database}];\n",
					"                IF NOT EXISTS (SELECT [name] FROM sys.schemas WHERE [name] = '{serverless_sql_pool_schema}')\n",
					"                BEGIN\n",
					"                    EXEC('CREATE SCHEMA [{serverless_sql_pool_schema}]')\n",
					"                END\n",
					"            '''\n",
					"            #print(create_schema)\n",
					"            print(\"Schema created: \", serverless_sql_pool_schema)\n",
					"            cursor.execute(create_schema)\n",
					"    \n",
					"        create_credential = f'''\n",
					"            USE [{serverless_sql_pool_database}];\n",
					"            IF NOT EXISTS (SELECT symmetric_key_id FROM sys.symmetric_keys WHERE symmetric_key_id = 101)\n",
					"            BEGIN\n",
					"                CREATE MASTER KEY\n",
					"            END;\n",
					"            IF NOT EXISTS (SELECT [name] from sys.database_scoped_credentials WHERE [name] = 'cred_managed_identity')\n",
					"            BEGIN\n",
					"                CREATE DATABASE SCOPED CREDENTIAL [cred_managed_identity]\n",
					"                WITH IDENTITY = 'MANAGED IDENTITY'\n",
					"            END;\n",
					"        '''\n",
					"        cursor.execute(create_credential)\n",
					"\n",
					"        external_data_source_name = f'exds_gen2_{container_name}'\n",
					"        external_data_source_metadata = f'exds_gen2_metadata'\n",
					"        external_data_source_raw = f'exds_gen2_raw'\n",
					"        external_data_source_enriched = f'exds_gen2_enriched'\n",
					"        external_data_source_curated = f'exds_gen2_curated'\n",
					"\n",
					"        create_exds = f'''\n",
					"            IF NOT EXISTS (SELECT [name] FROM sys.external_data_sources WHERE [name] = '{external_data_source_name}')\n",
					"            BEGIN\n",
					"                CREATE EXTERNAL DATA SOURCE [{external_data_source_name}]\n",
					"                WITH (\n",
					"                    LOCATION = N'https://{datalake_name}.dfs.core.windows.net/{container_name}',\n",
					"                    CREDENTIAL = [cred_managed_identity]\n",
					"                );\n",
					"            END;\n",
					"\n",
					"            IF NOT EXISTS (SELECT [name] FROM sys.external_data_sources WHERE [name] = '{external_data_source_metadata}')\n",
					"            BEGIN\n",
					"                CREATE EXTERNAL DATA SOURCE [{external_data_source_metadata}]\n",
					"                WITH (\n",
					"                    LOCATION = N'https://{datalake_name}.dfs.core.windows.net/metadata',\n",
					"                    CREDENTIAL = [cred_managed_identity]\n",
					"                );\n",
					"            END;\n",
					"\n",
					"            IF NOT EXISTS (SELECT [name] FROM sys.external_data_sources WHERE [name] = '{external_data_source_raw}')\n",
					"            BEGIN\n",
					"                CREATE EXTERNAL DATA SOURCE [{external_data_source_raw}]\n",
					"                WITH (\n",
					"                    LOCATION = N'https://{datalake_name}.dfs.core.windows.net/raw',\n",
					"                    CREDENTIAL = [cred_managed_identity]\n",
					"                );\n",
					"            END;\n",
					"\n",
					"            IF NOT EXISTS (SELECT [name] FROM sys.external_data_sources WHERE [name] = '{external_data_source_enriched}')\n",
					"            BEGIN\n",
					"                CREATE EXTERNAL DATA SOURCE [{external_data_source_enriched}]\n",
					"                WITH (\n",
					"                    LOCATION = N'https://{datalake_name}.dfs.core.windows.net/enriched',\n",
					"                    CREDENTIAL = [cred_managed_identity]\n",
					"                );\n",
					"            END;\n",
					"\n",
					"            IF NOT EXISTS (SELECT [name] FROM sys.external_data_sources WHERE [name] = '{external_data_source_curated}')\n",
					"            BEGIN\n",
					"                CREATE EXTERNAL DATA SOURCE [{external_data_source_curated}]\n",
					"                WITH (\n",
					"                    LOCATION = N'https://{datalake_name}.dfs.core.windows.net/curated',\n",
					"                    CREDENTIAL = [cred_managed_identity]\n",
					"                );\n",
					"            END;\n",
					"\n",
					"            IF NOT EXISTS (SELECT [name] FROM sys.external_file_formats WHERE [name] = 'exff_csv')\n",
					"            BEGIN\n",
					"                CREATE EXTERNAL FILE FORMAT exff_csv\n",
					"                WITH (\n",
					"                    FORMAT_TYPE = DELIMITEDTEXT,\n",
					"                    FORMAT_OPTIONS (FIELD_TERMINATOR = ','),\n",
					"                    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.GzipCodec'\n",
					"                );\n",
					"            END;\n",
					"            IF NOT EXISTS (SELECT [name] FROM sys.external_file_formats WHERE [name] = 'exff_parquet')\n",
					"            BEGIN\n",
					"                CREATE EXTERNAL FILE FORMAT exff_parquet\n",
					"                WITH (\n",
					"                    FORMAT_TYPE = PARQUET,\n",
					"                    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n",
					"                );\n",
					"            END;\n",
					"            IF NOT EXISTS (SELECT [name] FROM sys.external_file_formats WHERE [name] = 'exff_delta')\n",
					"            BEGIN\n",
					"                CREATE EXTERNAL FILE FORMAT exff_delta\n",
					"                WITH (\n",
					"                    FORMAT_TYPE = DELTA\n",
					"                );\n",
					"            END;\n",
					"        '''\n",
					"        cursor.execute(create_exds)\n",
					"        d.close()\n",
					"    return"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def map_data_types(df: pyspark.sql.dataframe.DataFrame) -> pyspark.sql.dataframe.DataFrame:\n",
					"    \"\"\"\n",
					"    Takes in the information schema dataframe extracted from Metadata DB. It maps data types from\n",
					"    source systems to datatypes supported in Serverless SQL Pool and makes formatting changes \n",
					"    to column data types and maximum length in a PySpark DataFrame. \n",
					"\n",
					"    The function performs the following transformations:\n",
					"        - Maps data types based on a dictionary of mappings\n",
					"        - Replaces spaces in column names with underscores\n",
					"        - Changes the maximum length of columns with a length of \"-1\" to \"4000\"\n",
					"        - Replaces \"varchar(4000)\" data type with \"varchar\"\n",
					"        - Adds brackets to column names to make them SQL-safe\n",
					"\n",
					"    Returns an information_schema dataframe suitable for Serverless SQL Pool external tables creation. \n",
					"\n",
					"    Parameters:\n",
					"    - df: PySpark DataFrame - the input DataFrame to be transformed\n",
					"\n",
					"    Returns:\n",
					"    - PySpark DataFrame - the transformed DataFrame\n",
					"\n",
					"    Example Usage:\n",
					"    ```\n",
					"    from pyspark.sql import SparkSession\n",
					"    spark = SparkSession.builder.appName(\"MapDataTypes\").getOrCreate()\n",
					"\n",
					"    transformed_df = map_data_types(df) \n",
					"    ```\n",
					"    \"\"\"\n",
					"    # Dictionary to map source data types to target data types\n",
					"    data_type_mappings = {\n",
					"        \"nvarchar\": \"varchar\",\n",
					"        \"nchar\": \"char\",\n",
					"        \"ntext\": \"varchar\",\n",
					"        \"text\": \"varchar\",\n",
					"        \"character varying\": \"varchar\",\n",
					"        \"geography\": \"varbinary\",\n",
					"        \"geometry\": \"varbinary\",\n",
					"        \"hierarchyid\": \"nvarchar(4000)\",\n",
					"        \"image\": \"varbinary(4000)\",\n",
					"        \"timestamp\": \"datetime2\",\n",
					"        \"rowversion\": \"varbinary(8)\",\n",
					"        \"xml\": \"varchar\",\n",
					"        \"double\": \"varchar(8000)\",\n",
					"        \"longtext\": \"varchar(8000)\",\n",
					"        \"integer\": \"int\",\n",
					"        \"boolean\": \"bit\",\n",
					"        \"timestamp without time zone\": \"datetime2\",\n",
					"        \"timestamp with time zone\": \"datetime2\",\n",
					"        \"datetimeoffset\": \"datetime2\",\n",
					"        \"ARRAY\": \"varchar(max)\",\n",
					"        \"numeric\": \"varchar(8000)\",\n",
					"        \"json\": \"varchar(max)\",\n",
					"        \"mediumtext\": \"varchar\",\n",
					"        \"tinytext\": \"varchar\"\n",
					"    }\n",
					"\n",
					"    # UDF to map data types using the dictionary\n",
					"    map_data_type_udf = udf(lambda data_type: data_type_mappings.get(data_type, data_type), StringType())\n",
					"\n",
					"    # Change 'timestamp' data type to 'varbinary(8)' for SQL Server source\n",
					"    df = df.withColumn(\"DATA_TYPE\", when((col(\"SOURCE_TYPE\") == \"SQL_SERVER\") & (col(\"DATA_TYPE\") == \"timestamp\"), \"varbinary(8)\").otherwise(df[\"DATA_TYPE\"]))\n",
					"\n",
					"    # Double the length of 'nvarchar' columns if length is >= 1000\n",
					"    df = df.withColumn(\"CHARACTER_MAXIMUM_LENGTH\", when((col(\"CHARACTER_MAXIMUM_LENGTH\").cast(\"bigint\") >= 1000) & (col(\"DATA_TYPE\") == \"nvarchar\"), (col(\"CHARACTER_MAXIMUM_LENGTH\") * 2).cast(\"string\")).otherwise(df[\"CHARACTER_MAXIMUM_LENGTH\"]))\n",
					"\n",
					"    # Apply data type mappings\n",
					"    df = df.withColumn(\"DATA_TYPE\", map_data_type_udf(col(\"DATA_TYPE\")))\n",
					"\n",
					"    # Change length of columns with length '-1' to 'MAX'\n",
					"    df = df.withColumn(\"CHARACTER_MAXIMUM_LENGTH\", when(df[\"CHARACTER_MAXIMUM_LENGTH\"] == \"-1\", \"MAX\").otherwise(df[\"CHARACTER_MAXIMUM_LENGTH\"]))\n",
					"\n",
					"    # Replace 'null' length with '8000'\n",
					"    df = df.withColumn(\"CHARACTER_MAXIMUM_LENGTH\", when(df[\"CHARACTER_MAXIMUM_LENGTH\"] == \"null\", \"8000\").otherwise(df[\"CHARACTER_MAXIMUM_LENGTH\"]))\n",
					"\n",
					"    # Remove special characters from length values\n",
					"    df = df.withColumn(\"CHARACTER_MAXIMUM_LENGTH\", F.regexp_replace(col(\"CHARACTER_MAXIMUM_LENGTH\"), \"[\\$#,]\", \"\"))\n",
					"\n",
					"    # Change length of columns with length '-4000' to '8000'\n",
					"    df = df.withColumn(\"CHARACTER_MAXIMUM_LENGTH\", when(df[\"CHARACTER_MAXIMUM_LENGTH\"] == \"-4000\", \"8000\").otherwise(df[\"CHARACTER_MAXIMUM_LENGTH\"]))\n",
					"\n",
					"    # Change length of columns with length >= 4000 to '8000'\n",
					"    df = df.withColumn(\"CHARACTER_MAXIMUM_LENGTH\", when(col(\"CHARACTER_MAXIMUM_LENGTH\").cast(\"bigint\") >= 4000, \"8000\").otherwise(df[\"CHARACTER_MAXIMUM_LENGTH\"]))\n",
					"\n",
					"    # Replace 'varchar(4000)' data type with 'varchar'\n",
					"    df = df.withColumn(\"DATA_TYPE\", when(df[\"DATA_TYPE\"] == \"varchar(4000)\", \"varchar\").otherwise(df[\"DATA_TYPE\"]))\n",
					"\n",
					"    # Set 'varchar' data type to 'varchar(8000)' if length is null\n",
					"    df = df.withColumn(\"DATA_TYPE\", when((col(\"DATA_TYPE\") == \"varchar\") & (col(\"CHARACTER_MAXIMUM_LENGTH\").isNull()), \"varchar(8000)\").otherwise(df[\"DATA_TYPE\"]))\n",
					"\n",
					"    # Trim leading and trailing spaces from column names\n",
					"    df = df.withColumn(\"COLUMN_NAME\", ltrim(col(\"COLUMN_NAME\")))\n",
					"    df = df.withColumn(\"COLUMN_NAME\", rtrim(col(\"COLUMN_NAME\")))\n",
					"\n",
					"    # Replace spaces in column names with underscores\n",
					"    df = df.withColumn(\"COLUMN_NAME\", regexp_replace(col(\"COLUMN_NAME\"), \" \", \"_\"))\n",
					"\n",
					"    return df"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# not logged\n",
					"def ext_table_mapping(tables_list:list, information_schemas:pyspark.sql.dataframe.DataFrame, staged_instructions:dict, datalake_staged_container:str) -> dict:\n",
					"    '''\n",
					"    This function uses the information_schema to create a dictionary with all the information required for \n",
					"    Serverless SQL Pool external tables creation.\n",
					"\n",
					"    Args:\n",
					"    tables_list (list): List of tables\n",
					"    information_schemas (DataFrame): DataFrame containing schema information\n",
					"    staged_instructions (dict): Dictionary containing staged instructions\n",
					"    datalake_staged_container (str): Name of the datalake staged container\n",
					"\n",
					"    Returns:\n",
					"    external_tables_instructions (dict): Dictionary with the following structure:\n",
					"        {'table_name': {'TABLE_QUERY': 'columns_query', 'FILE_PATH': 'staged_file_path',\n",
					"                        'SERVERLESS_SQL_POOL_DATABASE': 'database_name', 'SERVERLESS_SQL_SCHEMA': 'schema',\n",
					"                        'EXTERNAL_DATA_SOURCE': 'exds_gen2_datalake_staged_container'}}\n",
					"    '''\n",
					"\n",
					"    # Define empty dictionary to populate\n",
					"    external_tables_instructions ={}\n",
					"\n",
					"    # Loop through the list of tables\n",
					"    for table in tables_list:\n",
					"        \n",
					"        # Use information_schemas dataframe (which contains column-level info) to start composing the query, specifically the data type part.\n",
					"        if information_schemas.select(\"CHARACTER_MAXIMUM_LENGTH\") == \"undefined\":\n",
					"            query = information_schemas.select(concat_ws(\" \", information_schemas.COLUMN_NAME, information_schemas.DATA_TYPE).alias(\"QUERY\"), information_schemas.TABLE_NAME.alias(\"TABLE_NAME\"), information_schemas.COLUMN_NAME.alias(\"COLUMN_NAME\"))\n",
					"        elif information_schemas.select(\"CHARACTER_MAXIMUM_LENGTH\") != \"undefined\":\n",
					"            query = information_schemas.select(concat_ws(\" \", information_schemas.COLUMN_NAME, information_schemas.DATA_TYPE, information_schemas.CHARACTER_MAXIMUM_LENGTH).alias(\"QUERY\"), information_schemas.TABLE_NAME.alias(\"TABLE_NAME\"),information_schemas.COLUMN_NAME.alias(\"COLUMN_NAME\"))\n",
					"        \n",
					"        # Aggregate, comma-separate, and concatenate all column info per table\n",
					"        query = query.groupby(\"TABLE_NAME\").agg(collect_list(\"QUERY\").alias(\"QUERY\"))\n",
					"        query_string = query.withColumn(\"QUERY\", concat_ws(\", \", \"QUERY\"))\n",
					"\n",
					"        # Extract info about each external table\n",
					"        database = staged_instructions[table][\"SERVERLESS_SQL_POOL_DATABASE\"]\n",
					"        schema = staged_instructions[table][\"SERVERLESS_SQL_POOL_SCHEMA\"]\n",
					"        \n",
					"        file_path = f\"{staged_instructions[table]['SERVERLESS_SQL_POOL_DATABASE']}/{staged_instructions[table]['SERVERLESS_SQL_POOL_SCHEMA']}/{staged_instructions[table]['TABLE_NAME']}\"\n",
					"        external_tables_instructions.update({f'{staged_instructions[table][\"TABLE_NAME\"]}': {\"TABLE_QUERY\" : f'placeholder' , \"FILE_PATH\":f'{file_path}' , \"SERVERLESS_SQL_POOL_DATABASE\":staged_instructions[table][\"SERVERLESS_SQL_POOL_DATABASE\"], \"SERVERLESS_SQL_SCHEMA\":staged_instructions[table][\"SERVERLESS_SQL_POOL_SCHEMA\"], \"EXTERNAL_DATA_SOURCE\":f'exds_gen2_{datalake_staged_container}'}})\n",
					"\n",
					"    for row in query_string.toLocalIterator():\n",
					"        external_tables_instructions[row.TABLE_NAME][\"TABLE_QUERY\"] = row.QUERY\n",
					"\n",
					"    return external_tables_instructions"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# not logged\n",
					"def produce_external_table_definition(database_name:str,schema_name: str, table_name: str, file_system: str, data_source: str, location: str, columns_meta: str) -> str:\n",
					"    \"\"\"\n",
					"\n",
					"    The purpose of this function is to create an external table definition string, using various table parameters.\n",
					"\n",
					"    Parameters:\n",
					"     1. schema_name (str): Takes in the constant schema name (PARAM_SCHEMA_NAME)\n",
					"     2. table_name (str): Takes in the variable table names derived from the tables JSON metadata \n",
					"     3. file_system (str): Takes in the constant ADLS file system where the data is situated (PARAM_FILESYSTEM)\n",
					"     4. data_source (str): Takes in the external_data_source_name variable, which is the result of configure_database_lake_access function\n",
					"     5. hierarchy (str): Takes in the file hierarchy in ADLS, derived from the tables JSON metadata\n",
					"     6. columns_meta (str): Takes in the tables columns metadata, derived from the JSON metadata\n",
					"\n",
					"\n",
					"\n",
					"    Output: A string matching exactly a CREATE EXTERNAL TABLE Serverless SQL query. It contains all the D365 tables information required to create the views, including a schema.\n",
					"\n",
					"    \"\"\"\n",
					"\n",
					"    \n",
					"    return f\"\"\"\n",
					"        USE [{database_name}];\n",
					"        \n",
					"        IF NOT EXISTS (select s.name as [SCHEMA_NAME],t.name AS [TABLE_NAME] from sys.tables t, sys.schemas s where t.schema_id = s.schema_id and s.name = '{schema_name}' and t.name = '{table_name}')\n",
					"            CREATE EXTERNAL TABLE [{schema_name}].[{table_name}](\n",
					"                {columns_meta},\n",
					"                etl_action varchar(5),\n",
					"                etl_timestamp datetime\n",
					"                )  \n",
					"                WITH (\n",
					"                LOCATION = '{location}',\n",
					"                DATA_SOURCE = [{data_source}],  \n",
					"                FILE_FORMAT = [exff_delta]\n",
					"                )\n",
					"    \"\"\"\n",
					" "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# This function creates the external tables via JDBC driver, SQL injection.\n",
					"# The query that it injects is the output of the produceExternalTableDefinition function\n",
					"# primary\n",
					"def create_external_table(odbc_url:str, odbc_token:str, database_name:str, table_definition:str):\n",
					"    \"\"\"\n",
					"    The purpose of this function is to execute the query created before, against the Serverless SQL DB.\n",
					"\n",
					"    Parameters:\n",
					"     1. odbc_url (str): Takes in the url required for the odbc connection\n",
					"     2. odbc_token (str): Takes in the authentication token required for the odbc connection\n",
					"     4. database_name (str): Takes in the constant Serverless SQL DB (DATABASE_NAME_TO_CREATE)\n",
					"     5. table_definition (str): Takes in the result of the produceExternalTableDefinition function.\n",
					"\n",
					"    Output: Connects to the Serverless SQL DB through JDBC and injects the Queries produced using produceExternalTableDefinition function.\n",
					"\n",
					"    \"\"\"\n",
					"\n",
					"    d = pyodbc.connect(odbc_url, attrs_before = { SQL_COPT_SS_ACCESS_TOKEN:odbc_token })\n",
					"    cursor = d.cursor()\n",
					"    Query = table_definition\n",
					"    cursor.execute(Query)\n",
					"    d.commit() \n",
					"    d.close()"
				],
				"execution_count": null
			}
		]
	}
}