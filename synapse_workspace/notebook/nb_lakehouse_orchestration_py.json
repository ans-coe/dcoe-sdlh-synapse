{
	"name": "nb_lakehouse_orchestration_py",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "5407bb1e-a15e-4e50-b9a1-e94f301a20c0"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## SDLH (Strategic Data Lakehouse) Orchestration\n",
					"<p><b>Description: </b>Enter notebook decription</p>\n",
					"<b>Parent Process: </b>SDLH Pipelines</p>\n",
					"<table align=\"left\">\n",
					" <thead>\n",
					"  <tr>\n",
					"   <th>Contributor</th>\n",
					"   <th>Date</th>\n",
					"   <th>Version</th>\n",
					"   <th>Comment</th>\n",
					"   <th>WorkItem No</th>\n",
					"  </tr>\n",
					" </thead>\n",
					" <tbody>\n",
					"  <tr>\n",
					"   <td>Andrei Dumitru</td>\n",
					"   <td>2022-12-09</td>\n",
					"   <td>1.0</td>\n",
					"   <td>Create initial release</td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Darren Price</td>\n",
					"   <td>2023-10-23</td>\n",
					"   <td>2.0</td>\n",
					"   <td>Updated for SDLH v2.0</td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Darren Price</td>\n",
					"   <td>2024-01-09</td>\n",
					"   <td>2.1</td>\n",
					"   <td>Updated for SDLH v2.1</td>\n",
					"   <td></td>\n",
					"  <tr>\n",
					"   <td>Darren Price</td>\n",
					"   <td>2024-03-08</td>\n",
					"   <td>2.2.0</td>\n",
					"   <td>Updated for SDLH v2.2.0</td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Andrei Dumitru</td>\n",
					"   <td>2024-03-18</td>\n",
					"   <td>2.2.2</td>\n",
					"   <td>Updated for SDLH v2.2.2</td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Darren Price</td>\n",
					"   <td>2024-05-07</td>\n",
					"   <td>2.2.3</td>\n",
					"   <td>Removed lower() from column name <br> Added data type swap for datetimeoffset to datetime2<br> Column INFO schema removed reference to IS_NULLABLE</td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Andrei Dumitru</td>\n",
					"   <td>2024-08-16</td>\n",
					"   <td>2.2.4</td>\n",
					"   <td>Updated notebook to be compatible with both Spark 3.3 and 3.4 <br>dynamically, based on the cluster's Pyspark version. </td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"  <tr>\n",
					"   <td>Andrei Dumitru</td>\n",
					"   <td>2024-10-01</td>\n",
					"   <td>2.2.15</td>\n",
					"   <td>Updated notebook to be compatible with both Spark 3.3 and 3.4 <br>dynamically, when calling the raw to enriched function. </td>\n",
					"   <td></td>\n",
					"  </tr>\n",
					"\n",
					" </tbody>\n",
					"</table>"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Configuration Sections"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Install Python Packages"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"pip install azure-storage-queue azure-identity"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Import Python Libraries"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import re\n",
					"import pandas as pd\n",
					"import numpy as np\n",
					"import uuid\n",
					"from itertools import chain\n",
					"import pyspark\n",
					"import logging\n",
					"import json\n",
					"import copy\n",
					"import pyspark.sql.types as T\n",
					"import pyodbc\n",
					"import struct\n",
					"from notebookutils import mssparkutils \n",
					"\n",
					"from collections.abc import Iterable\n",
					"\n",
					"from azure.identity import DefaultAzureCredential\n",
					"\n",
					"from azure.core.credentials import AzureNamedKeyCredential\n",
					"from azure.core.exceptions import ResourceExistsError, ResourceNotFoundError\n",
					"\n",
					"from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import substring_index, concat_ws, col\n",
					"\n",
					"import fnmatch\n",
					"from delta import *\n",
					"from datetime import datetime\n",
					"import pyspark.sql.functions as F\n",
					"import msal\n",
					"from pyspark.sql.functions import col, lit, trim, ltrim, rtrim, regexp_replace, concat, udf, when, expr, from_json, explode, arrays_zip, current_date, year,month, dayofmonth, hour, days, lower\n",
					"from pyspark.sql.types import *\n",
					"from pyspark.context import SparkContext\n",
					"\n",
					"from azure.storage.queue import (\n",
					"        QueueClient,\n",
					"        BinaryBase64EncodePolicy,\n",
					"        BinaryBase64DecodePolicy\n",
					")\n",
					"from azure.identity import DefaultAzureCredential\n",
					"import os, uuid\n",
					"from threading import Thread\n",
					"from queue import Queue\n",
					"\n",
					"import re\n",
					"from pyspark.sql.functions import col, lit, regexp_replace, concat, collect_list\n",
					"from pyspark.sql import SparkSession"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Set Spark Configuration\n",
					"- Fixes issue where pyspark and timestamps less than 1900-01-01T00:00:00 in parquet files aren't compatible with spark 3+ and delta tables."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.conf.set(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")\n",
					"spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
					"spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n",
					"spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Import Utility Notebooks\n",
					"- nb_util_sql_staged_etl_and_logging_py contains functions for metadata (extraction, preparation), connectivity, ETL and logging.\n",
					"- nb_util_sql_query_py contains functions for source-target information schema mapping and programatical interaction with lakehouse databases."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/nb_util_sql_staged_etl_and_logging_py"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/nb_util_sql_query_py"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Toggle Parameters Cell\n",
					"- Please note that the below cell is marked with \"Parameters\". (Can be seen in the botton right hand corner of the cell)\n",
					"- When the notebook is ran by a pipeline, dynamic parameter values are passed from the pipeline.\n",
					"- The dynamic parameter values will appear as a cell below the \"Parameters\" cell, overriding any manually added ones."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"# Pipeline Parameters\n",
					"PARAM_SOURCE_TYPE = \"\"\n",
					"PARAM_SOURCE_SYSTEM = \"\"\n",
					"PARAM_SOURCE_GROUPING_ID = \"\"\n",
					"PARAM_STORAGE_QUEUE_NAME = \"\"\n",
					"PARAM_PIPELINE_NAME = \"\"\n",
					"PARAM_PIPELINE_RUN_ID = \"\"\n",
					"PARAM_TRIGGERING_PIPELINE_NAME = \"\"\n",
					"PARAM_TRIGGERING_PIPELINE_RUN_ID = \"\"\n",
					"PARAM_TRIGGER_TYPE = \"\"\n",
					"PARAM_TRIGGER_TIME = \"\""
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Keyvault Variables\n",
					"Reading environment-specific variables from AKV:\n",
					"- Metadata SQL Server fully qualified domain name\n",
					"- Storage account name (ADLS)\n",
					"- AKV connection string\n",
					"- Synapse Serverless SQL fully qualified domain name endpoint\n",
					"- Synapse workspace name\n",
					"- Environment name"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"azuresql_server_fqdn = mssparkutils.credentials.getSecretWithLS(\"keyvault_data_azir\", \"azure-sql-server-fqdn\")\n",
					"azuresql_db = mssparkutils.credentials.getSecretWithLS(\"keyvault_data_azir\", \"azure-sql-database-name-001\")\n",
					"\n",
					"datalake_storage_name = mssparkutils.credentials.getSecretWithLS(\"keyvault_data_azir\", \"datalake-account-name\")\n",
					"datalake_access_key = mssparkutils.credentials.getSecretWithLS(\"keyvault_data_azir\", \"datalake-access-key\")\n",
					"datalake_connection_string = f\"DefaultEndpointsProtocol=https;AccountName={datalake_storage_name};AccountKey={datalake_access_key};EndpointSuffix=core.windows.net\"\n",
					"\n",
					"synapse_serverless_server_fqdn = mssparkutils.credentials.getSecretWithLS(\"keyvault_data_azir\", \"synapse-sql-serverless-server-fqdn\")\n",
					"\n",
					"synapse_workspace = mssparkutils.env.getWorkspaceName()\n",
					"\n",
					"print (synapse_workspace)"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Global Variables\n",
					"- Set a number of reuseable variables"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark_version_full = spark.version\n",
					"spark_version_short = float(str(pyspark.util.VersionUtils.majorMinorVersion(spark_version_full)[0]) + '.' + str(pyspark.util.VersionUtils.majorMinorVersion(spark_version_full)[1]))\n",
					"\n",
					"print(\"Spark version: \" + str(spark_version_short))\n",
					"\n",
					"sql_port = \"1433\"\n",
					"jdbc_options = \"encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30\"\n",
					"odbc_options = \"Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30\"\n",
					"\n",
					"azuresql_server_jdbc_url = f\"jdbc:sqlserver://{azuresql_server_fqdn}:{sql_port};database={azuresql_db};{jdbc_options}\"\n",
					"azuresql_server_odbc_url = f\"DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={azuresql_server_fqdn},{sql_port};DATABASE={azuresql_db};{odbc_options}\"\n",
					"\n",
					"synapse_serverless_odbc_url = f\"DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={synapse_serverless_server_fqdn},{sql_port};DATABASE=master;{odbc_options}\"\n",
					"\n",
					"datalake_staged_container = \"enriched\""
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Connectivity Sections"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Generate SQL Access Tokens\n",
					"- Get SQL based access token using synapse managed identity.\n",
					"- Once we get the token we complete config to make it useable by pyodbc."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Get access token for Synapse Managed identity Auth\n",
					"sql_access_token = mssparkutils.credentials.getConnectionStringOrCreds(\"azure_sql_data_no_param_azir\")\n",
					"\n",
					"# Reconfigure access token for usage with pyodbc.connect\n",
					"SQL_COPT_SS_ACCESS_TOKEN = 1256\n",
					"\n",
					"exp_token = b\"\"\n",
					"for i in bytes(sql_access_token, \"UTF-8\"):\n",
					"    exp_token += bytes({i})\n",
					"    exp_token += bytes(1)\n",
					"\n",
					"sql_access_token_odbc = struct.pack(\"=i\", len(exp_token)) + exp_token"
				],
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Metadata DB ODBC connection\n",
					"- Using the pyodbc library, instantiate a connection between the Spark cluster and the Metadata DB."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Preparing the ODBC connection between Apache Spark and the Metadata DB for logging the ETL process\n",
					"try:\n",
					"    etl_sql_driver = pyodbc.connect(azuresql_server_odbc_url, attrs_before = { SQL_COPT_SS_ACCESS_TOKEN:sql_access_token_odbc })\n",
					"    etl_sql_driver.autocommit = True\n",
					"    cursor_etl_sql = etl_sql_driver.cursor()\n",
					"except Exception as e:\n",
					"    logging.error(\"*** ERROR - Metadata Database connection has failed. ***\")\n",
					"    raise e"
				],
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### ETL Queue API connection"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Prepare the ETL Queue\n",
					"queue_client = QueueClient.from_connection_string(conn_str = datalake_connection_string, queue_name = PARAM_STORAGE_QUEUE_NAME)\n",
					"messages = queue_client.receive_messages(visibility_timeout = 3600)\n",
					"migrated_to_raw = messages.by_page()\n",
					"\n",
					"etl_queue = []\n",
					"for batch in migrated_to_raw:\n",
					"    for table_msg in batch:\n",
					"        table = table_msg.content\n",
					"        etl_queue.append(table)\n",
					"\n",
					"# Use set to remove any duplicates in the qeeue (added for a potential re-run scenario)\n",
					"# Then sort list alphabetically for consistent ordering when we next run\n",
					"etl_queue = sorted(set(etl_queue))\n",
					"print(etl_queue)"
				],
				"execution_count": 12
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Metadata Sections"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Extract metadata from the Metadata DB"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df_objects_metadata_extracted = deserialize_json_meta_to_notebook_etl_relational_meta(azuresql_server_odbc_url, sql_access_token_odbc)[0]\n",
					"df_information_schema = deserialize_json_meta_to_notebook_etl_relational_meta(azuresql_server_odbc_url, sql_access_token_odbc)[1]\n",
					"\n",
					"#display(df_objects_metadata_extracted)"
				],
				"execution_count": 13
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Prepare ETL metadata instructions (table level)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"staged_instructions = get_raw_staged_etl_instructions(datalake_staged_container, datalake_storage_name, spark_version_short)"
				],
				"execution_count": 16
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Prepare Lakehouse metadata instructions (column level)\n",
					"- Source-target datatype mapping.\n",
					"- Generate dictionary containing Lakehouse metadata instructions only for tables in the current batch."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"information_schemas = map_data_types(df_information_schema)\n",
					"\n",
					"tables_list = information_schemas.select('TABLE_NAME').distinct().rdd.flatMap(lambda x:x).collect()#etl_queue\n",
					"external_tables_instructions = ext_table_mapping(tables_list, information_schemas, staged_instructions, datalake_staged_container)"
				],
				"execution_count": 17
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Raw To Enriched ETL Sections"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"try:\n",
					"    CURRENT_VERSIONS = get_delta_versions(tables_list=etl_queue, staged_instructions=staged_instructions)\n",
					"    tables_ingested_successfully = []\n",
					"    print(\"Starting tables ingestion...\")\n",
					"    for table in etl_queue:\n",
					"        raw_to_staged_etl_plus_logging(table, PARAM_STORAGE_QUEUE_NAME, PARAM_TRIGGER_TIME, PARAM_TRIGGER_TYPE, PARAM_PIPELINE_NAME, PARAM_PIPELINE_RUN_ID, staged_instructions, spark_version_short)\n",
					"        tables_ingested_successfully.append(table)\n",
					"        print(f\"\"\"Completed Table: {table}\"\"\")\n",
					"    print(\"All of the tables in the batch were ingested successfully.\")\n",
					"\n",
					"except Exception as exception:\n",
					"    try:\n",
					"        print(\"=====================================================================\")\n",
					"        print(\"A table has failed. Restoring the current batch tables.\")\n",
					"        print(f\"Delta tables to roll back: {', '.join(map(str, tables_ingested_successfully))}\") \n",
					"        # Get current version for delta tables that have been processed prior to error\n",
					"        #  and revert the delta tables to their state at notebook runtime\n",
					"        rollback_requirements = CURRENT_VERSIONS\n",
					"        if rollback_requirements:\n",
					"            rollback_deltatables(rollback_versions=rollback_requirements, successful_ingestions=tables_ingested_successfully)\n",
					"            print(\"Delta tables restored.\")\n",
					"    except NameError as no_tables_processed:\n",
					"        print(\"No objects needed delta version rollback.\")\n",
					"        \n",
					"    # Reinstate all the queue objects as they were in case of failure to aid a re-run\n",
					"    print(\"=====================================================================\")\n",
					"    print(\"Reinstating original queue objects...\")\n",
					"    for table in etl_queue:\n",
					"        queue_client.send_message(table)\n",
					"    print(f\"Queue name: {PARAM_STORAGE_QUEUE_NAME}\")\n",
					"    print(f\"Elements reinstated: {','.join(etl_queue)}\")\n",
					"\n",
					"    raise RuntimeError(exception)"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Lakehouse Creation/Expansion Sections"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Serverless SQL DB pre-requisites\n",
					"\n",
					"The following pre-requisites are created if they don't already exist:\n",
					"- Lakehouse database\n",
					"- Lakehouse schema\n",
					"- Enriched external data source\n",
					"- External file formats\n",
					"- Master Key Encryption"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"create_serverless_databases_and_schemas(synapse_serverless_odbc_url, sql_access_token_odbc, datalake_storage_name, datalake_staged_container)"
				],
				"execution_count": 18
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Serverless SQL DB external tables\n",
					"- External tables are created if they don't exist, using the pre-requisites."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Here, we are looping through the table names in the tables_query_dictionary, and running the createView function for each one of them\n",
					"## through JDBC, against the Serverless SQL DB.\n",
					"list_errors = []\n",
					"for k in list(external_tables_instructions.keys()):\n",
					"    input_table_name = k\n",
					"    input_columns_meta = external_tables_instructions.get(k)[\"TABLE_QUERY\"]\n",
					"    input_file_path = external_tables_instructions.get(k)[\"FILE_PATH\"]\n",
					"    input_schema_name = external_tables_instructions.get(k)[\"SERVERLESS_SQL_SCHEMA\"]\n",
					"    input_database_name = external_tables_instructions.get(k)[\"SERVERLESS_SQL_POOL_DATABASE\"]\n",
					"    input_external_data_source_name = external_tables_instructions.get(k)[\"EXTERNAL_DATA_SOURCE\"]\n",
					"\n",
					"    try:\n",
					"        ext_table_definition = produce_external_table_definition(database_name = input_database_name, schema_name=input_schema_name, table_name=input_table_name, file_system=datalake_staged_container, data_source=input_external_data_source_name, location=input_file_path, columns_meta=input_columns_meta)\n",
					"        create_external_table(synapse_serverless_odbc_url, sql_access_token_odbc, str(input_database_name), ext_table_definition)\n",
					"        print(f\"\"\"Completed Table: {input_table_name}\"\"\")\n",
					"    except Exception as err_message:\n",
					"        print(err_message)\n",
					"        list_errors.append({\"table_name\": input_table_name, \"error\":err_message})\n",
					"        raise err_message"
				],
				"execution_count": 19
			}
		]
	}
}